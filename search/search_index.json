{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Via Foundry's Documentation! Getting Started Platform Overview Pipeline Examples About How To Cite Us User Guide Quick Start Guide Project Guide Run Guide Profile Guide Frequently Asked Questions","title":"Home"},{"location":"#welcome-to-via-foundrys-documentation","text":"","title":"Welcome to Via Foundry's Documentation!"},{"location":"#getting-started","text":"Platform Overview Pipeline Examples About How To Cite Us","title":"Getting Started"},{"location":"#user-guide","text":"Quick Start Guide Project Guide Run Guide Profile Guide Frequently Asked Questions","title":"User Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/","text":"Welcome to Via Foundry's Documentation! Getting Started Platform Overview Pipeline Examples About User Guide Quick Start Guide Project Guide Run Guide Profile Guide Frequently Asked Questions","title":"Welcome to Via Foundry's Documentation!"},{"location":"vfdocs/vf_docs/ViaFoundry/#welcome-to-via-foundrys-documentation","text":"","title":"Welcome to Via Foundry's Documentation!"},{"location":"vfdocs/vf_docs/ViaFoundry/#getting-started","text":"Platform Overview Pipeline Examples About","title":"Getting Started"},{"location":"vfdocs/vf_docs/ViaFoundry/#user-guide","text":"Quick Start Guide Project Guide Run Guide Profile Guide Frequently Asked Questions","title":"User Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/about/about/","text":"About Via Foundry, formerly known as DolphinNext, is developed by the Bioinformatics Core at the University of Massachusetts Medical School (UMMS). This project is licensed under the GNU General Public License 3.0. The source code of DolphinNext is available at https://github.com/UMMS-Biocore/dolphinnext and you can reach our website at https://www.viafoundry.com . How To Cite Us If you use Via Foundry (formerly DolphinNext) in your research, please cite: Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x","title":"About"},{"location":"vfdocs/vf_docs/ViaFoundry/about/about/#about","text":"Via Foundry, formerly known as DolphinNext, is developed by the Bioinformatics Core at the University of Massachusetts Medical School (UMMS). This project is licensed under the GNU General Public License 3.0. The source code of DolphinNext is available at https://github.com/UMMS-Biocore/dolphinnext and you can reach our website at https://www.viafoundry.com .","title":"About"},{"location":"vfdocs/vf_docs/ViaFoundry/about/about/#how-to-cite-us","text":"If you use Via Foundry (formerly DolphinNext) in your research, please cite: Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x","title":"How To Cite Us"},{"location":"vfdocs/vf_docs/ViaFoundry/cite/cite/","text":"How To Cite Us If you use Via Foundry (formerly DolphinNext) in your research, please cite: Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x","title":"How To Cite Us"},{"location":"vfdocs/vf_docs/ViaFoundry/cite/cite/#how-to-cite-us","text":"If you use Via Foundry (formerly DolphinNext) in your research, please cite: Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x","title":"How To Cite Us"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/","text":"Frequently Asked Questions Installation Guide How can I install Singularity? Follow this link to install Singularity (Version 3) for your pipelines. For your convenience, attached below are the commands needed to download the newest version in a Linux environment: ## Remove old version of Singularity # sudo rm -rf /usr/local/libexec/singularity /usr/local/var/singularity /usr/local/etc/singularity /usr/local/bin/singularity /usr/local/bin/run-singularity /usr/local/etc/bash_completion.d/singularity ## Install Singularity Version 3 apt-get install -y build-essential libssl-dev uuid-dev libgpgme11-dev libseccomp-dev pkg-config squashfs-tools wget https://dl.google.com/go/go1.12.7.linux-amd64.tar.gz tar -C /usr/local -xzf go1.12.7.linux-amd64.tar.gz export PATH=$PATH:/usr/local/go/bin export VERSION=3.2.1 wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz tar -xzf singularity-${VERSION}.tar.gz cd singularity ./mconfig && make -C ./builddir sudo make -C ./builddir install How can I install Docker? Follow this link to install Docker for pipelines, or follow the commands below: ## Uninstall Old Versions of Docker # sudo apt-get remove docker docker-engine docker.io ## Install Docker sudo apt install docker.io How can I install JAVA? Installing Java v8+ for Nextflow: apt-get install -y openjdk-8-jdk && apt-get install -y ant && apt-get clean; # Fix certificate issues apt-get update && apt-get install ca-certificates-java && apt-get clean && update-ca-certificates -f; export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ How can I install Nextflow? JAVA (v8+) should be installed before installing Nextflow. Once Java has been successfully installed, click this link to install Nextflow or use the commands below: ## To install to your ~/bin directory: mkdir ~/bin cd ~/bin curl -fsSL get.nextflow.io | bash # Add Nextflow binary to your bin PATH or any accessible path in your environment: chmod 755 nextflow mv nextflow ~/bin/ # OR system-wide installation: # sudo mv nextflow /usr/local/bin Connection Issues Why can't I validate my SSH Keys? If you're having trouble validating your SSH keys, here are a few things to check: Make sure you copy the entire key, including the initial part (e.g. \"ssh-rsa\"). The key should span the entire file, like in the following example: ssh-rsa AA1AB3N4nX3a.................... ................................ ................................ ...............b9Rj @viafoundry The SSH protocol requires specific permissions for files and directories to establish secure connections. Please execute the following commands to make sure your SSH-related files are properly secured: chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys Ensure that your home directory is not writable by other users. Setting the permissions of your home directory to 777 can create security issues and block SSH connections. Instead, set the permissions to more secure options such as 750, 755 or 754. How can I create SSH keys in my computer? You can find your SSH key pairs on your local machine at their default location: ~/.ssh/id_rsa for private and ~/.ssh/id_rsa.pub for public key. If no keys exist there or you want to create new ones, then on the command line, enter: ssh-keygen -t rsa You will be prompted to supply a filename and a password. If you want to accept the default filename (and location) for your key pair, just press Enter without entering a filename. Your SSH keys will be generated using the default filename ( id_rsa and id_rsa.pub ), and they will be saved in the \"~/.ssh/\" directory in your machine. Run Questions I can't reach my files in the file window There might be a connection issue. Please check to make sure you've followed these steps: The SSH protocol requires specific permissions for files and directories to establish secure connections. Please execute the following commands to make sure your SSH-related files are properly secured: chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys Ensure that your home directory is not writable by other users. Setting the permissions of your home directory to 777 can create security issues and block SSH connections. Instead, set the permissions to more secure options such as 750, 755 or 754. Error: Run directory cannot be created It's possible that there's an issue with your connection. Please check the Why can't I validate my SSH Keys section to ensure you've followed all the necessary steps. Profile Questions How should I configure my executor settings? In Via Foundry, there are four different sections to control executor settings: the first two are defined in Profile -> Run Environment , and the remaining two are adjusted in the Advanced tab of the run page. If you select an executor other than \"Local\" or \"Ignite\", Via Foundry prompts you to enter additional settings, such as the queue/partition, memory, CPU, and time. 1. Executor of Nextflow (navigate to Profile -> Run Environments) : This setting controls how Via Foundry initiates Nextflow. Currently, Via Foundry supports the Local, SGE, SLURM, and LSF executors to initiate Nextflow. For the SGE, SLURM, and LSF executors, Via Foundry only uses them to run Nextflow itself, so the time limit should be long enough to execute all processes in the pipeline. For local execution, DolphinNext limits the total amount of memory and CPU that can be used, so these values should be close to the maximum capacity of your computer. Suggested parameters for SGE/SLURM/LSF: long (queue) 8 (GB Memory) 1 (CPU) 5000-8000 (min, Time) Suggested parameters for Local: 100 (GB Memory) 8 (CPU) 2. Executor of Nextflow Jobs (navigate to Profile -> Run Environments) : This setting will be used as the default setting for submitted jobs by Nextflow if you don't set any parameters in the Advanced section of your run page. Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB Memory) 1 (CPU) 240 (min, Time) Suggested parameters for Local: 20 (GB Memory) 1 (CPU) 3. Executor Settings for All Processes (in the advanced tab of run page) : These settings will overwrite those in Executor of Nextflow Jobs and set default parameters for all Nextflow Jobs. Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB Memory) 1 (CPU) 240 (min, Time) Suggested parameters for Local: 20 (GB Memory) 1 (CPU) 4. Executor Settings for Each Process (in the advanced tab of run page) : If a particular process requires different parameters than the defaults (which are defined in the Executor Settings for All Processes or Executor of Nextflow Jobs sections), you can overwrite the general settings by clicking the checkbox of the process that you want to change. This will only affect the settings of the selected process and keep the original settings for the rest of the processes. Suggested parameters for SGE/SLURM/LSF: long (queue) 20 (GB Memory) 4 (CPU) 1000-5000 (min, Time) Suggested parameters for Local: 20 (GB Memory) 4 (CPU) Note: If non-standard resources or settings are required for the executor, then you can specify these parameters by using Other Options box. For instance, to submit an SGE job with 3 CPU using parallel environments, you can enter -pe orte 3 (to use MPI for distributed-memory machines) or -pe smp 3 (to use OpenMP for shared-memory machines) in the Other Options box, leaving the CPU box empty.","title":"Frequently Asked Questions"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#installation-guide","text":"","title":"Installation Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#how-can-i-install-singularity","text":"Follow this link to install Singularity (Version 3) for your pipelines. For your convenience, attached below are the commands needed to download the newest version in a Linux environment: ## Remove old version of Singularity # sudo rm -rf /usr/local/libexec/singularity /usr/local/var/singularity /usr/local/etc/singularity /usr/local/bin/singularity /usr/local/bin/run-singularity /usr/local/etc/bash_completion.d/singularity ## Install Singularity Version 3 apt-get install -y build-essential libssl-dev uuid-dev libgpgme11-dev libseccomp-dev pkg-config squashfs-tools wget https://dl.google.com/go/go1.12.7.linux-amd64.tar.gz tar -C /usr/local -xzf go1.12.7.linux-amd64.tar.gz export PATH=$PATH:/usr/local/go/bin export VERSION=3.2.1 wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz tar -xzf singularity-${VERSION}.tar.gz cd singularity ./mconfig && make -C ./builddir sudo make -C ./builddir install","title":"How can I install Singularity?"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#how-can-i-install-docker","text":"Follow this link to install Docker for pipelines, or follow the commands below: ## Uninstall Old Versions of Docker # sudo apt-get remove docker docker-engine docker.io ## Install Docker sudo apt install docker.io","title":"How can I install Docker?"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#how-can-i-install-java","text":"Installing Java v8+ for Nextflow: apt-get install -y openjdk-8-jdk && apt-get install -y ant && apt-get clean; # Fix certificate issues apt-get update && apt-get install ca-certificates-java && apt-get clean && update-ca-certificates -f; export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/","title":"How can I install JAVA?"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#how-can-i-install-nextflow","text":"JAVA (v8+) should be installed before installing Nextflow. Once Java has been successfully installed, click this link to install Nextflow or use the commands below: ## To install to your ~/bin directory: mkdir ~/bin cd ~/bin curl -fsSL get.nextflow.io | bash # Add Nextflow binary to your bin PATH or any accessible path in your environment: chmod 755 nextflow mv nextflow ~/bin/ # OR system-wide installation: # sudo mv nextflow /usr/local/bin","title":"How can I install Nextflow?"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#connection-issues","text":"","title":"Connection Issues"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#why-cant-i-validate-my-ssh-keys","text":"If you're having trouble validating your SSH keys, here are a few things to check: Make sure you copy the entire key, including the initial part (e.g. \"ssh-rsa\"). The key should span the entire file, like in the following example: ssh-rsa AA1AB3N4nX3a.................... ................................ ................................ ...............b9Rj @viafoundry The SSH protocol requires specific permissions for files and directories to establish secure connections. Please execute the following commands to make sure your SSH-related files are properly secured: chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys Ensure that your home directory is not writable by other users. Setting the permissions of your home directory to 777 can create security issues and block SSH connections. Instead, set the permissions to more secure options such as 750, 755 or 754.","title":"Why can't I validate my SSH Keys?"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#how-can-i-create-ssh-keys-in-my-computer","text":"You can find your SSH key pairs on your local machine at their default location: ~/.ssh/id_rsa for private and ~/.ssh/id_rsa.pub for public key. If no keys exist there or you want to create new ones, then on the command line, enter: ssh-keygen -t rsa You will be prompted to supply a filename and a password. If you want to accept the default filename (and location) for your key pair, just press Enter without entering a filename. Your SSH keys will be generated using the default filename ( id_rsa and id_rsa.pub ), and they will be saved in the \"~/.ssh/\" directory in your machine.","title":"How can I create SSH keys in my computer?"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#run-questions","text":"","title":"Run Questions"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#i-cant-reach-my-files-in-the-file-window","text":"There might be a connection issue. Please check to make sure you've followed these steps: The SSH protocol requires specific permissions for files and directories to establish secure connections. Please execute the following commands to make sure your SSH-related files are properly secured: chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys Ensure that your home directory is not writable by other users. Setting the permissions of your home directory to 777 can create security issues and block SSH connections. Instead, set the permissions to more secure options such as 750, 755 or 754.","title":"I can't reach my files in the file window"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#error-run-directory-cannot-be-created","text":"It's possible that there's an issue with your connection. Please check the Why can't I validate my SSH Keys section to ensure you've followed all the necessary steps.","title":"Error: Run directory cannot be created"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#profile-questions","text":"","title":"Profile Questions"},{"location":"vfdocs/vf_docs/ViaFoundry/faq/faq/#how-should-i-configure-my-executor-settings","text":"In Via Foundry, there are four different sections to control executor settings: the first two are defined in Profile -> Run Environment , and the remaining two are adjusted in the Advanced tab of the run page. If you select an executor other than \"Local\" or \"Ignite\", Via Foundry prompts you to enter additional settings, such as the queue/partition, memory, CPU, and time. 1. Executor of Nextflow (navigate to Profile -> Run Environments) : This setting controls how Via Foundry initiates Nextflow. Currently, Via Foundry supports the Local, SGE, SLURM, and LSF executors to initiate Nextflow. For the SGE, SLURM, and LSF executors, Via Foundry only uses them to run Nextflow itself, so the time limit should be long enough to execute all processes in the pipeline. For local execution, DolphinNext limits the total amount of memory and CPU that can be used, so these values should be close to the maximum capacity of your computer. Suggested parameters for SGE/SLURM/LSF: long (queue) 8 (GB Memory) 1 (CPU) 5000-8000 (min, Time) Suggested parameters for Local: 100 (GB Memory) 8 (CPU) 2. Executor of Nextflow Jobs (navigate to Profile -> Run Environments) : This setting will be used as the default setting for submitted jobs by Nextflow if you don't set any parameters in the Advanced section of your run page. Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB Memory) 1 (CPU) 240 (min, Time) Suggested parameters for Local: 20 (GB Memory) 1 (CPU) 3. Executor Settings for All Processes (in the advanced tab of run page) : These settings will overwrite those in Executor of Nextflow Jobs and set default parameters for all Nextflow Jobs. Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB Memory) 1 (CPU) 240 (min, Time) Suggested parameters for Local: 20 (GB Memory) 1 (CPU) 4. Executor Settings for Each Process (in the advanced tab of run page) : If a particular process requires different parameters than the defaults (which are defined in the Executor Settings for All Processes or Executor of Nextflow Jobs sections), you can overwrite the general settings by clicking the checkbox of the process that you want to change. This will only affect the settings of the selected process and keep the original settings for the rest of the processes. Suggested parameters for SGE/SLURM/LSF: long (queue) 20 (GB Memory) 4 (CPU) 1000-5000 (min, Time) Suggested parameters for Local: 20 (GB Memory) 4 (CPU) Note: If non-standard resources or settings are required for the executor, then you can specify these parameters by using Other Options box. For instance, to submit an SGE job with 3 CPU using parallel environments, you can enter -pe orte 3 (to use MPI for distributed-memory machines) or -pe smp 3 (to use OpenMP for shared-memory machines) in the Other Options box, leaving the CPU box empty.","title":"How should I configure my executor settings?"},{"location":"vfdocs/vf_docs/ViaFoundry/overview/overview/","text":"Short Overview What is Via Foundry? Via Foundry is the web interface of an intuitive and powerful bioinformatics platform designed to simplify pipeline design, development and maintenance, enabling analysis and management of mass quantities of samples on High Performance Computing (HPC) environments, cloud services, or personal workstations. It automatically builds Nextflow pipelines by assembling components such as processes and modules, enabling seamless implementation of complex bioinformatics workflows. Benefits of Via Foundry's Design Build: Via Foundry's drag-and-drop interface allows for effortless creation of new pipelines, without the need to write commands from scratch. Existing processes/modules can be reused to create new pipelines, which can then be exported as Nextflow files or readily run within Via Foundry. Run: Pipelines can be executed in any host environment with different executors, such as SGE, LSF, SLURM, Ignite, and more. Via Foundry also seamlessly integrates with Amazon/Google Cloud, allowing for easy creation of a cluster in the cloud, execution of the pipeline, and transfer of results to a cloud storage service (Amazon Storage, S3 or Google Storage, GS). Resume: Via Foundry's continuous checkpoint mechanism keeps track of each step of the running pipeline, enabling partially completed pipelines to be resumed at any stage, even after parameter changes. Analyze: Via Foundry's report section provides an execution summary of each step and selected outputs, permitting easy data analysis with interactive plots and custom R-Markdown reports. Improve: Via Foundry's revisioning system keeps track of pipeline and process versions, as well as their parameters. This allows for easy editing, improvement, and customization of shared pipelines according to your needs. Share: Via Foundry allows for easy sharing of pipelines across different platforms, with the ability to isolate pipeline-specific dependencies in a container and easily replicate methods in other clusters. What is Nextflow? Nextflow is an incredibly potent and versatile framework. Based on the dataflow programming model, it is used for building parallelized, scalable and reproducible workflows using software containers, which provides an abstraction layer between the execution and the logic of the pipeline, which means that the same pipeline code can be executed on multiple platforms. Who is Via Foundry for? Via Foundry is designed for a wide variety of users, from bench biologists to expert bioinformaticians. Executing pipelines in Via Foundry requires no programming knowledge. We aim to provide comprehensive explanations to guide users before they execute their pipelines. After a run completes, we provide an overall execution summary of each step, gathering all of the samples in simple tables or interactive plots for ultimate comparison. Building pipelines in Via Foundry requires basic scripting knowledge and familiarization with Nextflow to effectively use its operators. You don't need to learn all of the Nextflow syntax; instead, you can easily focus on the processes where basic input and output nodes are defined. The rest, such as creating and linking the channels, is handled by Via Foundry. Why not just write a Nextflow pipeline? In many cases, building a static Nextflow pipeline is sufficient for achieving our goals. However, it can be difficult to maintain process and module revisions using simple text editors. With the help of Via Foundry's user interface, you can easily track the evolution of each process and module by accessing their previous revisions. When upgrading existing pipelines, it's much easier to update each process because all of the process-related scripts are isolated in a process circle, and you don't need to deal with other parameters or channel operations that are defined in other parts of the Nextflow script. This modular structure gives us more flexibility and dynamism to create very complex pipelines with little effort. Furthermore, Via Foundry has a built-in form creator that dynamically updates the run page according to the parameters defined in the process. This tool becomes especially powerful when creating complex pipelines with hundreds of optional parameters. As seen in the example below, you can easily isolate process-related parameters in their specific windows. Please feel free to reference this image, which outlines the features of Via Foundry and Nextflow, to better understand the ethos of Via Foundry. Public Pipelines Attached is a brief list of Via Foundry's current public pipelines, along with some important sub-modules, all of which are ready to execute in your environment. RNA-Seq Pipelines (RSEM, HISAT, STAR, Tophat2) ATAC-Seq Pipeline ChIP Seq Pipeline Single Cell Pipelines (10X Genomics, Indrop) piRNA Pipelines (piPipes ChIP-Seq, Degradome/RAGE/CAGE, smallRNA) Sub-Modules : Trimmer Adapter Removal Quality Filtering Common RNA Filtering ESAT FastQC, MultiQC RSeQC Picard IGV and UCSC genome browser file conversion","title":"Platform Overview"},{"location":"vfdocs/vf_docs/ViaFoundry/overview/overview/#short-overview","text":"","title":"Short Overview"},{"location":"vfdocs/vf_docs/ViaFoundry/overview/overview/#what-is-via-foundry","text":"Via Foundry is the web interface of an intuitive and powerful bioinformatics platform designed to simplify pipeline design, development and maintenance, enabling analysis and management of mass quantities of samples on High Performance Computing (HPC) environments, cloud services, or personal workstations. It automatically builds Nextflow pipelines by assembling components such as processes and modules, enabling seamless implementation of complex bioinformatics workflows.","title":"What is Via Foundry?"},{"location":"vfdocs/vf_docs/ViaFoundry/overview/overview/#benefits-of-via-foundrys-design","text":"Build: Via Foundry's drag-and-drop interface allows for effortless creation of new pipelines, without the need to write commands from scratch. Existing processes/modules can be reused to create new pipelines, which can then be exported as Nextflow files or readily run within Via Foundry. Run: Pipelines can be executed in any host environment with different executors, such as SGE, LSF, SLURM, Ignite, and more. Via Foundry also seamlessly integrates with Amazon/Google Cloud, allowing for easy creation of a cluster in the cloud, execution of the pipeline, and transfer of results to a cloud storage service (Amazon Storage, S3 or Google Storage, GS). Resume: Via Foundry's continuous checkpoint mechanism keeps track of each step of the running pipeline, enabling partially completed pipelines to be resumed at any stage, even after parameter changes. Analyze: Via Foundry's report section provides an execution summary of each step and selected outputs, permitting easy data analysis with interactive plots and custom R-Markdown reports. Improve: Via Foundry's revisioning system keeps track of pipeline and process versions, as well as their parameters. This allows for easy editing, improvement, and customization of shared pipelines according to your needs. Share: Via Foundry allows for easy sharing of pipelines across different platforms, with the ability to isolate pipeline-specific dependencies in a container and easily replicate methods in other clusters.","title":"Benefits of Via Foundry's Design"},{"location":"vfdocs/vf_docs/ViaFoundry/overview/overview/#what-is-nextflow","text":"Nextflow is an incredibly potent and versatile framework. Based on the dataflow programming model, it is used for building parallelized, scalable and reproducible workflows using software containers, which provides an abstraction layer between the execution and the logic of the pipeline, which means that the same pipeline code can be executed on multiple platforms.","title":"What is Nextflow?"},{"location":"vfdocs/vf_docs/ViaFoundry/overview/overview/#who-is-via-foundry-for","text":"Via Foundry is designed for a wide variety of users, from bench biologists to expert bioinformaticians. Executing pipelines in Via Foundry requires no programming knowledge. We aim to provide comprehensive explanations to guide users before they execute their pipelines. After a run completes, we provide an overall execution summary of each step, gathering all of the samples in simple tables or interactive plots for ultimate comparison. Building pipelines in Via Foundry requires basic scripting knowledge and familiarization with Nextflow to effectively use its operators. You don't need to learn all of the Nextflow syntax; instead, you can easily focus on the processes where basic input and output nodes are defined. The rest, such as creating and linking the channels, is handled by Via Foundry.","title":"Who is Via Foundry for?"},{"location":"vfdocs/vf_docs/ViaFoundry/overview/overview/#why-not-just-write-a-nextflow-pipeline","text":"In many cases, building a static Nextflow pipeline is sufficient for achieving our goals. However, it can be difficult to maintain process and module revisions using simple text editors. With the help of Via Foundry's user interface, you can easily track the evolution of each process and module by accessing their previous revisions. When upgrading existing pipelines, it's much easier to update each process because all of the process-related scripts are isolated in a process circle, and you don't need to deal with other parameters or channel operations that are defined in other parts of the Nextflow script. This modular structure gives us more flexibility and dynamism to create very complex pipelines with little effort. Furthermore, Via Foundry has a built-in form creator that dynamically updates the run page according to the parameters defined in the process. This tool becomes especially powerful when creating complex pipelines with hundreds of optional parameters. As seen in the example below, you can easily isolate process-related parameters in their specific windows. Please feel free to reference this image, which outlines the features of Via Foundry and Nextflow, to better understand the ethos of Via Foundry.","title":"Why not just write a Nextflow pipeline?"},{"location":"vfdocs/vf_docs/ViaFoundry/overview/overview/#public-pipelines","text":"Attached is a brief list of Via Foundry's current public pipelines, along with some important sub-modules, all of which are ready to execute in your environment. RNA-Seq Pipelines (RSEM, HISAT, STAR, Tophat2) ATAC-Seq Pipeline ChIP Seq Pipeline Single Cell Pipelines (10X Genomics, Indrop) piRNA Pipelines (piPipes ChIP-Seq, Degradome/RAGE/CAGE, smallRNA) Sub-Modules : Trimmer Adapter Removal Quality Filtering Common RNA Filtering ESAT FastQC, MultiQC RSeQC Picard IGV and UCSC genome browser file conversion","title":"Public Pipelines"},{"location":"vfdocs/vf_docs/ViaFoundry/pipeline_example/pipeline_example/","text":"Pipeline Examples There are numerous publicly available pipelines and processes available on the Via Foundry website. Please note that this document only contains a small sampling of the pipelines Via Foundry offers. For the full list of pipelines and to explore each pipeline in detail, please visit the Via Foundry Pipeline page . On the main page, you can click on the box containing the name of the pipeline you\\'re interested in to view a detailed summary of the pipeline, including information about the minimum required inputs and a table of all the pipeline\\'s constituent processes. Please remember that the document provided here showcases only a limited selection of pipelines, and for a comprehensive list, as well as in-depth exploration of each pipeline, we recommend visiting the Via Foundry Pipeline page . RNA-Seq Pipeline Github Repository for RNA-Seq Pipeline The RNA-seq pipeline publicly available in Via Foundry includes several key steps for processing RNA-seq data: Quality Control: FastQC is used to generate quality control (QC) outputs. Optional processes such as read quality filtering (trimmomatic), read quality trimming (trimmomatic), and adapter removal (cutadapt) are available. rRNA Filtering and Genome Alignment: Bowtie2, Bowtie, and STAR are utilized for counting or filtering out and estimating the abundance of both standard and predefined sets of genomic loci, such as rRNAs, miRNAs, tRNAs, piRNAs, snoRNAs, and ERCC. Gene and Isoform Expression Estimation: RSEM is employed to align RNA-seq reads to reference transcripts and estimate gene and isoform expression levels. Genome Alignment: HISAT2, STAR, and Tophat2 are employed to align RNA-seq reads to the genome. Optional estimation of gene and isoform expression levels can be performed using featureCount. Quality Metrics and Reports: If the user opts to perform genomic alignments, the pipeline generates overall quality metrics, including coverage and the number of mapped reads to different genomic and transcriptomic regions. These reports rely on Picard\\'s CollectRNASeqMetrics program (Broad Institute, n.d.) and the RSeQC program (Wang, Wang, and Li 2012). Visualization: Optional generation of Integrative Genomics Viewer (IGV) and Genome Browser Files (TDF and Bigwig) is available. Quantification Matrix and Analysis: The RNA-seq pipeline provides a quantification matrix that includes estimated counts and transcript per million (TPM) values for each gene and annotated isoform. These matrices serve as input for differential gene expression analysis and can be directly uploaded to an embedded instance of DEBrowser software for interactive exploration of the resulting data (Kucukural et al. 2019). ATAC-Seq and ChIP-Seq pipelines Github Repository for ATAC-seq Pipeline Github Repository for CHIP-seq Pipeline Via Foundry offers comprehensive pipelines for the processing of ChIP-Seq and ATAC-Seq data, which are widely used in genomic research. Although these pipelines share many common processes, they exhibit specific differences at certain stages. Moreover, they rely on identical data preparation steps employed in the RNA-Seq pipeline, including read filtering, read quality reporting, and alignment to desired genomic locations. The key steps involved in the ChIP-Seq and ATAC-Seq pipelines are as follows: Quality Control: The pipelines utilize FastQC to assess the quality of the sequencing reads and generate quality control outputs. Additionally, optional processes such as read quality filtering (trimmomatic), read quality trimming (trimmomatic), and adapter removal (cutadapt) can be employed to further refine the data. Counting and Filtering: To estimate the abundance of both standard and predefined sets of genomic loci (e.g., rRNAs, miRNAs, tRNAs, piRNAs, snoRNAs, ERCC), the pipelines employ tools like Bowtie2/Bowtie/STAR. These tools facilitate read counting or filtering to obtain valuable insights into the genomic regions of interest. Read Alignment: The short-read aligner Bowtie2 is employed to align the sequencing reads to a reference genome (Langmead and Salzberg 2012). In cases where the input files are large, such as those obtained from ATAC-Seq experiments, the pipeline optimizes alignment speed by splitting the files into smaller chunks and performing parallel alignments. PCR Duplicate Removal: The pipelines incorporate the Picard mark duplicates function (Broad Institute, n.d.) and Samtools (H. Li et al. 2009) to estimate and remove PCR duplicates. By employing merged alignments, the duplicate reads can be efficiently identified and eliminated, ensuring accurate downstream analysis. ATAC-Seq-specific Analysis: In the case of ATAC-Seq data, the pipeline performs additional steps. It identifies accessible chromatin regions by estimating the Tn5 transposase cut sites. This estimation involves positioning on the 9th base upstream of the 5\\' read end and extending by 29 bases downstream. This extension process is based on studies (Donnard et al. 2018; Buenrostro et al. 2013) that have shown it to more accurately reflect the exact positions accessible to the transposase. Subsequently, peaks are called using MACS2 (Zhang et al. 2008) in both the ChIP-Seq and ATAC-Seq pipelines. Consensus Peak Calling and Quantification: When processing multiple samples together, the ATAC-Seq and ChIP-Seq pipelines offer the option of generating consensus peak calls. This is achieved by merging all peaks individually called in each sample using Bedtools (Quinlan and Hall 2010). Furthermore, the pipelines quantify the number of reads in each peak location using Bedtools\\' coverage function, facilitating comprehensive analysis of the data. Data Analysis: As a result, both the ATAC-Seq and ChIP-Seq pipelines generate a matrix containing count values for each peak region and sample. This matrix can be directly uploaded to the embedded version of DEBrowser (Kucukural et al. 2019) for performing differential analysis. Alternatively, the matrix can be downloaded for further analysis using other tools or methods. Other Pipelines Via Foundry has several other pipelines, including the Cell Ranger Pipeline and the Cell Ranger Multi Pipeline, currently available for public use, and are actively adding more to the platform. Stay tuned for updates. How To Cite Us If you use Via Foundry (formerly DolphinNext) in your research, please cite: Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x Support For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Pipeline Examples"},{"location":"vfdocs/vf_docs/ViaFoundry/pipeline_example/pipeline_example/#pipeline-examples","text":"There are numerous publicly available pipelines and processes available on the Via Foundry website. Please note that this document only contains a small sampling of the pipelines Via Foundry offers. For the full list of pipelines and to explore each pipeline in detail, please visit the Via Foundry Pipeline page . On the main page, you can click on the box containing the name of the pipeline you\\'re interested in to view a detailed summary of the pipeline, including information about the minimum required inputs and a table of all the pipeline\\'s constituent processes. Please remember that the document provided here showcases only a limited selection of pipelines, and for a comprehensive list, as well as in-depth exploration of each pipeline, we recommend visiting the Via Foundry Pipeline page .","title":"Pipeline Examples"},{"location":"vfdocs/vf_docs/ViaFoundry/pipeline_example/pipeline_example/#rna-seq-pipeline","text":"Github Repository for RNA-Seq Pipeline The RNA-seq pipeline publicly available in Via Foundry includes several key steps for processing RNA-seq data: Quality Control: FastQC is used to generate quality control (QC) outputs. Optional processes such as read quality filtering (trimmomatic), read quality trimming (trimmomatic), and adapter removal (cutadapt) are available. rRNA Filtering and Genome Alignment: Bowtie2, Bowtie, and STAR are utilized for counting or filtering out and estimating the abundance of both standard and predefined sets of genomic loci, such as rRNAs, miRNAs, tRNAs, piRNAs, snoRNAs, and ERCC. Gene and Isoform Expression Estimation: RSEM is employed to align RNA-seq reads to reference transcripts and estimate gene and isoform expression levels. Genome Alignment: HISAT2, STAR, and Tophat2 are employed to align RNA-seq reads to the genome. Optional estimation of gene and isoform expression levels can be performed using featureCount. Quality Metrics and Reports: If the user opts to perform genomic alignments, the pipeline generates overall quality metrics, including coverage and the number of mapped reads to different genomic and transcriptomic regions. These reports rely on Picard\\'s CollectRNASeqMetrics program (Broad Institute, n.d.) and the RSeQC program (Wang, Wang, and Li 2012). Visualization: Optional generation of Integrative Genomics Viewer (IGV) and Genome Browser Files (TDF and Bigwig) is available. Quantification Matrix and Analysis: The RNA-seq pipeline provides a quantification matrix that includes estimated counts and transcript per million (TPM) values for each gene and annotated isoform. These matrices serve as input for differential gene expression analysis and can be directly uploaded to an embedded instance of DEBrowser software for interactive exploration of the resulting data (Kucukural et al. 2019).","title":"RNA-Seq Pipeline"},{"location":"vfdocs/vf_docs/ViaFoundry/pipeline_example/pipeline_example/#atac-seq-and-chip-seq-pipelines","text":"Github Repository for ATAC-seq Pipeline Github Repository for CHIP-seq Pipeline Via Foundry offers comprehensive pipelines for the processing of ChIP-Seq and ATAC-Seq data, which are widely used in genomic research. Although these pipelines share many common processes, they exhibit specific differences at certain stages. Moreover, they rely on identical data preparation steps employed in the RNA-Seq pipeline, including read filtering, read quality reporting, and alignment to desired genomic locations. The key steps involved in the ChIP-Seq and ATAC-Seq pipelines are as follows: Quality Control: The pipelines utilize FastQC to assess the quality of the sequencing reads and generate quality control outputs. Additionally, optional processes such as read quality filtering (trimmomatic), read quality trimming (trimmomatic), and adapter removal (cutadapt) can be employed to further refine the data. Counting and Filtering: To estimate the abundance of both standard and predefined sets of genomic loci (e.g., rRNAs, miRNAs, tRNAs, piRNAs, snoRNAs, ERCC), the pipelines employ tools like Bowtie2/Bowtie/STAR. These tools facilitate read counting or filtering to obtain valuable insights into the genomic regions of interest. Read Alignment: The short-read aligner Bowtie2 is employed to align the sequencing reads to a reference genome (Langmead and Salzberg 2012). In cases where the input files are large, such as those obtained from ATAC-Seq experiments, the pipeline optimizes alignment speed by splitting the files into smaller chunks and performing parallel alignments. PCR Duplicate Removal: The pipelines incorporate the Picard mark duplicates function (Broad Institute, n.d.) and Samtools (H. Li et al. 2009) to estimate and remove PCR duplicates. By employing merged alignments, the duplicate reads can be efficiently identified and eliminated, ensuring accurate downstream analysis. ATAC-Seq-specific Analysis: In the case of ATAC-Seq data, the pipeline performs additional steps. It identifies accessible chromatin regions by estimating the Tn5 transposase cut sites. This estimation involves positioning on the 9th base upstream of the 5\\' read end and extending by 29 bases downstream. This extension process is based on studies (Donnard et al. 2018; Buenrostro et al. 2013) that have shown it to more accurately reflect the exact positions accessible to the transposase. Subsequently, peaks are called using MACS2 (Zhang et al. 2008) in both the ChIP-Seq and ATAC-Seq pipelines. Consensus Peak Calling and Quantification: When processing multiple samples together, the ATAC-Seq and ChIP-Seq pipelines offer the option of generating consensus peak calls. This is achieved by merging all peaks individually called in each sample using Bedtools (Quinlan and Hall 2010). Furthermore, the pipelines quantify the number of reads in each peak location using Bedtools\\' coverage function, facilitating comprehensive analysis of the data. Data Analysis: As a result, both the ATAC-Seq and ChIP-Seq pipelines generate a matrix containing count values for each peak region and sample. This matrix can be directly uploaded to the embedded version of DEBrowser (Kucukural et al. 2019) for performing differential analysis. Alternatively, the matrix can be downloaded for further analysis using other tools or methods.","title":"ATAC-Seq and ChIP-Seq pipelines"},{"location":"vfdocs/vf_docs/ViaFoundry/pipeline_example/pipeline_example/#other-pipelines","text":"Via Foundry has several other pipelines, including the Cell Ranger Pipeline and the Cell Ranger Multi Pipeline, currently available for public use, and are actively adding more to the platform. Stay tuned for updates.","title":"Other Pipelines"},{"location":"vfdocs/vf_docs/ViaFoundry/pipeline_example/pipeline_example/#how-to-cite-us","text":"If you use Via Foundry (formerly DolphinNext) in your research, please cite: Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x","title":"How To Cite Us"},{"location":"vfdocs/vf_docs/ViaFoundry/pipeline_example/pipeline_example/#support","text":"For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Support"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/","text":"Profile Guide In this guide, you'll discover all the available options you can explore on your Via Foundry Profile page. Profile Page After logging in, simply click on the \"Profile\" tab located at the top-right corner of your screen. You'll see a number of different tabs on your profile page that you can click through and explore. The platform offers several options for creating connection profiles and managing your credentials. First up is the Run Environments tab, which is your main hub for creating connection profiles. Next, you can use the Groups tab to create a group and add members to it, allowing you to share your runs or pipelines with others. To manage your SSH keys, you'll need to head over to the Run Environments tab, where you can create new keys or enter existing SSH key pairs in the Edit section to establish connections with hosts. You can also add your Amazon Keys or Google Keys in order to execute your runs in the cloud. Under the Repositories tab (formerly called GitHub ), you can enter your security credentials to push your pipeline information to your Github or Bitbuckets account. If you're not using Google sign-in, you can change your password within the Change Password section. Lastly, in the Notification tab, you can opt into receiving emails about completed or failed runs. It's important to note that before creating a run environment, you'll need to create SSH keys in the Run Environments tab. And if you plan on executing runs in the cloud, you'll need to add your Amazon or Google keys as well. SSH Keys To create or enter existing SSH key pairs, navigate to the SSH Keys tab and click on the Add SSH Key button in the top right corner. Next, you'll need to choose between two methods: A. Use your own keys: If you choose this option, you'll need to provide your private and public key pairs. You can find these keys on your computer at the default location: '~/.ssh/id_rsa 'for the private key and '~/.ssh/id_rsa.pub' for the public key. Simply copy and paste these keys into the appropriate fields in your browser. If these files don't exist or you want to create new ones, check out this link for guidance. B. Create new keys: To generate a new pair of SSH keys, simply click the \"Generate Keys\" button. After you've saved your key, your information will be encrypted and kept secure. To establish a connection, you'll need to add your public key to the '~/.ssh/authorized_keys' file on the host machine. For help with this step, feel free to contact us at support@viascientific.com . Amazon Keys To enter your AWS security credentials (access key, secret key, and default region), head over to the Amazon Keys tab and click on the Add Amazon Key button. Rest assured that your information will be encrypted and kept secure, and only you will have full access to view and modify your key information. Note: Once you've saved your key, it won't be visible for security purposes. However, you can always overwrite it with a new key or delete it if needed. Google Keys To enter your Project ID and Service Account Key in the Google keys tab, start by clicking the Add Google Key button. For your Project ID , head over to the Google Cloud Console and navigate to the Dashboard section. From there, check the Project info box to find your Project ID, which should look something like \"viafoundry-193616\". To input your Service Account Key , also head over to the Google Cloud Console and navigate to APIs & Services \u2192 Credentials . From there, click on the Create Credentials drop-down and select Service Account Key . On the following page, choose an existing service account or create a new one if needed, then select JSON as the \"Key Type\". Finally, click the Create button and download the JSON file with a name of your choice (e.g., creds.json). Remember that, after saving your key, you won't be able to view your Service Account Key for security reasons. However, you can always overwrite it with a new key or delete it if necessary. Groups In the Groups tab, you can create groups by selecting the Create a Group button. Once you have created a group, you can add members by clicking the Options > Edit Group Members button. This interactive platform allows you to share your process, pipeline, or projects with your group members. To view the current members of the group, select the Options > View Group Members button. Additionally, you have the option to delete your group by selecting the Options > Delete Group button, or to edit its name with Options > Edit Group Name . Run Environments This section is used for defining connection profiles by clicking on the Add Environment button. You can choose from three options: Host, Amazon or Google. Host : This option is for users who have access to High Performance Computing (HPC) environments or personal workstations. Amazon : This option is for users who have an Amazon Web Services (AWS) account or plan to create an EC2 instance to run jobs in the cloud. Google : This option is for users who want to use their Google Cloud account to run jobs in the cloud. A. Defining Host Profiles: Username/Hostname: To connect to a remote host, you need to provide your username and the hostname of the remote host in the format \" yourusername@yourhostname \". For instance, for the username \"us2r\" and hostname \"ghpcc06.umassrc.org\", you should enter \" us2r@ghpcc06.umassrc.org \". SSH Port (optional): By default, Via Foundry uses TCP port 22 for SSH connections. However, you can specify a different port number if needed. SSH Keys: Via Foundry stores your SSH keys in the SSH keys tab and uses them to authenticate your SSH connections. Run Command (optional): You can specify a command or a series of commands to run before starting the Nextflow job. Separate multiple commands using \"&&\". For example: source /etc/bashrc && module load java/1.8.0_77 && module load singularity/singularity-3.4.0 Nextflow Path (optional): If the Nextflow executable is not in your $PATH, you can specify the path to the executable in this block. For example: /project/umw_biocore/bin Singularity Cache Folder: Via Foundry uses a local directory to store Singularity images downloaded from remote hosts. By default, this directory is located in your home directory. However, if you are using a computing cluster, you need to specify a shared directory that is accessible from all computing nodes. Profile Variables: To facilitate the use of genome reference and index files in your pipelines, you can specify a download directory in which these files are stored. If multiple users are using Via Foundry, it is recommended to use a shared path in your cluster. For example: params.DOWNDIR=\"/share/dolphinnext/downloads\" Environment Variables: You can set BASH environmental variables here. Note: don't use spaces to separate multiple variables; use newlines instead. Executor Settings: In Via Foundry, there are four different sections to control executor settings: the first two are defined in Profile -> Run Environment , and the remaining two are adjusted in the Advanced tab of the run page. If you select an executor other than \"Local\" or \"Ignite\", Via Foundry prompts you to enter additional settings, such as the queue/partition, memory, CPU, and time. 1. Executor of Nextflow (navigate to Profile -> Run Environments) : This setting controls how Via Foundry initiates Nextflow. Currently, Via Foundry supports the Local, SGE, SLURM, and LSF executors to initiate Nextflow. For the SGE, SLURM, and LSF executors, Via Foundry only uses them to run Nextflow itself, so the time limit should be long enough to execute all processes in the pipeline. For local execution, DolphinNext limits the total amount of memory and CPU that can be used, so these values should be close to the maximum capacity of your computer. Suggested parameters for SGE/SLURM/LSF: long (queue) 8 (GB Memory) 1 (CPU) 5000-8000 (min, Time) Suggested parameters for Local: 100 (GB Memory) 8 (CPU) 2. Executor of Nextflow Jobs (navigate to Profile -> Run Environments) : This setting will be used as the default setting for submitted jobs by Nextflow if you don't set any parameters in the Advanced section of your run page. Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB Memory) 1 (CPU) 240 (min, Time) Suggested parameters for Local: 20 (GB Memory) 1 (CPU) 3. Executor Settings for All Processes (in the advanced tab of run page) : These settings will overwrite those in Executor of Nextflow Jobs and set default parameters for all Nextflow Jobs. Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB Memory) 1 (CPU) 240 (min, Time) Suggested parameters for Local: 20 (GB Memory) 1 (CPU) 4. Executor Settings for Each Process (in the advanced tab of run page) : If a particular process requires different parameters than the defaults (which are defined in the Executor Settings for All Processes or Executor of Nextflow Jobs sections), you can overwrite the general settings by clicking the checkbox of the process that you want to change. This will only affect the settings of the selected process and keep the original settings for the rest of the processes. Suggested parameters for SGE/SLURM/LSF: long (queue) 20 (GB Memory) 4 (CPU) 1000-5000 (min, Time) Suggested parameters for Local: 20 (GB Memory) 4 (CPU) Note: If non-standard resources or settings are required for the executor, then you can specify these parameters by using Other Options box. For instance, to submit an SGE job with 3 CPU using parallel environments, you can enter -pe orte 3 (to use MPI for distributed-memory machines) or -pe smp 3 (to use OpenMP for shared-memory machines) in the Other Options box, leaving the CPU box empty. ::: B. Defining AWS Batch Profiles: Please choose type of the run environment as \"Host\" and enter following information. Username/Hostname: To connect to a remote host, you need to provide your username and the hostname of the remote host in the format \" yourusername@yourhostname \". For instance, for the username \"us2r\" and hostname \"ghpcc06.umassrc.org\", you should enter \" us2r@ghpcc06.umassrc.org \". SSH Port (optional): By default, Via Foundry uses TCP port 22 for SSH connections. However, you can specify a different port number if needed. SSH Keys: Via Foundry stores your SSH keys in the SSH keys tab and uses them to authenticate your SSH connections. Run Command (optional): You can specify a command or a series of commands to run before starting the Nextflow job. Separate multiple commands using \"&&\". For example: source /etc/bashrc && module load java/1.8.0_77 && module load singularity/singularity-3.4.0 Nextflow Path (optional): If the Nextflow executable is not in your $PATH, you can specify the path to the executable in this block. For example: /project/umw_biocore/bin Singularity Cache Folder: Via Foundry uses a local directory to store Singularity images downloaded from remote hosts. By default, this directory is located in your home directory. However, if you are using a computing cluster, you need to specify a shared directory that is accessible from all computing nodes. Profile Variables: You can set commonly used pipeline variables here. For instance, params.DOWNDIR is used in most of our public pipelines to save all genome related files (fasta, index etc.), so you can set this variable like this: params.DOWNDIR = \"/share/dnext_data\" Also, you can enter multiple variables by separating them with newlines. Environment Variables: You can set BASH environmental variables here. Note: don't use spaces to separate multiple variables; use newlines instead. Executor of Nextflow: Please select the Local Nextflow executor. Executor Settings for Nextflow: Please enter 10GB in the Memory field and 1 in the CPU field. Executor of Nextflow Jobs: Please select AWS Batch for Nextflow jobs. Queue, Memory, CPU, and other options: Please enter the queue name and set the default memory and CPU you'll allocate for each job (e.g. 10GB memory and 1CPU). These settings can be adjusted in the run page. Amazon Keys: AWS credentials that are saved in the Amazon Keys tab will allow you to submit jobs to AWS Batch. Default Working Directory: Default directory in the host machine where runs will be executed. (eg. /data/dnext ) Default Bucket Location for Publishing: Default bucket location where dolphinnext reports will be published. (e.g. s3://bucket/dnext ) C. Defining Non-Batch Amazon Web Services Profiles: SSH Keys: These are saved in the SSH Keys tab and will be used when connecting to a host. SSH keys are a secure way to authenticate and encrypt connections between servers. It's recommended to generate a new key pair for each instance you plan to connect to. * Amazon Keys: These AWS credentials are saved in the Amazon Keys tab and allow you to start/stop Amazon EC2 instances. These credentials consist of an access key and a secret access key. * Instance Type: This refers to the type of _Amazon EC2 instance that you will use. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. For example, \"m3.xlarge\" is an instance type that provides a balance of CPU and memory resources. * Image ID: This is the virtual machine ID (VM ID) that you will use to launch the instance. The image ID is specific to the region and operating system that you are using. For example, \"ami-032a33ebe57465518\" is a sample image ID for an Amazon Machine Image (AMI) that is based on the Ubuntu operating system. If you want to create your own image, please install the following programs: Singularity Docker engine (version 1.11 or higher) Apache Ignite with Cloud-init package Nextflow AWS CLI Subnet ID/Security Group/Shared Storage ID/Shared Storage Mount: The filesystem needs to be created at https://console.aws.amazon.com/efs/ , and this information will be obtained upon the creation of a shared file system. Subnet ID: Identifier of the VPC subnet to be applied, e.g., subnet-05222a43. Security Group: Identifier of the security group to be applied, e.g., sg-df72b9ba, which is used by default. Shared Storage ID: Identifier of the shared file system instance, e.g., fs-1803efd1. Shared Storage Mount: Mount path of the shared file system, e.g., /mnt/efs. Please make sure the following criteria are satisfied: The image used must have the directory to mount this storage. The output directory must be under this mount location. The storage system must be created in the selected region, and necessary rights need to be given in the console. The EC2FullAccess and S3FullAccess permissions have been added. Warning: Both the EFS and images should be located in the same location. Default Working Directory: This is the default directory in the host machine where runs will be executed. It is an optional parameter for AWS. For example, you can set it to \"/data/dnext\". Default Bucket Location for Publishing: This is the default bucket location where Via Foundry reports will be published. It is also an optional parameter for AWS. For example, you can set it to s3://bucket/dnext . Run Command (optional): You may specify a command or multiple commands to be run before the Nextflow job starts. Separate multiple commands with the && sign. For example, you could use the following command to load modules before running the job: source /etc/bashrc && module load java/1.8.0_31 && module load bowtie2/2.3.2 . Nextflow Path (optional): If the Nextflow path is not added to the $PATH environment variable, you can define the path in this block. For example, you could set it to /project/umw_biocore/bin . Singularity Cache Folder: This is the directory where remote Singularity images are stored. By default, the home directory is used. Note that when using a computing cluster, it must be a shared folder that is accessible from all computing nodes. Profile Variables: You can set commonly used pipeline variables here. For instance, params.DOWNDIR is used in most of our public pipelines to save all genome related files (fasta, index etc.), so you can set this variable like this: params.DOWNDIR = \"/share/dnext_data\" You can also enter multiple variables by separating them with newlines. Executor of Nextflow/Executor of Nextflow Jobs: Amazon instances are automatically configured to use the Ignite executors. As such, when defining an Amazon profile, you should select Local for Executor of Nextflow and Ignite for Executor of Nextflow Jobs. D. Defining Google Profiles: SSH Keys: are saved in SSH keys tab and will be used while connecting to host. Google Keys: Google credentials that are saved in Google keys tab and will allow to start/stop Google Cloud instances. Zone: The Google zone where the computation is executed.(eg. us-east1-b) Instance Type: Google Cloud machine types that comprise varying combinations of CPU, memory, storage, and networking capacity (eg. n1-standard-4 ). Image ID: Virtual machine ID (eg. dolphinnext-193616/global/images/dolphinnext-images-v1 ). If you want to create your own image, please install following programs: Singularity Docker engine (version 1.11 or higher) Apache Ignite with Cloud-init package Nextflow gcloud Default Working Directory: This is the default directory in the host machine where runs will be executed. It is mandatory for Google Cloud profiles. (e.g., /data/dnext) Default Bucket Location for Publishing: This is the default bucket location where Via Foundry reports will be published. It is mandatory for Google Cloud profiles, and you can always edit this path in the run page. For example, you could set it to gs://bucket/dnext . Run Command (optional): You may specify a command or multiple commands to be run before the Nextflow job starts. Separate multiple commands with the && sign. For example, you could use the following command to load modules before running the job: source /etc/bashrc && module load java/1.8.0_31 && module load bowtie2/2.3.2 . Nextflow Path (optional): If the Nextflow path is not added to the $PATH environment variable, you can define the path in this block. For example, you could set it to /project/umw_biocore/bin . Singularity Cache Folder: This is the directory where remote Singularity images are stored. By default, the home directory is used. Note that when using a computing cluster, it must be a shared folder that is accessible from all computing nodes. Profile Variables: You can set commonly used pipeline variables here. For instance, params.DOWNDIR is used in most of our public pipelines to save all genome related files (fasta, index etc.), so you can set this variable like this: params.DOWNDIR = \"/share/dnext_data\" You can also enter multiple variables by separating them with newlines. Executor of Nextflow/Executor of Nextflow Jobs: Google instances are automatically configured to use the Ignite executors. As such, when defining a Google profile, you can select Local for Executor of Nextflow and Ignite for Executor of Nextflow Jobs. GitHub Connection You can enter your GitHub or Bitbucket security credentials (Username, E-mail, Password) by clicking on the Add Account button in the Repositories tab. Your information will be encrypted and kept secure. By adding a Github or Bitbucket account, you'll be able to push your pipeline information into a public account, where you can then share it with others. Change Password If you're not using a Google sign-in, you can change your Via Foundry password by using this section. Support For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Profile Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#profile-guide","text":"In this guide, you'll discover all the available options you can explore on your Via Foundry Profile page.","title":"Profile Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#profile-page","text":"After logging in, simply click on the \"Profile\" tab located at the top-right corner of your screen. You'll see a number of different tabs on your profile page that you can click through and explore. The platform offers several options for creating connection profiles and managing your credentials. First up is the Run Environments tab, which is your main hub for creating connection profiles. Next, you can use the Groups tab to create a group and add members to it, allowing you to share your runs or pipelines with others. To manage your SSH keys, you'll need to head over to the Run Environments tab, where you can create new keys or enter existing SSH key pairs in the Edit section to establish connections with hosts. You can also add your Amazon Keys or Google Keys in order to execute your runs in the cloud. Under the Repositories tab (formerly called GitHub ), you can enter your security credentials to push your pipeline information to your Github or Bitbuckets account. If you're not using Google sign-in, you can change your password within the Change Password section. Lastly, in the Notification tab, you can opt into receiving emails about completed or failed runs. It's important to note that before creating a run environment, you'll need to create SSH keys in the Run Environments tab. And if you plan on executing runs in the cloud, you'll need to add your Amazon or Google keys as well.","title":"Profile Page"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#ssh-keys","text":"To create or enter existing SSH key pairs, navigate to the SSH Keys tab and click on the Add SSH Key button in the top right corner. Next, you'll need to choose between two methods: A. Use your own keys: If you choose this option, you'll need to provide your private and public key pairs. You can find these keys on your computer at the default location: '~/.ssh/id_rsa 'for the private key and '~/.ssh/id_rsa.pub' for the public key. Simply copy and paste these keys into the appropriate fields in your browser. If these files don't exist or you want to create new ones, check out this link for guidance. B. Create new keys: To generate a new pair of SSH keys, simply click the \"Generate Keys\" button. After you've saved your key, your information will be encrypted and kept secure. To establish a connection, you'll need to add your public key to the '~/.ssh/authorized_keys' file on the host machine. For help with this step, feel free to contact us at support@viascientific.com .","title":"SSH Keys"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#amazon-keys","text":"To enter your AWS security credentials (access key, secret key, and default region), head over to the Amazon Keys tab and click on the Add Amazon Key button. Rest assured that your information will be encrypted and kept secure, and only you will have full access to view and modify your key information. Note: Once you've saved your key, it won't be visible for security purposes. However, you can always overwrite it with a new key or delete it if needed.","title":"Amazon Keys"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#google-keys","text":"To enter your Project ID and Service Account Key in the Google keys tab, start by clicking the Add Google Key button. For your Project ID , head over to the Google Cloud Console and navigate to the Dashboard section. From there, check the Project info box to find your Project ID, which should look something like \"viafoundry-193616\". To input your Service Account Key , also head over to the Google Cloud Console and navigate to APIs & Services \u2192 Credentials . From there, click on the Create Credentials drop-down and select Service Account Key . On the following page, choose an existing service account or create a new one if needed, then select JSON as the \"Key Type\". Finally, click the Create button and download the JSON file with a name of your choice (e.g., creds.json). Remember that, after saving your key, you won't be able to view your Service Account Key for security reasons. However, you can always overwrite it with a new key or delete it if necessary.","title":"Google Keys"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#groups","text":"In the Groups tab, you can create groups by selecting the Create a Group button. Once you have created a group, you can add members by clicking the Options > Edit Group Members button. This interactive platform allows you to share your process, pipeline, or projects with your group members. To view the current members of the group, select the Options > View Group Members button. Additionally, you have the option to delete your group by selecting the Options > Delete Group button, or to edit its name with Options > Edit Group Name .","title":"Groups"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#run-environments","text":"This section is used for defining connection profiles by clicking on the Add Environment button. You can choose from three options: Host, Amazon or Google. Host : This option is for users who have access to High Performance Computing (HPC) environments or personal workstations. Amazon : This option is for users who have an Amazon Web Services (AWS) account or plan to create an EC2 instance to run jobs in the cloud. Google : This option is for users who want to use their Google Cloud account to run jobs in the cloud.","title":"Run Environments"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#a-defining-host-profiles","text":"Username/Hostname: To connect to a remote host, you need to provide your username and the hostname of the remote host in the format \" yourusername@yourhostname \". For instance, for the username \"us2r\" and hostname \"ghpcc06.umassrc.org\", you should enter \" us2r@ghpcc06.umassrc.org \". SSH Port (optional): By default, Via Foundry uses TCP port 22 for SSH connections. However, you can specify a different port number if needed. SSH Keys: Via Foundry stores your SSH keys in the SSH keys tab and uses them to authenticate your SSH connections. Run Command (optional): You can specify a command or a series of commands to run before starting the Nextflow job. Separate multiple commands using \"&&\". For example: source /etc/bashrc && module load java/1.8.0_77 && module load singularity/singularity-3.4.0 Nextflow Path (optional): If the Nextflow executable is not in your $PATH, you can specify the path to the executable in this block. For example: /project/umw_biocore/bin Singularity Cache Folder: Via Foundry uses a local directory to store Singularity images downloaded from remote hosts. By default, this directory is located in your home directory. However, if you are using a computing cluster, you need to specify a shared directory that is accessible from all computing nodes. Profile Variables: To facilitate the use of genome reference and index files in your pipelines, you can specify a download directory in which these files are stored. If multiple users are using Via Foundry, it is recommended to use a shared path in your cluster. For example: params.DOWNDIR=\"/share/dolphinnext/downloads\" Environment Variables: You can set BASH environmental variables here. Note: don't use spaces to separate multiple variables; use newlines instead. Executor Settings: In Via Foundry, there are four different sections to control executor settings: the first two are defined in Profile -> Run Environment , and the remaining two are adjusted in the Advanced tab of the run page. If you select an executor other than \"Local\" or \"Ignite\", Via Foundry prompts you to enter additional settings, such as the queue/partition, memory, CPU, and time. 1. Executor of Nextflow (navigate to Profile -> Run Environments) : This setting controls how Via Foundry initiates Nextflow. Currently, Via Foundry supports the Local, SGE, SLURM, and LSF executors to initiate Nextflow. For the SGE, SLURM, and LSF executors, Via Foundry only uses them to run Nextflow itself, so the time limit should be long enough to execute all processes in the pipeline. For local execution, DolphinNext limits the total amount of memory and CPU that can be used, so these values should be close to the maximum capacity of your computer. Suggested parameters for SGE/SLURM/LSF: long (queue) 8 (GB Memory) 1 (CPU) 5000-8000 (min, Time) Suggested parameters for Local: 100 (GB Memory) 8 (CPU) 2. Executor of Nextflow Jobs (navigate to Profile -> Run Environments) : This setting will be used as the default setting for submitted jobs by Nextflow if you don't set any parameters in the Advanced section of your run page. Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB Memory) 1 (CPU) 240 (min, Time) Suggested parameters for Local: 20 (GB Memory) 1 (CPU) 3. Executor Settings for All Processes (in the advanced tab of run page) : These settings will overwrite those in Executor of Nextflow Jobs and set default parameters for all Nextflow Jobs. Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB Memory) 1 (CPU) 240 (min, Time) Suggested parameters for Local: 20 (GB Memory) 1 (CPU) 4. Executor Settings for Each Process (in the advanced tab of run page) : If a particular process requires different parameters than the defaults (which are defined in the Executor Settings for All Processes or Executor of Nextflow Jobs sections), you can overwrite the general settings by clicking the checkbox of the process that you want to change. This will only affect the settings of the selected process and keep the original settings for the rest of the processes. Suggested parameters for SGE/SLURM/LSF: long (queue) 20 (GB Memory) 4 (CPU) 1000-5000 (min, Time) Suggested parameters for Local: 20 (GB Memory) 4 (CPU) Note: If non-standard resources or settings are required for the executor, then you can specify these parameters by using Other Options box. For instance, to submit an SGE job with 3 CPU using parallel environments, you can enter -pe orte 3 (to use MPI for distributed-memory machines) or -pe smp 3 (to use OpenMP for shared-memory machines) in the Other Options box, leaving the CPU box empty. :::","title":"A. Defining Host Profiles:"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#b-defining-aws-batch-profiles","text":"Please choose type of the run environment as \"Host\" and enter following information. Username/Hostname: To connect to a remote host, you need to provide your username and the hostname of the remote host in the format \" yourusername@yourhostname \". For instance, for the username \"us2r\" and hostname \"ghpcc06.umassrc.org\", you should enter \" us2r@ghpcc06.umassrc.org \". SSH Port (optional): By default, Via Foundry uses TCP port 22 for SSH connections. However, you can specify a different port number if needed. SSH Keys: Via Foundry stores your SSH keys in the SSH keys tab and uses them to authenticate your SSH connections. Run Command (optional): You can specify a command or a series of commands to run before starting the Nextflow job. Separate multiple commands using \"&&\". For example: source /etc/bashrc && module load java/1.8.0_77 && module load singularity/singularity-3.4.0 Nextflow Path (optional): If the Nextflow executable is not in your $PATH, you can specify the path to the executable in this block. For example: /project/umw_biocore/bin Singularity Cache Folder: Via Foundry uses a local directory to store Singularity images downloaded from remote hosts. By default, this directory is located in your home directory. However, if you are using a computing cluster, you need to specify a shared directory that is accessible from all computing nodes. Profile Variables: You can set commonly used pipeline variables here. For instance, params.DOWNDIR is used in most of our public pipelines to save all genome related files (fasta, index etc.), so you can set this variable like this: params.DOWNDIR = \"/share/dnext_data\" Also, you can enter multiple variables by separating them with newlines. Environment Variables: You can set BASH environmental variables here. Note: don't use spaces to separate multiple variables; use newlines instead. Executor of Nextflow: Please select the Local Nextflow executor. Executor Settings for Nextflow: Please enter 10GB in the Memory field and 1 in the CPU field. Executor of Nextflow Jobs: Please select AWS Batch for Nextflow jobs. Queue, Memory, CPU, and other options: Please enter the queue name and set the default memory and CPU you'll allocate for each job (e.g. 10GB memory and 1CPU). These settings can be adjusted in the run page. Amazon Keys: AWS credentials that are saved in the Amazon Keys tab will allow you to submit jobs to AWS Batch. Default Working Directory: Default directory in the host machine where runs will be executed. (eg. /data/dnext ) Default Bucket Location for Publishing: Default bucket location where dolphinnext reports will be published. (e.g. s3://bucket/dnext )","title":"B. Defining AWS Batch Profiles:"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#c-defining-non-batch-amazon-web-services-profiles","text":"SSH Keys: These are saved in the SSH Keys tab and will be used when connecting to a host. SSH keys are a secure way to authenticate and encrypt connections between servers. It's recommended to generate a new key pair for each instance you plan to connect to. * Amazon Keys: These AWS credentials are saved in the Amazon Keys tab and allow you to start/stop Amazon EC2 instances. These credentials consist of an access key and a secret access key. * Instance Type: This refers to the type of _Amazon EC2 instance that you will use. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. For example, \"m3.xlarge\" is an instance type that provides a balance of CPU and memory resources. * Image ID: This is the virtual machine ID (VM ID) that you will use to launch the instance. The image ID is specific to the region and operating system that you are using. For example, \"ami-032a33ebe57465518\" is a sample image ID for an Amazon Machine Image (AMI) that is based on the Ubuntu operating system. If you want to create your own image, please install the following programs: Singularity Docker engine (version 1.11 or higher) Apache Ignite with Cloud-init package Nextflow AWS CLI Subnet ID/Security Group/Shared Storage ID/Shared Storage Mount: The filesystem needs to be created at https://console.aws.amazon.com/efs/ , and this information will be obtained upon the creation of a shared file system. Subnet ID: Identifier of the VPC subnet to be applied, e.g., subnet-05222a43. Security Group: Identifier of the security group to be applied, e.g., sg-df72b9ba, which is used by default. Shared Storage ID: Identifier of the shared file system instance, e.g., fs-1803efd1. Shared Storage Mount: Mount path of the shared file system, e.g., /mnt/efs. Please make sure the following criteria are satisfied: The image used must have the directory to mount this storage. The output directory must be under this mount location. The storage system must be created in the selected region, and necessary rights need to be given in the console. The EC2FullAccess and S3FullAccess permissions have been added. Warning: Both the EFS and images should be located in the same location. Default Working Directory: This is the default directory in the host machine where runs will be executed. It is an optional parameter for AWS. For example, you can set it to \"/data/dnext\". Default Bucket Location for Publishing: This is the default bucket location where Via Foundry reports will be published. It is also an optional parameter for AWS. For example, you can set it to s3://bucket/dnext . Run Command (optional): You may specify a command or multiple commands to be run before the Nextflow job starts. Separate multiple commands with the && sign. For example, you could use the following command to load modules before running the job: source /etc/bashrc && module load java/1.8.0_31 && module load bowtie2/2.3.2 . Nextflow Path (optional): If the Nextflow path is not added to the $PATH environment variable, you can define the path in this block. For example, you could set it to /project/umw_biocore/bin . Singularity Cache Folder: This is the directory where remote Singularity images are stored. By default, the home directory is used. Note that when using a computing cluster, it must be a shared folder that is accessible from all computing nodes. Profile Variables: You can set commonly used pipeline variables here. For instance, params.DOWNDIR is used in most of our public pipelines to save all genome related files (fasta, index etc.), so you can set this variable like this: params.DOWNDIR = \"/share/dnext_data\" You can also enter multiple variables by separating them with newlines. Executor of Nextflow/Executor of Nextflow Jobs: Amazon instances are automatically configured to use the Ignite executors. As such, when defining an Amazon profile, you should select Local for Executor of Nextflow and Ignite for Executor of Nextflow Jobs.","title":"C. Defining Non-Batch Amazon Web Services Profiles:"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#d-defining-google-profiles","text":"SSH Keys: are saved in SSH keys tab and will be used while connecting to host. Google Keys: Google credentials that are saved in Google keys tab and will allow to start/stop Google Cloud instances. Zone: The Google zone where the computation is executed.(eg. us-east1-b) Instance Type: Google Cloud machine types that comprise varying combinations of CPU, memory, storage, and networking capacity (eg. n1-standard-4 ). Image ID: Virtual machine ID (eg. dolphinnext-193616/global/images/dolphinnext-images-v1 ). If you want to create your own image, please install following programs: Singularity Docker engine (version 1.11 or higher) Apache Ignite with Cloud-init package Nextflow gcloud Default Working Directory: This is the default directory in the host machine where runs will be executed. It is mandatory for Google Cloud profiles. (e.g., /data/dnext) Default Bucket Location for Publishing: This is the default bucket location where Via Foundry reports will be published. It is mandatory for Google Cloud profiles, and you can always edit this path in the run page. For example, you could set it to gs://bucket/dnext . Run Command (optional): You may specify a command or multiple commands to be run before the Nextflow job starts. Separate multiple commands with the && sign. For example, you could use the following command to load modules before running the job: source /etc/bashrc && module load java/1.8.0_31 && module load bowtie2/2.3.2 . Nextflow Path (optional): If the Nextflow path is not added to the $PATH environment variable, you can define the path in this block. For example, you could set it to /project/umw_biocore/bin . Singularity Cache Folder: This is the directory where remote Singularity images are stored. By default, the home directory is used. Note that when using a computing cluster, it must be a shared folder that is accessible from all computing nodes. Profile Variables: You can set commonly used pipeline variables here. For instance, params.DOWNDIR is used in most of our public pipelines to save all genome related files (fasta, index etc.), so you can set this variable like this: params.DOWNDIR = \"/share/dnext_data\" You can also enter multiple variables by separating them with newlines. Executor of Nextflow/Executor of Nextflow Jobs: Google instances are automatically configured to use the Ignite executors. As such, when defining a Google profile, you can select Local for Executor of Nextflow and Ignite for Executor of Nextflow Jobs.","title":"D. Defining Google Profiles:"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#github-connection","text":"You can enter your GitHub or Bitbucket security credentials (Username, E-mail, Password) by clicking on the Add Account button in the Repositories tab. Your information will be encrypted and kept secure. By adding a Github or Bitbucket account, you'll be able to push your pipeline information into a public account, where you can then share it with others.","title":"GitHub Connection"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#change-password","text":"If you're not using a Google sign-in, you can change your Via Foundry password by using this section.","title":"Change Password"},{"location":"vfdocs/vf_docs/ViaFoundry/profile/profile/#support","text":"For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Support"},{"location":"vfdocs/vf_docs/ViaFoundry/project/project/","text":"Project Guide This guide will walk you through creating a project using the Via Foundry platform, and how to insert pipelines and files into it. Creating Projects and Adding Pipelines Projects are platforms to categorize all of your runs and files. To run a pipeline, you must add it into your project. There are two ways to create a new project and add pipelines to it: A. First, click the \"Projects\" button in the top of the screen. From the dropdown menu, you can create a new project by clicking Add a New Project . After saving the name of the project, it will be added to your Projects table, which is accessible from the dropdown menu, as shown in the image below: Note: You can always edit or remove your projects by clicking on the project name from the dropdown menu, navigating to the Settings tab, and clicking the Edit or Delete Project icon. ::: Now you are able to enter the project page by clicking the name of the project from the Projects dropdown menu. You will notice five tabs in the project page: Dashboard: Your project's homepage. The dashboard contains the Description box, where you can write down information about the project; and the Analysis section, which will contain reports on the project and its runs. Data Collections: The hub of whatever data you need to add in order to run your desired pipelines. Add data by clicking the Add Collection button. Pipelines: The pipeline(s) you want available for your project. Click Create Pipeline or Add Pipeline to customize your project's pipeline environment. Runs: Table containing information about all the runs conducted in your current project (see image below). You can enter a specific run's page by clicking on the name of the run. Settings: Information about your project: name, creator, and date/time created. From this tab, you can edit or delete a given project, as mentioned above. B. You can also create a project by clicking the Pipelines button on the top left of the page. Select the pipeline you want to run by clicking on it. At the top right of the new pipeline page, you'll see a Run button; press this to initiate a run. The Run button opens a new window where you can create a new project by clicking Create a Project . After entering and saving the project name, it will be added to your project list. From here, you can select your project by clicking on it, as shown in the image below. You can proceed by entering the run name, which will subsequently be added to your project's run list. Clicking Save Run will redirect you to the \"Run Page\", where you can initiate your run. Support For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Project Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/project/project/#project-guide","text":"This guide will walk you through creating a project using the Via Foundry platform, and how to insert pipelines and files into it.","title":"Project Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/project/project/#creating-projects-and-adding-pipelines","text":"Projects are platforms to categorize all of your runs and files. To run a pipeline, you must add it into your project. There are two ways to create a new project and add pipelines to it: A. First, click the \"Projects\" button in the top of the screen. From the dropdown menu, you can create a new project by clicking Add a New Project . After saving the name of the project, it will be added to your Projects table, which is accessible from the dropdown menu, as shown in the image below: Note: You can always edit or remove your projects by clicking on the project name from the dropdown menu, navigating to the Settings tab, and clicking the Edit or Delete Project icon. ::: Now you are able to enter the project page by clicking the name of the project from the Projects dropdown menu. You will notice five tabs in the project page: Dashboard: Your project's homepage. The dashboard contains the Description box, where you can write down information about the project; and the Analysis section, which will contain reports on the project and its runs. Data Collections: The hub of whatever data you need to add in order to run your desired pipelines. Add data by clicking the Add Collection button. Pipelines: The pipeline(s) you want available for your project. Click Create Pipeline or Add Pipeline to customize your project's pipeline environment. Runs: Table containing information about all the runs conducted in your current project (see image below). You can enter a specific run's page by clicking on the name of the run. Settings: Information about your project: name, creator, and date/time created. From this tab, you can edit or delete a given project, as mentioned above. B. You can also create a project by clicking the Pipelines button on the top left of the page. Select the pipeline you want to run by clicking on it. At the top right of the new pipeline page, you'll see a Run button; press this to initiate a run. The Run button opens a new window where you can create a new project by clicking Create a Project . After entering and saving the project name, it will be added to your project list. From here, you can select your project by clicking on it, as shown in the image below. You can proceed by entering the run name, which will subsequently be added to your project's run list. Clicking Save Run will redirect you to the \"Run Page\", where you can initiate your run.","title":"Creating Projects and Adding Pipelines"},{"location":"vfdocs/vf_docs/ViaFoundry/project/project/#support","text":"For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Support"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/","text":"Quick Start Guide Signing Up This guide will walk you through how to start using Via Foundry (formerly DolphinNext) pipelines. First off, you need to navigate to the Via Foundry web page at https://www.viafoundry.com and click the Log in button. You will be asked to enter your institute's log-in information. An email will be sent to you once your information has been verified. Creating Profile For information on how to set up and customize your Via Foundry profile to your specifications, please reference our Profile Guide . Software Dependencies In order to execute our pipelines, you have to install and validate certain software dependencies into your host machine. To enable proper pipeline execution, Nextflow should be installed into your host environment. Since most of our pipelines isolate their dependencies within their Docker or Singularity containers, please install these softwares into your machine by following the guidelines below. If your platform doesn't support the installation of Docker, you can still use our pipelines with just Singularity. Installing Nextflow Installing Singularity (Version 3) Installing Docker How to Add Software to Your $PATH Environment: JAVA Command (optional): If JAVA is not added to the $PATH environment, you can run the command ( module load java/8.0 ) to manipulate your $PATH environment and gain access to JAVA. Nextflow Path or Command (optional): If Nextflow is not added to the $PATH environment, you can either enter the path of the nextflow (eg. /project/bin ), or run the command ( module load nextflow ) to manipulate your $PATH environment and gain access to new software. Docker/Singularity Command (optional): You can run a command (eg. module load docker/1.0.0 or module load singularity/3.0.0 ) to manipulate your $PATH environment in order to gain access to new software. You can set general run settings by following the Run Environments section: Executor of Nextflow: Nextflow itself is initiated with this method, which will be only used for running Nextflow itself. Executor of Nextflow Jobs: This setting will be used as the default setting for submitted jobs by Nextflow. Download Directory: Used to download shared pipeline files such as genome indexes. If your platform already has an allocated path for such files, please enter that path. Otherwise, you can set any path that you have permission to write. e.g. /share/viafoundry/downloads Once you complete these steps, you're now able to start using publicly available pipelines. Running Pipelines The easiest way to run a pipeline is from the main page, by clicking the Pipelines button at the top left of the screen. From here, you can investigate publicly available pipelines as shown below and select the pipeline you want to run by clicking on it. Once the pipeline is loaded, you will notice a Run button at the right top of the page. Pressing this button opens a new window, where you can create a new project by clicking Create a Project . After you enter and save the name of the project, it will be added to your project list. Now you can select your project by clicking on it, as shown in the figure below. After clicking Select Project , you may proceed with entering your desired run name, which will be added to your project's run list. Clicking Save Run will redirect you to the \"Run Page\". Initially, in the header of the run page, an orange Waiting button will be shown. In order to initiate a run, the following data need to be entered: A. Work Directory: Full path of the directory from whence Nextflow runs will be executed. B. Run Environment: The environment, discussed in the Profile page, within which you'd like to conduct your run. If an Amazon profile or a Google profile is selected, then the status of the profile should be \" Running \". C. Inputs: Various values and filepaths, specifying what data will be processed and how, need to be entered. For additional information, please check the Adding Files section. Once all requirements are satisfied, the Waiting button will turn into a green Run button as shown below. You can initiate your run by clicking the Run button. Please go through the Run Guide for detailed explanation about each module is used. Adding Files Remote Files You can reach your remote files by entering: Full path of a directory: eg. /share/data/umw_biocore/genome_data/mousetest/mm10/gz Web link: eg. https://galaxyweb.umassmed.edu/pub/dnext_data/test/reads Amazon (S3) Bucket: eg. s3://biocore/fastq Google (GS) Bucket: eg. gs://biocore/fastq Geo Files If you want to download and use NCBI (GEO data) in the pipeline, you can simply use the GEO Files tab. Here are the few examples for GEO ID: GSM1331276 , GSE55190 , SRR10095965 Upload Files If you need to upload your local files and transfer into Target Directory in the Host , you can use the Upload Files tab. How To Cite Us If you use Via Foundry (formerly DolphinNext) in your research, please cite: Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x Support For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Quick Start Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#quick-start-guide","text":"","title":"Quick Start Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#signing-up","text":"This guide will walk you through how to start using Via Foundry (formerly DolphinNext) pipelines. First off, you need to navigate to the Via Foundry web page at https://www.viafoundry.com and click the Log in button. You will be asked to enter your institute's log-in information. An email will be sent to you once your information has been verified.","title":"Signing Up"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#creating-profile","text":"For information on how to set up and customize your Via Foundry profile to your specifications, please reference our Profile Guide .","title":"Creating Profile"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#software-dependencies","text":"In order to execute our pipelines, you have to install and validate certain software dependencies into your host machine. To enable proper pipeline execution, Nextflow should be installed into your host environment. Since most of our pipelines isolate their dependencies within their Docker or Singularity containers, please install these softwares into your machine by following the guidelines below. If your platform doesn't support the installation of Docker, you can still use our pipelines with just Singularity. Installing Nextflow Installing Singularity (Version 3) Installing Docker How to Add Software to Your $PATH Environment: JAVA Command (optional): If JAVA is not added to the $PATH environment, you can run the command ( module load java/8.0 ) to manipulate your $PATH environment and gain access to JAVA. Nextflow Path or Command (optional): If Nextflow is not added to the $PATH environment, you can either enter the path of the nextflow (eg. /project/bin ), or run the command ( module load nextflow ) to manipulate your $PATH environment and gain access to new software. Docker/Singularity Command (optional): You can run a command (eg. module load docker/1.0.0 or module load singularity/3.0.0 ) to manipulate your $PATH environment in order to gain access to new software. You can set general run settings by following the Run Environments section: Executor of Nextflow: Nextflow itself is initiated with this method, which will be only used for running Nextflow itself. Executor of Nextflow Jobs: This setting will be used as the default setting for submitted jobs by Nextflow. Download Directory: Used to download shared pipeline files such as genome indexes. If your platform already has an allocated path for such files, please enter that path. Otherwise, you can set any path that you have permission to write. e.g. /share/viafoundry/downloads Once you complete these steps, you're now able to start using publicly available pipelines.","title":"Software Dependencies"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#running-pipelines","text":"The easiest way to run a pipeline is from the main page, by clicking the Pipelines button at the top left of the screen. From here, you can investigate publicly available pipelines as shown below and select the pipeline you want to run by clicking on it. Once the pipeline is loaded, you will notice a Run button at the right top of the page. Pressing this button opens a new window, where you can create a new project by clicking Create a Project . After you enter and save the name of the project, it will be added to your project list. Now you can select your project by clicking on it, as shown in the figure below. After clicking Select Project , you may proceed with entering your desired run name, which will be added to your project's run list. Clicking Save Run will redirect you to the \"Run Page\". Initially, in the header of the run page, an orange Waiting button will be shown. In order to initiate a run, the following data need to be entered: A. Work Directory: Full path of the directory from whence Nextflow runs will be executed. B. Run Environment: The environment, discussed in the Profile page, within which you'd like to conduct your run. If an Amazon profile or a Google profile is selected, then the status of the profile should be \" Running \". C. Inputs: Various values and filepaths, specifying what data will be processed and how, need to be entered. For additional information, please check the Adding Files section. Once all requirements are satisfied, the Waiting button will turn into a green Run button as shown below. You can initiate your run by clicking the Run button. Please go through the Run Guide for detailed explanation about each module is used.","title":"Running Pipelines"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#adding-files","text":"","title":"Adding Files"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#remote-files","text":"You can reach your remote files by entering: Full path of a directory: eg. /share/data/umw_biocore/genome_data/mousetest/mm10/gz Web link: eg. https://galaxyweb.umassmed.edu/pub/dnext_data/test/reads Amazon (S3) Bucket: eg. s3://biocore/fastq Google (GS) Bucket: eg. gs://biocore/fastq","title":"Remote Files"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#geo-files","text":"If you want to download and use NCBI (GEO data) in the pipeline, you can simply use the GEO Files tab. Here are the few examples for GEO ID: GSM1331276 , GSE55190 , SRR10095965","title":"Geo Files"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#upload-files","text":"If you need to upload your local files and transfer into Target Directory in the Host , you can use the Upload Files tab.","title":"Upload Files"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#how-to-cite-us","text":"If you use Via Foundry (formerly DolphinNext) in your research, please cite: Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x","title":"How To Cite Us"},{"location":"vfdocs/vf_docs/ViaFoundry/quick/quick/#support","text":"For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Support"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/","text":"Run Guide In the previous tutorial ( Project Guide ), we went through the process of creating a project within the Via Foundry (formerly DolphinNext) platform and adding pipelines to it. In this guide, we will look through all the relevant run settings needed to initiate a new run. Brief Refresher To access a run page, navigate to the project the run is housed within by clicking on the appropriate project name in the Project dropdown menu. On the Dashboard page, under the Analysis section, click the name of your run. Basics On the run page's header, you can see the names of the current project, pipeline being used, and run in progress. Ensure that you're in the right project and running the correct pipeline before proceeding. As you can see, Save Run , Download Pipeline , and Delete Run icons are conveniently placed next to the information mentioned above in the run page's header. Additionally, you can find Delete Run , Duplicate Run , and Move Run options by clicking on the three dots next to the status indicator. Run Status The status of your current run is displayed at the far right of the run page's header. Initially, you'll see an orange Waiting button. In order to initiate a run, the following data need to be entered: Work Directory: Full path of the directory where Nextflow runs will be executed. Run Environment: The environment, discussed in the profile page, within which you'd like to conduct your run. If an Amazon profile or a Google profile is selected, then the status of the profile should be Running . Inputs: Various values and filepaths, specifying which data will be processed and how (i.e. whether single-end vs. paired-end data are being used), need to be entered in the Run Settings page. For additional information, please check the Adding Files section. More details on the data that must be entered prior to a run can be found in the Run Settings section of this page. All possible status messages are listed here: Status Meaning Waiting Waiting for inputs, output directory and selection of active run environment Ready Ready to initiate run Connecting Sending SSH queries to selected host system Waits Job is submitted, waiting for run execution Running Nextflow has executed and is running the jobs. Completed The Nextflow job is completed. Run Error Error occurred before submitting the jobs or while executing the jobs. Terminated User terminated the run by using the Terminate Run button. Run Settings Run Environment: The environment, discussed in the profile page, within which you'd like to conduct your run. Edit this via the dropdown menu on the Run Settings page. Inputs: A suite of settings governing how your data will be processed during the run. Enter information like the path of your reads and whether you want to run various data-filtering packages by clicking on the appropriate dropdown menus. Work Directory: Full path of the directory where Nextflow runs will be executed. Example path: /home/newuser/workdir Advanced Options Run Command (optional): You may run the command or commands (by seperating each command with && sign) before the nextflow job starts, e.g.: source /etc/bashrc && module load java/1.8.0_31 && module load bowtie2/2.3.2 Publish Directory: The Work Directory also serves as the default directory to which output files are sent for Via Foundry runs. If you want to change the path to a different directory, just enter the full path of your desired Publish Directory in this box. Local paths (eg. /home/user/test ), Amazon S3 paths (eg. s3://yourbucket/test ) or Google Storage paths (eg. gs://yourbucket/test ) are all accepted. Use Docker Image: Nextflow supports the use of Docker containers, which allow you to create fully reproducible pipelines. Docker images can contain whatever software you might need to execute your pipeline. It works transparently, and it creates output files in the host system without requiring any additional steps. The only requirement is that you install Docker first. To use Docker for a Via Foundry run, just click the Use Docker Image checkbox (selected by default) and enter the following information: Image: Docker image name. Example: public.ecr.aws/t4w5x8f2/viascientific/rnaseq:4.0 RunOptions (optional): You can enter any command line arguments supported by the Docker run command. Please click this Docker link for details on how you can configure this section. Use Singularity Image: Instead of Docker, you can activate a Singularity image if you wish by clicking the Use Singularity Image checkbox and entering the relevant information, expounded upon below. In order to use a Singularity image, you must first install Singularity . Image: Path to your desired Singularity image. For example: shub://UMMS-biocore/singularitysc /project/umw_biocore/singularity/UMMS-Biocore-singularity-master.simg RunOptions (optional): You can enter any command line options supported by the Singularity exec . For instance, you can mount directories by using --bind command . See below for an example of the bind command, and please click this link for more details about the command line arguments Singularity supports. > --bind /project:/project --bind /nl:/nl --bind > /share:/share Tip: Mounting directories in Singularity requires you to create the directories in the image beforehand. Executor Settings: A series of parameters governing the execution of your run, including what packages to run and how much processing power to allocate to each package. 1. Executor Settings for Nextflow (navigate to Profile --> Run Environments --> Edit Run Environment) : You can determine the system where Nextflow itself is initiated. Currently, Via Foundry supports the initiation of Nextflow via Local, SGE, SLURM and LSF executors, which will be only used for running Nextflow itself. Suggested parameters: long 8GB 1CPU 5000-8000min 2. Executor of Nextflow Jobs (navigate to Profile --> Run Environments --> Edit Run Environment) : This setting will be used if you don't manually set any parameters in the Advanced section of your run page. If any option other than Local is selected, you'll be prompted to input values for Queue , Memory(GB) , CPU and Time(min.) . You can adjust these parameters as you wish. Suggested parameters: short 20GB 1CPU 240min 3. Executor Settings for All Processes (in Advanced tab of run page) : This setting will overwrite the parameters entered in \"Executor of Nextflow Jobs\". Suggested parameters: short 20GB 1CPU 240min 4. Executor Settings for Each Process (in Advanced tab of run page) : If a particular process needs special parameters other than Executor settings for all processes , you can override the default parameters by clicking on the checkbox corresponding to the process that you want to change. This will only affect the settings of the selected process while retaining the original settings for all other processes. Suggested parameters: long 20GB 4CPU 1000-5000min Delete intermediate files after run: By default, Via Foundry deletes any non-output files created during a run, only retaining the necessary output files in the Work/Publish Directories. This setting is aimed at minimizing the storage required for a project, but you can uncheck the box to keep all intermediate files. Permissions and Groups: By default, all new runs can only be seen by their owner. However, you can share your run with a group by changing permissions to \"Only my group\" and choose the group you want to share with from the Group Selection dropdown. Workflow To provide a visualization of the current run's architecture, the selected pipeline and its modules are showed on this page. To see more information about the pipeline's settings, click the Go to Pipeline link at the top of this page. Run Logs This section keeps track of each run. You can monitor each stage of the run both before and after Nextflow execution, as shown here: You can view various log files, such as timeline.html, dag.html, trace.txt, .nextflow.log, nextflow.nf, nextflow.config, as shown here: timeline.html: dag.html: trace.txt: .nextflow.log: nextflow.nf: nextflow.config: If an error occurred at any point during the run, a detailed explanation about the error will be displayed here, and the status of the run will change to Run Error . Report This tab will appear in the run page upon run initialization. You can view the output files in various modules such as R-Markdown, Datatables, Highcharts, HTML or PDF Viewer. For reference, check the example Report section of an RSEM pipeline at below. Each report row corresponds to an output parameter in the pipeline's workflow, and you can easily see a row's content by clicking on it. All these sections have Download , Full Screen , and Open in New Window icons to help you best analyze each report. Note: If you want to integrate your own visualization tool into Via Foundry, please let us know about it at support@viascientific.com , and we'd be happy to add it for you. DEBrowser: DEBrowser is an R library which provides an easy way to perform and visualize DE (Differential Expression) analysis. This module takes count matrices as input and allows interactive exploration of the resulting data. You can find their documentation here . R-Markdown: The R-Markdown feature enables interactive analysis of the newly-produced data from a run. We have prepared a series of R-Markdown reports, which will allow you to reach your report in an HTML or PDF format as soon as your run completes. Within an R-Markdown (.rmd) file, R code chunks can be embedded with the native Markdown syntax for fenced code regions. For example, the following code chunk computes a data histogram and renders a bar plot as a PNG image: For more information about R-Markdown, click rmarkdown link . At the top of the R-Markdown module, you can find various icons that will help you edit your .rmd file, save it as a new file, and download it in various formats such as RMD, PDF or HTML. In order to facilitate the review process, you can click \"full screen\" icon to fit the application in your screen. You can also adjust the Auto Updating Output and Autosave features, explained below, by clicking the Settings icon. Auto Updating Output: If enabled, the preview panel updates automatically as you code. If disabled, use the Run Script button to update the preview panel. Autosave: If enabled, Via Foundry will automatically save the file's content every 30 seconds. Datatables: This module, powered by Datatables , allows you to view, sort, and search the table's content. The following two examples depict alignment and RSEM summaries within Datatables. Alignment Summary: RSEM Summary: You can fit the entire table in your screen by clicking the Full screen icon at the top of the module. HTML Viewer: You can easily embed HTML content in our Report section by using HTML Viewer. Reference this image, which shows MultiQC output, for an example: PDF Viewer: Similar to HTML Viewer, PDF files can be embedded in the Report section. You can see the piPipes report as an example here: Support For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Run Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#run-guide","text":"In the previous tutorial ( Project Guide ), we went through the process of creating a project within the Via Foundry (formerly DolphinNext) platform and adding pipelines to it. In this guide, we will look through all the relevant run settings needed to initiate a new run.","title":"Run Guide"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#brief-refresher","text":"To access a run page, navigate to the project the run is housed within by clicking on the appropriate project name in the Project dropdown menu. On the Dashboard page, under the Analysis section, click the name of your run.","title":"Brief Refresher"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#basics","text":"On the run page's header, you can see the names of the current project, pipeline being used, and run in progress. Ensure that you're in the right project and running the correct pipeline before proceeding. As you can see, Save Run , Download Pipeline , and Delete Run icons are conveniently placed next to the information mentioned above in the run page's header. Additionally, you can find Delete Run , Duplicate Run , and Move Run options by clicking on the three dots next to the status indicator.","title":"Basics"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#run-status","text":"The status of your current run is displayed at the far right of the run page's header. Initially, you'll see an orange Waiting button. In order to initiate a run, the following data need to be entered: Work Directory: Full path of the directory where Nextflow runs will be executed. Run Environment: The environment, discussed in the profile page, within which you'd like to conduct your run. If an Amazon profile or a Google profile is selected, then the status of the profile should be Running . Inputs: Various values and filepaths, specifying which data will be processed and how (i.e. whether single-end vs. paired-end data are being used), need to be entered in the Run Settings page. For additional information, please check the Adding Files section. More details on the data that must be entered prior to a run can be found in the Run Settings section of this page. All possible status messages are listed here: Status Meaning Waiting Waiting for inputs, output directory and selection of active run environment Ready Ready to initiate run Connecting Sending SSH queries to selected host system Waits Job is submitted, waiting for run execution Running Nextflow has executed and is running the jobs. Completed The Nextflow job is completed. Run Error Error occurred before submitting the jobs or while executing the jobs. Terminated User terminated the run by using the Terminate Run button.","title":"Run Status"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#run-settings","text":"Run Environment: The environment, discussed in the profile page, within which you'd like to conduct your run. Edit this via the dropdown menu on the Run Settings page. Inputs: A suite of settings governing how your data will be processed during the run. Enter information like the path of your reads and whether you want to run various data-filtering packages by clicking on the appropriate dropdown menus. Work Directory: Full path of the directory where Nextflow runs will be executed. Example path: /home/newuser/workdir","title":"Run Settings"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#advanced-options","text":"Run Command (optional): You may run the command or commands (by seperating each command with && sign) before the nextflow job starts, e.g.: source /etc/bashrc && module load java/1.8.0_31 && module load bowtie2/2.3.2 Publish Directory: The Work Directory also serves as the default directory to which output files are sent for Via Foundry runs. If you want to change the path to a different directory, just enter the full path of your desired Publish Directory in this box. Local paths (eg. /home/user/test ), Amazon S3 paths (eg. s3://yourbucket/test ) or Google Storage paths (eg. gs://yourbucket/test ) are all accepted. Use Docker Image: Nextflow supports the use of Docker containers, which allow you to create fully reproducible pipelines. Docker images can contain whatever software you might need to execute your pipeline. It works transparently, and it creates output files in the host system without requiring any additional steps. The only requirement is that you install Docker first. To use Docker for a Via Foundry run, just click the Use Docker Image checkbox (selected by default) and enter the following information: Image: Docker image name. Example: public.ecr.aws/t4w5x8f2/viascientific/rnaseq:4.0 RunOptions (optional): You can enter any command line arguments supported by the Docker run command. Please click this Docker link for details on how you can configure this section. Use Singularity Image: Instead of Docker, you can activate a Singularity image if you wish by clicking the Use Singularity Image checkbox and entering the relevant information, expounded upon below. In order to use a Singularity image, you must first install Singularity . Image: Path to your desired Singularity image. For example: shub://UMMS-biocore/singularitysc /project/umw_biocore/singularity/UMMS-Biocore-singularity-master.simg RunOptions (optional): You can enter any command line options supported by the Singularity exec . For instance, you can mount directories by using --bind command . See below for an example of the bind command, and please click this link for more details about the command line arguments Singularity supports. > --bind /project:/project --bind /nl:/nl --bind > /share:/share Tip: Mounting directories in Singularity requires you to create the directories in the image beforehand. Executor Settings: A series of parameters governing the execution of your run, including what packages to run and how much processing power to allocate to each package. 1. Executor Settings for Nextflow (navigate to Profile --> Run Environments --> Edit Run Environment) : You can determine the system where Nextflow itself is initiated. Currently, Via Foundry supports the initiation of Nextflow via Local, SGE, SLURM and LSF executors, which will be only used for running Nextflow itself. Suggested parameters: long 8GB 1CPU 5000-8000min 2. Executor of Nextflow Jobs (navigate to Profile --> Run Environments --> Edit Run Environment) : This setting will be used if you don't manually set any parameters in the Advanced section of your run page. If any option other than Local is selected, you'll be prompted to input values for Queue , Memory(GB) , CPU and Time(min.) . You can adjust these parameters as you wish. Suggested parameters: short 20GB 1CPU 240min 3. Executor Settings for All Processes (in Advanced tab of run page) : This setting will overwrite the parameters entered in \"Executor of Nextflow Jobs\". Suggested parameters: short 20GB 1CPU 240min 4. Executor Settings for Each Process (in Advanced tab of run page) : If a particular process needs special parameters other than Executor settings for all processes , you can override the default parameters by clicking on the checkbox corresponding to the process that you want to change. This will only affect the settings of the selected process while retaining the original settings for all other processes. Suggested parameters: long 20GB 4CPU 1000-5000min Delete intermediate files after run: By default, Via Foundry deletes any non-output files created during a run, only retaining the necessary output files in the Work/Publish Directories. This setting is aimed at minimizing the storage required for a project, but you can uncheck the box to keep all intermediate files. Permissions and Groups: By default, all new runs can only be seen by their owner. However, you can share your run with a group by changing permissions to \"Only my group\" and choose the group you want to share with from the Group Selection dropdown.","title":"Advanced Options"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#workflow","text":"To provide a visualization of the current run's architecture, the selected pipeline and its modules are showed on this page. To see more information about the pipeline's settings, click the Go to Pipeline link at the top of this page.","title":"Workflow"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#run-logs","text":"This section keeps track of each run. You can monitor each stage of the run both before and after Nextflow execution, as shown here: You can view various log files, such as timeline.html, dag.html, trace.txt, .nextflow.log, nextflow.nf, nextflow.config, as shown here: timeline.html: dag.html: trace.txt: .nextflow.log: nextflow.nf: nextflow.config: If an error occurred at any point during the run, a detailed explanation about the error will be displayed here, and the status of the run will change to Run Error .","title":"Run Logs"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#report","text":"This tab will appear in the run page upon run initialization. You can view the output files in various modules such as R-Markdown, Datatables, Highcharts, HTML or PDF Viewer. For reference, check the example Report section of an RSEM pipeline at below. Each report row corresponds to an output parameter in the pipeline's workflow, and you can easily see a row's content by clicking on it. All these sections have Download , Full Screen , and Open in New Window icons to help you best analyze each report. Note: If you want to integrate your own visualization tool into Via Foundry, please let us know about it at support@viascientific.com , and we'd be happy to add it for you. DEBrowser: DEBrowser is an R library which provides an easy way to perform and visualize DE (Differential Expression) analysis. This module takes count matrices as input and allows interactive exploration of the resulting data. You can find their documentation here . R-Markdown: The R-Markdown feature enables interactive analysis of the newly-produced data from a run. We have prepared a series of R-Markdown reports, which will allow you to reach your report in an HTML or PDF format as soon as your run completes. Within an R-Markdown (.rmd) file, R code chunks can be embedded with the native Markdown syntax for fenced code regions. For example, the following code chunk computes a data histogram and renders a bar plot as a PNG image: For more information about R-Markdown, click rmarkdown link . At the top of the R-Markdown module, you can find various icons that will help you edit your .rmd file, save it as a new file, and download it in various formats such as RMD, PDF or HTML. In order to facilitate the review process, you can click \"full screen\" icon to fit the application in your screen. You can also adjust the Auto Updating Output and Autosave features, explained below, by clicking the Settings icon. Auto Updating Output: If enabled, the preview panel updates automatically as you code. If disabled, use the Run Script button to update the preview panel. Autosave: If enabled, Via Foundry will automatically save the file's content every 30 seconds. Datatables: This module, powered by Datatables , allows you to view, sort, and search the table's content. The following two examples depict alignment and RSEM summaries within Datatables. Alignment Summary: RSEM Summary: You can fit the entire table in your screen by clicking the Full screen icon at the top of the module. HTML Viewer: You can easily embed HTML content in our Report section by using HTML Viewer. Reference this image, which shows MultiQC output, for an example: PDF Viewer: Similar to HTML Viewer, PDF files can be embedded in the Report section. You can see the piPipes report as an example here:","title":"Report"},{"location":"vfdocs/vf_docs/ViaFoundry/run/run/#support","text":"For any questions or help, please reach out to support@viascientific.com with your name and question.","title":"Support"}]}