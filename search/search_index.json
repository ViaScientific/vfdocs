{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-via-foundrys-documentation","title":"Welcome to Via Foundry's Documentation!","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Platform Overview</li> <li>Pipeline Examples</li> <li>About</li> <li>How To Cite Us</li> <li>Frequently Asked Questions</li> <li>News</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li>Running RNA-Seq Pipeline</li> <li>Running Cellranger Pipeline</li> <li>Developer Tutorial - Creating Pipeline</li> <li>Metadata Tracker - Basics</li> <li>Metadata Tracker - NCBI-GEO</li> <li>Metadata Portal</li> <li>Run Submission through API</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<ul> <li>Quick Start Guide</li> <li>Project Guide</li> <li>Run Guide</li> <li>App Guide</li> <li>Metadata Guide</li> <li>Profile Guide</li> <li>Frequently Asked Questions</li> </ul>"},{"location":"#developer-guide","title":"Developer Guide","text":"<ul> <li>Pipeline Guide</li> <li>Process Guide</li> <li>App Guide</li> <li>API Guide</li> <li>Frequently Asked Questions</li> </ul>"},{"location":"#app-library","title":"App Library","text":"<ul> <li>GSEA Explorer</li> <li>Network Explorer</li> </ul>"},{"location":"#administrator-guide","title":"Administrator Guide","text":"<ul> <li>Installation</li> <li>Configuration</li> <li>Testing Pipeline</li> <li>App Section</li> <li>Browser Support</li> </ul>"},{"location":"ViaFoundry/about/","title":"About","text":""},{"location":"ViaFoundry/about/#about","title":"About","text":"<p>Via Foundry, formerly known as DolphinNext, is developed by the Bioinformatics Core at the University of Massachusetts Medical School (UMMS).</p> <p>You can reach our website at https://www.viafoundry.com.</p>"},{"location":"ViaFoundry/about/#how-to-cite-us","title":"How To Cite Us","text":"<p>If you use Via Foundry (formerly DolphinNext) in your research, please cite:</p> <p>Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x</p>"},{"location":"ViaFoundry/admin-browser-support/","title":"Browser Support","text":""},{"location":"ViaFoundry/admin-browser-support/#via-foundry-supported-browsers","title":"Via Foundry Supported browsers","text":"<p>Last Updated: December 1, 2023</p>"},{"location":"ViaFoundry/admin-browser-support/#via-foundry-officially-supports-the-following-browsers-on-windows-and-macos-desktop","title":"Via Foundry officially supports the following browsers on Windows and macOS desktop:","text":"<ul> <li>Google Chrome (current version)</li> <li>Firefox (current version)</li> <li>Microsoft Edge (current version). You might see performance degradation for some features on Microsoft Edge.</li> </ul>"},{"location":"ViaFoundry/admin-browser-support/#the-following-browsers-are-not-officially-supported","title":"The following browsers are not officially supported:","text":"<ul> <li>Safari</li> <li>Internet Explorer</li> <li>Mobile browsers</li> <li>Beta, \"preview\", or other pre-release versions of desktop browsers</li> </ul> <p>Using an unsupported browser might cause unexpected behavior, including security issues. Via Foundry can assist only in those support cases where an officially supported browser is being used.</p>"},{"location":"ViaFoundry/admin-configuration/","title":"Configuration","text":""},{"location":"ViaFoundry/admin-configuration/#configuration","title":"Configuration","text":""},{"location":"ViaFoundry/admin-configuration/#configuration-of-amazon-rdscloud-sql-for-mysql-optional","title":"Configuration of Amazon RDS/Cloud SQL for MySQL (optional):","text":"<p>Create <code>vpipe</code> database and load initial database file with following commands:</p> <pre><code>mysql -h hostname -u root -p -e 'CREATE DATABASE IF NOT EXISTS vpipe;'\nmysql -h hostname -u root -p vpipe &lt; /export/vpipe/db/db.sql\nmysql -h hostname -u root -p -e \"CREATE USER 'viafoundry'@'%' IDENTIFIED BY 'change_password';\"\nmysql -h hostname -u root -p -e \"GRANT ALL PRIVILEGES ON vpipe.* TO 'viafoundry'@'%';\"\n</code></pre> <p>After that please update DBUSER, DBPASS, DBHOST and DBPORT parameters in <code>/export/vpipe/config/.sec</code> file  and then execute the following command:</p> <pre><code>cd /export/vpipe/scripts &amp;&amp; python updateDN.py\n</code></pre>"},{"location":"ViaFoundry/admin-configuration/#configuration-of-vpipe","title":"Configuration of Vpipe:","text":"<p>Vpipe configuration file found in /export/vpipe/config/.sec file.</p> <p>By default, localhost:8080 is used for domain name. Please use static IP address or domain name and please update BASE_PATH and PUBWEB_URL as follows:</p> <pre><code>BASE_PATH=http://localhost:8080/vpipe\nPUBWEB_URL=http://localhost:8080/vpipe/tmp/pub\n</code></pre> <p>to</p> <pre><code>BASE_PATH=https://your_domain/vpipe\nPUBWEB_URL=https://your_domain/vpipe/tmp/pub\n</code></pre> <p>Other important parameters: </p><pre><code>[CONFIG]\nAUTH_RUN_FILES=true\nAUTH_RUN_FILES_ALLOWED_DOMAINS=your_domain\n</code></pre> <p>All of the configuration directive details can be found here</p> <p>Note: If you were using Standalone Vpipe (or DolphinNext) before, please check the SALT and PEPPER config parameters in /export/vpipe/config/.sec file. These values should be the same for /export/vsso/config.env. So, you need to update /export/vsso/config.env with Vpipe parameters.</p>"},{"location":"ViaFoundry/admin-configuration/#configuration-of-vsso","title":"Configuration of Vsso:","text":"<p>Vmeta configuration file found in /export/vsso/config.env.</p> <p>By default, localhost:3000 is used for domain name. Please use static IP address or domain name and please update BASE_URL as follows:</p> <pre><code>BASE_URL=http://localhost:3000/vsso\n</code></pre> <p>to</p> <pre><code>BASE_URL=https://your_domain/vsso\n</code></pre> <p>All of the configuration directive details can be found here</p>"},{"location":"ViaFoundry/admin-configuration/#configuration-of-vmeta","title":"Configuration of Vmeta:","text":"<p>Vmeta configuration file found in /export/vmeta/config.env.</p> <p>By default, localhost:4000 is used for domain name. Please use static IP address or domain name and please update BASE_URL, SSO_REDIRECT_URL, SSO_USER_INFO_URL and SSO_URL as follows:</p> <pre><code>BASE_URL=http://localhost:4000/vmeta\nSSO_URL=http://localhost:3000/vsso\nSSO_REDIRECT_URL=\"http://localhost:4000/vmeta/receivetoken\"\nSSO_USER_INFO_URL=\"http://localhost:3000/vsso/api/v1/users/info\"\n</code></pre> <p>to</p> <pre><code>BASE_URL=https://your_domain/vmeta\nSSO_URL=https://your_domain/vsso\nSSO_REDIRECT_URL=\"https://your_domain/vmeta/receivetoken\"\nSSO_USER_INFO_URL=\"https://your_domain/vsso/api/v1/users/info\"\n</code></pre> <p>All of the configuration directive details can be found here</p>"},{"location":"ViaFoundry/admin-configuration/#configuration-of-vfoundry","title":"Configuration of Vfoundry:","text":"<p>Vfoundry configuration file found in /export/vfoundry/config.env.</p> <p>By default, localhost:9000 is used for domain name. Please use static IP address or domain name and please update BASE_URL, SSO_REDIRECT_URL, SSO_USER_INFO_URL and SSO_URL as follows:</p> <pre><code>BASE_URL=http://localhost:9000\nSSO_URL=http://localhost:3000/vsso\nSSO_REDIRECT_URL=\"http://localhost:9000/receivetoken\"\nSSO_USER_INFO_URL=\"http://localhost:3000/vsso/api/v1/users/info\"\n</code></pre> <p>to</p> <pre><code>BASE_URL=https://your_domain\nSSO_URL=https://your_domain/vsso\nSSO_REDIRECT_URL=\"https://your_domain/receivetoken\"\nSSO_USER_INFO_URL=\"https://your_domain/vsso/api/v1/users/info\"\n</code></pre> <p>All of the configuration directive details can be found here</p>"},{"location":"ViaFoundry/admin-configuration/#configuration-of-vportal","title":"Configuration of Vportal:","text":"<p>Vportal configuration file found in /export/vportal/config.env.</p> <p>By default, localhost:8000 is used for domain name. Please use static IP address or domain name and please update BASE_URL, SSO_REDIRECT_URL, SSO_USER_INFO_URL and SSO_URL as follows:</p> <pre><code>BASE_URL=http://localhost:8000\nSSO_URL=http://localhost:3000/vsso\nSSO_REDIRECT_URL=\"http://localhost:8000/receivetoken\"\nSSO_USER_INFO_URL=\"http://localhost:3000/vsso/api/v1/users/info\"\n</code></pre> <p>to</p> <pre><code>BASE_URL=https://your_domain/vportal\nSSO_URL=https://your_domain/vsso\nSSO_REDIRECT_URL=\"https://your_domain/vportal/receivetoken\"\nSSO_USER_INFO_URL=\"https://your_domain/vsso/api/v1/users/info\"\n</code></pre> <p>All of the configuration directive details can be found here</p>"},{"location":"ViaFoundry/admin-configuration/#configuration-of-okta","title":"Configuration of OKTA:","text":"<p>If you're integrating OKTA for user authentication, you can use the SAML method. Below are the URLs and settings you'll need to input in your OKTA configuration:</p> <pre><code>Single Sign-On URL: https://viafoundry.{hostname}/vsso/auth/saml/callback\nRecipient URL: https://viafoundry.{hostname}/vsso/auth/saml/callback\nDestination URL: https://viafoundry.{hostname}/vsso/auth/saml/callback\nAudience Restriction: viafoundry-{ViaFoundryID}\nDefault Relay State: https://viafoundry.{hostname}/vsso/auth/saml\nSAML Issuer ID: http://www.okta.com/${org.externalKey}\n</code></pre> <p>Make sure to replace <code>{hostname}</code> with your actual server's hostname. <code>{ViaFoundryID}</code> value will be sent by Via Scientific Team.</p>"},{"location":"ViaFoundry/admin-configuration/#sending-user-attributes","title":"Sending User Attributes","text":"<p>In your OKTA setup, configure it to send the user's first name (<code>firstName</code>) and last name (<code>lastName</code>) when they log in. Here is the example: </p> <p></p>"},{"location":"ViaFoundry/admin-configuration/#foundry-logo-for-okta-dashboard","title":"Foundry Logo for OKTA dashboard","text":"<p>In your OKTA setup, you can use following image for app logo.</p> <p></p>"},{"location":"ViaFoundry/admin-configuration/#metadata-file","title":"Metadata File","text":"<p>Download the <code>metadata.xml</code> file from OKTA and place it in the specified location <code>SSO_SAML_METADATA</code>.</p>"},{"location":"ViaFoundry/admin-configuration/#configuration-file-for-okta","title":"Configuration File for OKTA","text":"<p>Finally, update your configuration file located at <code>/export/vsso/config.env</code> with the following parameters:</p> <pre><code>OKTA_SAML_LOGIN=true\nSSO_ISSUER=viafoundry-{ViaFoundryID}\nSSO_SAML_METADATA=/export/vsso/certs/metadata.xml\nSSO_SAML_DESTINATION_URL=https://viafoundry.{hostname}\n</code></pre> <p>Here, replace <code>{hostname}</code> with your server's hostname and <code>{viafoundryID}</code> with the ID that is sent by Via Scientific Team.</p>"},{"location":"ViaFoundry/admin-configuration/#configuration-of-microsoft-active-directory","title":"Configuration of Microsoft Active Directory:","text":"<p>If you're integrating Microsoft Active Directory for user authentication, you can use the SAML method. Below are the URLs and settings you'll need to input in your Microsoft Active Directory configuration:</p> <pre><code>Indentifier(Entity ID): https://viafoundry.{hostname}/vsso/auth/saml/callback\nReply URL: https://viafoundry.{hostname}/vsso/auth/saml/callback\nSign On URL: https://viafoundry.{hostname}/vsso/auth/saml/callback\nRelay State (Optional): https://viafoundry.{hostname}\nLogout Url (Optional):\n</code></pre> <p>Make sure to replace <code>{hostname}</code> with your actual server's hostname.</p>"},{"location":"ViaFoundry/admin-configuration/#metadata-file_1","title":"Metadata File","text":"<p>Download the <code>metadata.xml</code> file from Microsoft Entra and place it in the specified location <code>SSO_SAML_METADATA</code>.</p>"},{"location":"ViaFoundry/admin-configuration/#configuration-file-for-microsoft-active-directory","title":"Configuration File for Microsoft Active Directory","text":"<p>Finally, update your configuration file located at <code>/export/vsso/config.env</code> with the following parameters:</p> <pre><code>OKTA_SAML_LOGIN=true\nSSO_ISSUER=https://viafoundry.{hostname}/vsso/auth/saml/callback\nSSO_SAML_METADATA=/export/vsso/certs/metadata.xml\nSSO_SAML_DESTINATION_URL=https://viafoundry.{hostname}\nSSO_SAML_WANT_AUTHN_RESPONSE_SIGNED=false\n</code></pre>"},{"location":"ViaFoundry/admin-configuration/#apache-configuration-for-the-foundry-server","title":"Apache Configuration for the Foundry Server:","text":"<p>To configure Apache, you need to enable the mod_ssl and mod_proxy modules. Please follow the instructions below:</p> <ol> <li> <p>Create certificate files (SSLCertificateFile and SSLCertificateKeyFile) in PEM format. SSLCertificateChainFile file is optional.</p> </li> <li> <p>Save the following text into /etc/apache2/sites-enabled/viafoundry.conf file </p><pre><code>&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost *:443&gt;\n        ServerAdmin Your_Email\n        ServerName Your_Domain.com\n        ServerAlias Your_Domain.com\n        RewriteEngine On\n\n        # Some rewrite rules in this file were disabled on your HTTPS site,\n        # because they have the potential to create redirection loops.\n        SSLCertificateFile /etc/letsencrypt/live/Your_Domain.com/cert.pem\n        SSLCertificateKeyFile /etc/letsencrypt/live/Your_Domain.com/privkey.pem\n        SSLCertificateChainFile /etc/letsencrypt/live/Your_Domain.com/chain.pem\n        SSLProxyEngine on\n\n        &lt;Proxy *&gt;\n            Allow from localhost\n        &lt;/Proxy&gt;\n\n        ProxyPass /vtunnel http://localhost:6000/vtunnel\n        ProxyPassReverse /vtunnel http://localhost:6000/vtunnel\n        ProxyPass / http://localhost:8080/\n        ProxyPassReverse / http://localhost:8080/\n\n        ProxyRequests Off\n\n        CustomLog /var/log/apache2/access_vf.log \"%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\\"%r\\\" %b\"\n    &lt;/VirtualHost&gt;\n\n\n    ErrorLog /var/log/apache2/error_vf.log\n&lt;/IfModule&gt;\n</code></pre> </li> <li> <p>Enable mod_ssl: You need to make sure that the mod_ssl module is enabled in your Apache configuration. This module provides support for SSL/TLS encryption.     Enable mod_proxy: You also need to enable the mod_proxy module in your Apache configuration. This module allows Apache to act as a proxy server. </p><pre><code>a2enmod ssl rewrite proxy requestheader headers proxy_http\n</code></pre> </li> <li> <p>Replace \"Your_Domain.com\", \"Your_Email\", ProxyPass and ProxyPassReverse: In the Apache configuration, you need to replace \"Your_Domain.com\" with your actual domain name. </p> </li> <li> <p>Adjust SSL certificate locations: You need to specify the correct SSL certificate locations for the subdomain used by the apps. </p> </li> <li> <p>Check the apache config syntax. </p><pre><code>apache2ctl configtest\n</code></pre> </li> <li> <p>Restart Apache 2 web server, </p><pre><code>/etc/init.d/apache2 restart\nor \nsudo /etc/init.d/apache2 restart\nor \nsudo service apache2 restart\n</code></pre> </li> </ol>"},{"location":"ViaFoundry/admin-configuration/#after-configuration","title":"After Configuration","text":"<ol> <li>After changing the configurations, please restart vfoundry, vmeta, vportal and vsso.</li> </ol> <pre><code>pm2 restart all\n</code></pre> <ol> <li>Enter Vsso (https://your_domain/vsso) and login with any username and password. The first user will be assigned as admin.</li> <li>Enter Vmeta (https://your_domain/vmeta). Click the profile button at the top right and click the Servers tab. Here you can insert multiple foundry servers after clicking the \u201cadd a server\u201d button. Enter any name for your vpipe server and select type as \u201cvpipe\u201d.</li> <li>URL Client: https://your_domain/vpipe</li> <li>URL Server: https://your_domain/vpipe</li> <li>Type: vpipe</li> <li>Main Server: checked</li> </ol>"},{"location":"ViaFoundry/admin-configuration/#advanced-configuration","title":"Advanced Configuration:","text":""},{"location":"ViaFoundry/admin-configuration/#vpipe-config-file-details","title":"Vpipe Config File Details","text":"<p>All of the configuration directive details can be found below:</p> <p>[Vpipe] Section:</p> <ul> <li>DB: This is the name of the database used by Vpipe.</li> <li>DBUSER: It is the name of a database user who has access to the Vpipe database.</li> <li>DBPASS: This is the password for the specified database user.</li> <li>DBHOST: It represents the hostname or IP address of the MySQL server where the Vpipe database is hosted.</li> <li> <p>DBPORT: This specifies the port number used to connect to the MySQL server.</p> </li> <li> <p>SSHPATH: It denotes the secure path on the system where SSH files are saved. This path is used for SSH-related operations in Vpipe.</p> </li> <li>AMZPATH: This is the secure path on the system where Amazon files are saved. It is used for storing files related to Amazon integration in Vpipe.</li> <li> <p>GOOGPATH: Similarly, this is the secure path where Google files are saved. It is used for storing files related to Google integration in Vpipe.</p> </li> <li> <p>AMAZON: It is a randomly generated string or salt used for encrypting Amazon keys. This salt adds an extra layer of security to the encryption process.</p> </li> <li> <p>SALT, PEPPER, MASTER, VERIFY: These are randomly generated strings used for user authentication. They enhance the security of the authentication process by adding unique elements.</p> </li> <li> <p>JWT_SECRET: This is the secret key used for creating and verifying JSON Web Tokens (JWT) in Vpipe. JWTs are used for authentication and ensuring the integrity of data transmitted between the client and server.</p> </li> <li> <p>JWT_COOKIE_EXPIRES_IN: This specifies the expiration time for the JWT cookie. Once the specified time period has passed, the cookie will expire, and the user will need to log in again for authentication. The value is usually set in terms of days.</p> </li> </ul> <p>[CONFIG] Section:</p> <ul> <li> <p>ENV_PATH: This is an optional file path that can be specified to be sourced before starting the application's run. It could be a file like \"/home/.bashrc\" and is used to load environment variables or configuration settings.</p> </li> <li> <p>TIMEZONE: It sets the default timezone used by all date and time functions in the application. It ensures that the application works with the correct time representation based on the specified timezone.</p> </li> <li> <p>RUNPATH: This is the relative path where run logs are stored.</p> </li> <li> <p>TEMPPATH: Similarly, this is the relative path where temporary files created during the application's runtime are stored.</p> </li> <li> <p>API_URL: This is the Vpipe URL used inside the Docker container. It is the endpoint that the application will use to make API calls when it receives requests.</p> </li> <li> <p>BASE_PATH: This represents the Vpipe URL outside the Docker container. It is the base URL used to access Vpipe from external sources or systems.</p> </li> <li> <p>PUBWEB_URL: This is the URL that can be used to reach the public web directory. It specifies the address and path to access the directory where public web files are hosted. For example, \"http://localhost:8080/vpipe/tmp/pub\" can be the URL for accessing the public web directory when running locally.</p> </li> <li> <p>NEXTFLOW_VERSION: This setting specifies the version of Nextflow that will be used by the application.</p> </li> <li> <p>LDAP_SERVER, DN_STRING, BIND_USER, BIND_PASS: These configuration parameters are used for the LDAP (Lightweight Directory Access Protocol) server. They include details such as the LDAP server address, DN string, and credentials for binding to the LDAP server.</p> </li> <li> <p>EMAIL_SENDER: This configuration parameter represents the email address that will be used as the sender when the Vpipe application sends emails. It is the \"From\" address in the email communication.</p> </li> <li> <p>EMAIL_ADMIN: This configuration parameter specifies the email address (or multiple email addresses) of the administrator(s) who will receive notifications from the Vpipe server. These email addresses are used for administrative alerts or important notifications.</p> </li> <li> <p>EMAIL_TYPE: This configuration parameter determines the type of email sending mechanism to be used by the Vpipe application. There are three options:</p> </li> <li> <p>DEFAULT (sendMail): This option uses the default email sending method available in the system.</p> </li> <li>SMTP: This option utilizes the Simple Mail Transfer Protocol (SMTP) to send emails. It requires additional parameters for SMTP configuration.</li> <li> <p>HTTP: This option uses an HTTP-based email sending mechanism. It requires additional parameters for configuring the HTTP endpoint.</p> </li> <li> <p>Required Parameters for EMAIL_TYPE=DEFAULT/SMTP:</p> </li> <li> <p>EMAIL_HOST: This parameter specifies the hostname or IP address of the email server to be used for sending emails.</p> </li> <li>EMAIL_USERNAME: This parameter represents the username or account name used for authentication when connecting to the email server.</li> <li>EMAIL_PASSWORD: This parameter is the password associated with the email account used for authentication.</li> <li> <p>EMAIL_PORT: This parameter indicates the port number used to establish a connection with the email server.</p> </li> <li> <p>Required Parameters for EMAIL_TYPE=HTTP:</p> </li> <li>EMAIL_URL: This parameter represents the URL of the HTTP endpoint used for sending emails.</li> <li>EMAIL_HEADER_KEY: This parameter specifies the header key required by the HTTP endpoint for authentication or identification purposes.</li> <li>EMAIL_HEADER_VALUE: This parameter represents the value associated with the header key mentioned above. It is provided for successful communication with the HTTP endpoint.</li> </ul> <p>[UICONFIG] Section:</p> <ul> <li> <p>COMPANY_NAME: This setting defines the name of the company or organization that will be displayed on the webpage.</p> </li> <li> <p>PASSWORD_LOGIN: This parameter is used to enable or disable password-based login for users.</p> </li> <li> <p>ALLOW_SIGNUP: This parameter toggles the sign-up button on the home page. Enabling it allows users to sign up for an account.</p> </li> <li> <p>ALLOW_SIGNUPGOOGLE: This parameter toggles the Google sign-in button on the home page. To use this feature, you need to provide the <code>GOOGLE_CLIENT_ID</code> directive. Enabling this option allows users to sign in using their Google accounts.</p> </li> <li> <p>GOOGLE_CLIENT_ID: This parameter specifies the client ID required for Google sign-in functionality. It is used to authenticate users through Google services.</p> </li> <li> <p>SHOW_WIZARD: This parameter toggles the display of a wizard on the home page. Enabling it shows a step-by-step guide or tutorial to assist users.</p> </li> <li> <p>SHOW_AMAZON_KEYS: This parameter toggles the display of the Amazon Keys tab in the user's profile section.</p> </li> <li> <p>SHOW_GOOGLE_KEYS: This parameter toggles the display of the Google Keys tab in the user's profile section.</p> </li> <li> <p>SHOW_SSH_KEYS: This parameter toggles the display of the SSH Keys tab in the user's profile section.</p> </li> <li> <p>SHOW_GROUPS: This parameter toggles the display of the Groups tab in the user's profile section.</p> </li> <li> <p>SHOW_GITHUB: This parameter toggles the display of the GitHub tab in the user's profile section.</p> </li> <li> <p>SHOW_RUN_LOG: This parameter toggles the display of the Log.txt file in the Log tab of the run page.</p> </li> <li> <p>SHOW_RUN_TIMELINE: This parameter toggles the display of the timeline file in the Log tab of the run page.</p> </li> <li> <p>SHOW_RUN_REPORT: This parameter toggles the display of the report file in the Log tab of the run page.</p> </li> <li> <p>SHOW_RUN_DAG: This parameter toggles the display of the DAG (Directed Acyclic Graph) file in the Log tab of the run page.</p> </li> <li> <p>SHOW_RUN_TRACE: This parameter toggles the display of the trace file in the Log tab of the run page.</p> </li> <li> <p>SHOW_RUN_NEXTFLOWLOG: This parameter toggles the display of the .nextflow.log file in the Log tab of the run page.</p> </li> <li> <p>SHOW_RUN_NEXTFLOWNF: This parameter toggles the display of the nextflow.nf file in the Log tab of the run page.</p> </li> <li> <p>SHOW_RUN_NEXTFLOWCONFIG: This parameter toggles the display of the nextflow.config file in the Log tab of the run page.</p> </li> <li> <p>SHOW_HOMEPAGE: This parameter toggles the display of the homepage.</p> </li> <li> <p>SHOW_APPS: This parameter toggles the display of applications on the webpage.</p> </li> <li> <p>ENABLE_SHARE_WITH_EVERYONE_RUN: This parameter enables or disables the option to share runs with everyone.</p> </li> <li> <p>ENABLE_SHARE_WITH_EVERYONE_PIPELINE: This parameter enables or disables the option to share pipelines with everyone.</p> </li> <li> <p>CUSTOM_HELP_MESSAGE: This setting allows the customization of a help message to be displayed in the support section of the webpage.</p> </li> <li> <p>CUSTOM_FILE_LOCATION_MESSAGE: This setting allows the customization of a message to be displayed in the \"add file\" window on the run page.</p> </li> </ul> <p>[SSOCONFIG] Section:</p> <ul> <li> <p>SSO_LOGIN: This setting determines whether Single Sign-On (SSO) login is enabled or disabled for the application. Set it to <code>true</code> to enable SSO login and <code>false</code> to disable it.</p> </li> <li> <p>SSO_URL: This parameter specifies the URL of the SSO server. It is the endpoint where the authentication process takes place.</p> </li> <li> <p>CLIENT_ID, CLIENT_SECRET: These parameters represent the client credentials required for SSO integration. They are obtained from the SSO server. The <code>CLIENT_ID</code> is a unique identifier assigned to the Vpipe application, and the <code>CLIENT_SECRET</code> is a secure key used for authentication and authorization purposes.</p> </li> </ul> <p>[VIACONFIG] Section:</p> <ul> <li>VFOUNDRY_URL: This parameter represents the URL of the Via Foundry server. The VFOUNDRY_URL setting allows the Vpipe application to connect and interact with the Via Foundry server.</li> </ul>"},{"location":"ViaFoundry/admin-configuration/#vmetavportalvfoundry-config-file-details","title":"Vmeta/Vportal/Vfoundry Config File Details","text":"<p>All of the configuration directive details can be found below. By configuring these settings, you can customize various aspects of the application, including the environment, server configuration, database connection, authentication options, email settings, SSO integration, SSL/TLS certificates, and session management.</p> <ul> <li> <p>NODE_ENV: This parameter specifies the environment in which the application is running. The default value is \"production\".</p> </li> <li> <p>PROTOCOL: This parameter determines the protocol to be used for communication. It can be either \"http\" or \"https\".</p> </li> <li> <p>PORT: This parameter specifies the port number on which the application will run.</p> </li> <li> <p>BASE_URL: This parameter represents the base URL of the application. If the application is running under a subfolder of another application, the SUBFOLDER parameter should be defined.</p> </li> <li> <p>SUBFOLDER: This parameter is used when the application is running under a subfolder of another application. It specifies the subfolder path.</p> </li> <li> <p>DATABASE_LOCAL: This parameter defines the connection URL for the local MongoDB database. It includes the credentials (username and password), hostname, port, and database name.</p> </li> <li> <p>SESSION_SECRET: This is a secret key used for session management. It should be a long, secure string.</p> </li> <li> <p>JWT_SECRET: This is the secret key used for JWT (JSON Web Token) generation and verification. It should also be a long, secure string.</p> </li> <li> <p>JWT_EXPIRES_IN: This parameter defines the expiration time for JWTs in days.</p> </li> <li> <p>JWT_COOKIE_EXPIRES_IN: This parameter defines the expiration time for JWT cookies in days.</p> </li> <li> <p>CERTS_PRIVATE_KEY: This parameter specifies the file path for the private key used for SSL/TLS certificates.</p> </li> <li> <p>CERTS_CERTIFICATE: This parameter specifies the file path for the SSL/TLS certificate.</p> </li> <li> <p>TIME_TO_CHECK_EXPIRED_TOKENS: This parameter determines the time interval (in seconds) for checking the database for expired tokens.</p> </li> <li> <p>SSO_LOGIN: This setting determines whether Single Sign-On (SSO) login is enabled or disabled for the application.</p> </li> <li> <p>SSO_URL: This parameter represents the URL of the SSO authorization server.</p> </li> <li> <p>SSO_REDIRECT_URL: This parameter specifies the URL where the user will be redirected after successful SSO authentication.</p> </li> <li> <p>SSO_USER_INFO_URL: This parameter represents the URL for obtaining user information from the SSO server.</p> </li> <li> <p>CLIENT_ID, CLIENT_SECRET: These parameters represent the client credentials retrieved from the SSO server for authentication and authorization.</p> </li> </ul>"},{"location":"ViaFoundry/admin-configuration/#vsso-config-file-details","title":"Vsso Config File Details","text":"<p>All of the configuration directive details can be found below. By configuring these settings, you can customize various aspects of the application, including the environment, server configuration, database connection, authentication options, email settings, SSO integration, SSL/TLS certificates, and session management.</p> <ul> <li> <p>NODE_ENV: This parameter specifies the environment in which the application is running. The default value is \"production\".</p> </li> <li> <p>PORT: This parameter specifies the port number on which the application will run. It is set to 3000.</p> </li> <li> <p>PROTOCOL: This parameter determines the protocol to be used for communication.</p> </li> <li> <p>BASE_URL: This parameter represents the base URL of the application. If the application is running under a subfolder of another application, the SUBFOLDER parameter should be defined.</p> </li> <li> <p>SUBFOLDER: This parameter is used when the application is running under a subfolder of another application. It specifies the subfolder path, which is \"/vsso\" in this case.</p> </li> <li> <p>DATABASE: This parameter defines the connection URL for the MongoDB database. It includes the credentials (username and password), hostname, port, and database name.</p> </li> <li> <p>SALT, PEPPER: These parameters are random strings used for user authentication.</p> </li> <li> <p>JWT_SECRET: This is the secret key used for JWT (JSON Web Token) generation and verification. It should be a long, secure string.</p> </li> <li> <p>JWT_EXPIRES_IN: This parameter defines the expiration time for JWTs, set to 90 days.</p> </li> <li> <p>JWT_COOKIE_EXPIRES_IN: This parameter defines the expiration time for JWT cookies, also set to 90 days.</p> </li> <li> <p>COMPANY_NAME: This parameter specifies the name of the company that will be displayed in the user interface.</p> </li> <li> <p>ALLOW_SIGNUP: This parameter toggles the sign-up button on the home page. It is set to false, meaning sign-up is not allowed.</p> </li> <li> <p>PASSWORD_LOGIN: This parameter toggles the password login option. It is set to true, enabling password-based login.</p> </li> <li> <p>EMAIL_FROM, EMAIL_USERNAME, EMAIL_PASSWORD, EMAIL_HOST, EMAIL_PORT, EMAIL_ADMIN, EMAIL_ADMIN_NAME: These parameters configure the email settings, including the sender's email, SMTP credentials, host, port, admin email, and admin name.</p> </li> <li> <p>GOOGLE_LOGIN: This parameter enables Google login. It is set to true.</p> </li> <li> <p>GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GOOGLE_CALLBACK_URL: These parameters represent the client credentials and callback URL for Google login.</p> </li> <li> <p>OKTA_SAML_LOGIN: This parameter enables SAML-based login with Okta. It is set to true.</p> </li> <li> <p>SSO_ISSUER, SSO_SAML_METADATA, SSO_SAML_DESTINATION_URL: These parameters configure the SAML settings for Okta login, including the issuer, SAML metadata file path, and destination URL after successful login.</p> </li> <li> <p>CERTS_PRIVATE_KEY, CERTS_CERTIFICATE: These parameters specify the file paths for the private key and SSL/TLS certificate used by the application.</p> </li> <li> <p>SESSION_SECRET: This is a secret key used for session management. It should be a long, secure string.</p> </li> <li> <p>SESSION_MAXAGE: This parameter defines the maximum age (in milliseconds) for sessions.</p> </li> <li> <p>TIME_TO_CHECK_EXPIRED_TOKENS: This parameter determines the time interval (in seconds) for checking the database for expired tokens.</p> </li> <li> <p>ACCESS_TOKEN_EXPIRES_IN: This parameter defines the expiration time (in milliseconds) for access tokens.</p> </li> <li> <p>CODE_TOKEN_EXPIRES_IN: This parameter defines the expiration time (in seconds) for code tokens.</p> </li> <li> <p>REFRESH_TOKEN_EXPIRES_IN: This parameter defines the expiration time (in minutes) for refresh tokens.</p> </li> </ul>"},{"location":"ViaFoundry/admin-installation-apps/","title":"App Section","text":""},{"location":"ViaFoundry/admin-installation-apps/#app-section-documentation","title":"App Section Documentation","text":"<p>This documentation provides step-by-step instructions for setting up VTunnel and Kubernetes on your host machine, including installing required dependencies and configuring the environment. VTunnel is a critical component for managing applications on Foundry. </p>"},{"location":"ViaFoundry/admin-installation-apps/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following prerequisites ready on your host machine:</p> <ol> <li> <p>SSL Certificates for foundry-apps (e.g. Let\u2019s Encrypt)</p> </li> <li> <p>Use the following commands to install Node.js and npm:      </p><pre><code>sudo apt-get update\nsudo apt-get install -y --no-install-recommends nodejs npm\nsudo npm install pm2 npm@8.19.4 n -g\nsudo n 16.20.1\n</code></pre> </li> <li>Download and install Minikube from https://minikube.sigs.k8s.io/docs/start/.</li> </ol>"},{"location":"ViaFoundry/admin-installation-apps/#verify-prerequisites","title":"Verify Prerequisites","text":"<p>After installing Node.js and Minikube, verify the versions of Node.js and npm: </p><pre><code>$ npm -v\n8.19.4\n\n$ node -v\nv16.20.1\n</code></pre>"},{"location":"ViaFoundry/admin-installation-apps/#install-nodejs-pm2-and-modules","title":"Install Node.js PM2 and modules","text":"<p>Assume your vtunnel is install under /opt/viafoundry folder </p><pre><code>cd /opt/viafoundry/vtunnel\nsudo npm install pm2 -g\nnpm install </code></pre>"},{"location":"ViaFoundry/admin-installation-apps/#minikube-setup","title":"Minikube Setup","text":"<ol> <li>Stop and delete any existing outdated Minikube instance (if previously installed):    <pre><code>$ minikube stop\n$ minikube delete\n</code></pre></li> <li>Install Minikube with Kubernetes version v1.25.9 and additional configurations:    <pre><code>$ minikube start --kubernetes-version=v1.25.9 --addons=metrics-server,ingress --container-runtime=docker --cpus=32 --memory=128g --mount --mount-string=\"/opt:/opt1\"\n</code></pre></li> <li> <p>Obtain Minikube's IP address:    </p><pre><code>$ minikube ip\n192.168.49.2\n</code></pre> </li> <li> <p>Update the server's Apache configuration for VFoundry APP:    Use minikube IP at below.    Replace Your_APP_Domain.com with your actual app domain name (e.g. viafoundry-apps.com).     </p><pre><code>$ vi /etc/apache2/sites-enabled/foundry-app.conf\n\n&lt;IfModule mod_ssl.c&gt;\n &lt;VirtualHost *:443&gt;\n ServerName Your_APP_Domain.com\n RewriteEngine On\n SSLCertificateFile /etc/letsencrypt/live/Your_Domain.com/fullchain.pem\n SSLCertificateKeyFile /etc/letsencrypt/live/Your_Domain.com/privkey.pem\n SSLProxyEngine on\n\n SSLProxyVerify none\n SSLProxyCheckPeerCN off\n SSLProxyCheckPeerName off\n SSLProxyCheckPeerExpire off\n\n RewriteEngine on\n RewriteCond %{HTTP:Upgrade} =websocket\n RewriteRule /(.*) wss://192.168.49.2/$1 [P,L]\n RewriteRule /(.*) https://192.168.49.2/$1 [P,L]\n &lt;Proxy *&gt;\n   Allow from localhost\n &lt;/Proxy&gt;\n &lt;Location /&gt;\n   ProxyPreserveHost on\n   ProxyPass         https://192.168.49.2/\n   ProxyPassReverse  https://192.168.49.2/\n   RequestHeader set X-Forwarded-Port \"443\"\n   RequestHeader set \"X-Forwarded-Proto\" expr=https\n &lt;/Location&gt;\n\n CustomLog /var/log/apache2/access_vf_apps.log \\\n       \"%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\\"%r\\\" %b\"\n ErrorLog /var/log/apache2/error_vf_apps.log\n &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\n\n&lt;VirtualHost *:80&gt;\n ServerName Your_APP_Domain.com\n RewriteEngine On\n RewriteCond %{SERVER_NAME} =Your_APP_Domain.com\n RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,QSA,R=permanent]\n&lt;/VirtualHost&gt;\n</code></pre> </li> </ol>"},{"location":"ViaFoundry/admin-installation-apps/#install-vtunnel","title":"Install VTunnel","text":"<ol> <li> <p>Using root user change the owner of vtunnel directory. </p><pre><code>sudo su - root\nchown -R viafoundry /opt/vfoundry/vtunnel\n</code></pre> </li> <li> <p>Switch to viafoundry user and install VTunnel </p><pre><code>sudo su - viafoundry\ncd /opt/vfoundry/vtunnel\nnpm install\npm2 start pm2-process.json\n</code></pre> </li> </ol>"},{"location":"ViaFoundry/admin-installation-apps/#upgrade-vtunnel-config","title":"Upgrade VTunnel Config","text":"<ol> <li>Change the kubectl alias:    <pre><code>$ alias kubectl='minikube kubectl --'\n</code></pre></li> <li>Navigate to the VTunnel directory:    <pre><code>$ cd /export/vtunnel\n</code></pre></li> <li> <p>Deploy VTunnel using kustomize:    </p><pre><code>$ cd /export/vtunnel/tools/*/docs/deployment/*/1*\n$ kustomize build . | kubectl apply -f - --server-side\n$ # Repeat the command if CRD could not be created on the first attempt\n$ kustomize build . | kubectl apply -f - --server-side\n</code></pre> </li> <li> <p>Check the status of pods:    </p><pre><code>$ kubectl get pods -n shinyproxy\n</code></pre> </li> </ol>"},{"location":"ViaFoundry/admin-installation-apps/#upgrade-vpipe-config","title":"Upgrade VPipe Config","text":"<ol> <li>Edit the VPipe config file:    <pre><code>$ vi /export/vpipe/config/.sec\n</code></pre></li> <li>Update the following APPS section    <pre><code>[APPS]\nAPP_URL=https://YOUR_APP_DOMAIN\nMOUNTED_VOLUME=/opt1/vfoundry\nVTUNNEL_URL=https://YOUR_DOMAIN/vtunnel\n\n# MOUNTED_VOLUME will be used to mount run reports into app containers. It should be the location of the export directory outside of the container. e.g. /opt1/vfoundry. \n# If you mounted your drive to minikube on start e.g. minikube start --mount-string=\"/opt:/opt1\"\n# then /opt needs to be replaced with /opt1 on your path. e.g. /opt1/vfoundry. \n</code></pre></li> </ol>"},{"location":"ViaFoundry/admin-installation/","title":"Installation","text":"<p>Via Foundry's main server is located at https://viafoundry.com. Via Foundry can also be run as a standalone application using a docker container.</p>"},{"location":"ViaFoundry/admin-installation/#prerequisites","title":"Prerequisites:","text":"<ul> <li>Via Scientific Access Keys</li> <li>Via Scientific Docker Hub Access Keys</li> <li>Two Separate SSL Certificates for foundry and foundry-apps (e.g. Let\u2019s Encrypt)</li> </ul>"},{"location":"ViaFoundry/admin-installation/#recommended-computer-specifications","title":"Recommended Computer Specifications:","text":"<ul> <li>Hard disk: 100-200GB (root)</li> <li>16-32 CPUs</li> <li>64-128 GB of memory.</li> <li>e.g. AWS instance type m6i.4xlarge+, GCP VM type n2-standard-16+</li> <li>Operating System: Ubuntu 22.04</li> <li>1-2TB EBS (AWS) / Persistent SSD Disk (GCP) mounted drive</li> </ul>"},{"location":"ViaFoundry/admin-installation/#preparation-steps","title":"Preparation Steps","text":"<p>Before you begin, it is recommended to follow these best practices:</p> <ol> <li> <p>Create a user named <code>viafoundry</code> on the target machine.</p> </li> <li> <p>Configure the directory and perform the installation while logged in as the <code>viafoundry</code> user.</p> </li> <li> <p>Ensure that the default shell for the 'viafoundry' user is set to 'bash'. Use <code>echo $SHELL</code> to validate the current SHELL, if not, use <code>chsh</code> to update the SHELL to <code>/bin/bash</code>.</p> </li> </ol>"},{"location":"ViaFoundry/admin-installation/#software-dependencies-for-pipeline-execution","title":"Software Dependencies for Pipeline Execution","text":"<p>In order to execute Via Foundry pipelines, you have to install and validate certain software dependencies into your host machine.</p> <p>To enable proper pipeline execution, Nextflow should be installed into your host environment. Since most of our pipelines isolate their dependencies within their Docker, please install Docker or Podman into your machine by following the guidelines below. If your platform doesn't support the installation of Docker, you can still use our pipelines with just Singularity.</p> <ul> <li>Installing   Nextflow</li> <li>Java v11+ (for nextflow)</li> <li>Installing   Docker</li> <li>Installing Singularity (Version 3) (required when docker is not available)</li> <li>AWS CLI v2 for AWS resources access</li> <li>GCloud/GSUtil CLI for GCP resources access</li> </ul> <p>How to Add Software to Your $PATH Environment:</p> <ul> <li>JAVA Command (optional): If JAVA is not added to the $PATH   environment, you can run the command (<code>module load java/8.0</code>) to   manipulate your $PATH environment and gain access to JAVA.</li> <li>Nextflow Path or Command (optional): If Nextflow is not added   to the $PATH environment, you can either enter the path of the   nextflow (eg. <code>/project/bin</code>), or run the command   (<code>module load nextflow</code>) to manipulate your $PATH environment and   gain access to new software.</li> <li>Docker/Singularity Command (optional): You can run a command   (eg. <code>module load docker/1.0.0</code> or   <code>module load singularity/3.0.0</code>) to manipulate your $PATH   environment in order to gain access to new software.</li> </ul>"},{"location":"ViaFoundry/admin-installation/#software-dependencies-for-foundry-installation","title":"Software Dependencies for Foundry Installation","text":"<ul> <li>Apache2 or NGINX (to redirect the site to selected domain)</li> <li>Docker (latest version)</li> </ul>"},{"location":"ViaFoundry/admin-installation/#installation","title":"Installation","text":"<ol> <li> <p>We install the database and software outside of the container to be able to keep the changes in the database and software every time you start the container. Therefore, please choose a directory in your machine to mount. Then replace <code>/path/to/mount</code> with your path to create a directory. Please remove s permission from the directory for proper installation.</p> <pre><code>chmod ug-s /path/to/mount\nmkdir -p /path/to/mount\n</code></pre> </li> <li> <p>Use docker to download Foundry container from a private ViaScientific repository.</p> <pre><code>docker login\n# username:viasdock\n# password: will be sent with separate email\ndocker pull viascientific/vfoundry-docker\n</code></pre> </li> <li> <p>Please execute the following command to start the container. Please don't change the target directory(<code>/export</code>) in the docker image and bind it to host port 8080.</p> <pre><code>docker run -m 10G -p 8080:8080 --name vfoundry -v /path/to/mount:/export -dti viascientific/vfoundry-docker /bin/bash\n</code></pre> </li> <li> <p>After you start the container, you need to start the mysql and apache server using the command below:</p> <pre><code>startup (if this is for the initial setup time)\nstart.sh (if this is continuous maintenance after initial installation)\n</code></pre> </li> <li> <p>Verify that foundry and mysql folders located inside of the export folder.</p> <pre><code>ls /export\n</code></pre> </li> <li> <p>Update software version by executing following commands in docker:</p> <pre><code>cd /export/vsso &amp;&amp; git pull &amp;&amp; yarn install &amp;&amp; yarn build &amp;&amp; pm2 restart pm2-process.json\ncd /export/vmeta &amp;&amp; git pull &amp;&amp; yarn install &amp;&amp; yarn build &amp;&amp; pm2 restart pm2-process.json\ncd /export/vportal &amp;&amp; git pull &amp;&amp; yarn install &amp;&amp; yarn build &amp;&amp; pm2 restart pm2-process.json\ncd /export/vfoundry &amp;&amp; git pull &amp;&amp; yarn install &amp;&amp; yarn build &amp;&amp; pm2 restart pm2-process.json\npython /export/vpipe/scripts/updateDN.py\n</code></pre> </li> <li> <p>Now, you can open your browser to access foundry using the URL below:</p> <pre><code>http://localhost:8080\n</code></pre> </li> </ol>"},{"location":"ViaFoundry/admin-pipeline-testing/","title":"Testing Pipeline","text":""},{"location":"ViaFoundry/admin-pipeline-testing/#pipeline-testing","title":"Pipeline Testing","text":""},{"location":"ViaFoundry/admin-pipeline-testing/#downloading-genome-files","title":"Downloading Genome files","text":"<pre><code># Set localdownload directory\ngenomedata=\"/opt/viafoundry/run_data\"\n\n## Main Genomes\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/human/hg38/ -P ${genomedata}/genome_data/human/hg38/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/mouse/mm10/ -P ${genomedata}/genome_data/mouse/mm10/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/mousetest/mm10/ -P ${genomedata}/genome_data/mousetest/mm10/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\n\n\n## Optional Genomes\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/human/hg19/ -P ${genomedata}/genome_data/human/hg19/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/rat/rn6/ -P ${genomedata}/genome_data/rat/rn6/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/c_elegans/ce11/ -P ${genomedata}/genome_data/c_elegans/ce11/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/d_melanogaster/BDGP6_32/ -P ${genomedata}/genome_data/d_melanogaster/BDGP6_32/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/dog/canFam3/ -P ${genomedata}/genome_data/dog/canFam3/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/e_coli/ASM584v2_NC_000913_3/ -P ${genomedata}/genome_data/e_coli/ASM584v2_NC_000913_3/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/n_vectensis/jaNemVect1_1/ -P ${genomedata}/genome_data/n_vectensis/jaNemVect1_1/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/s_cerevisiae/sacCer3/ -P ${genomedata}/genome_data/s_cerevisiae/sacCer3/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/s_pombe/ASM294v2/ -P ${genomedata}/genome_data/s_pombe/ASM294v2/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\nwget https://web.dolphinnext.com/umw_biocore/dnext_data/genome_data/zebrafish/GRCz11/ -P ${genomedata}/genome_data/zebrafish/GRCz11/ -l inf -nc -nH --cut-dirs=4 -r --no-parent -R \"index.html*\"\n\n\n# If you're using AWS Cloud, Sync data with S3 bucket\naws s3 sync -r ${genomedata} s3://DEST_BUCKET/viafoundry/run_data/genome_data\n# Remove the local data after syncing with AWS S3\nrm -rf ${genomedata}\n\n# If you're using Google Cloud, Sync data with Google Cloud Storage bucket\ngsutil rsync -r ${genomedata} gs://DEST_BUCKET/viafoundry/run_data/genome_data\n# Remove the local data after syncing with Google Cloud Storage\nrm -rf ${genomedata}\n</code></pre>"},{"location":"ViaFoundry/admin-pipeline-testing/#configuration-of-run-environment","title":"Configuration of Run Environment","text":"<p>Once logged in, click on the profile tab in the top right of the screen. You'll notice several tabs to explore in profile page.</p> <ul> <li>SSH Keys: Needs to be configured to setup a connection and click hide from user button. </li> <li> <p>Run environments: This is your main segment for creating connection profiles for users.</p> </li> <li> <p>Run environments: -&gt; SSH Keys : Select SSH Keys.</p> </li> <li> <p>Run environments: -&gt; Profile Variables : Set the following directory for DOWNDIR:</p> <pre><code>## A. For Google Cloud:\n## if genome_data located at gs://DEST_BUCKET/viafoundry/run_data/genome_data\nparams.DOWNDIR = \"gs://DEST_BUCKET/viafoundry/run_data\"\n\n## B. For AWS Cloud:\n## if genome_data located at s3://DEST_BUCKET/viafoundry/run_data/genome_data\nparams.DOWNDIR = \"s3://DEST_BUCKET/viafoundry/run_data\"\n\n## C. For Local Execution (eg. clusters):\n## if genome_data located at /opt/viafoundry/run_data/genome_data\nparams.DOWNDIR = \"/opt/viafoundry/run_data\"\n</code></pre> </li> <li> <p>Run environments: -&gt; Default Working Directory : Set the following directory for runs:   </p><pre><code>/opt/runs\n</code></pre> </li> <li> <p>Run environments: -&gt; Default Bucket Location for Publishing : Set the following directory for cloud environmets:     </p><pre><code>s3://DEST_BUCKET/viafoundry/runs\nor\ngs://DEST_BUCKET/viafoundry/runs\n</code></pre> </li> </ul>"},{"location":"ViaFoundry/admin-pipeline-testing/#executing-test-run","title":"Executing Test Run","text":"<p>To test a pipeline, you can follow these steps:</p> <ol> <li> <p>Visit the following link: RNA-Seq pipeline.</p> </li> <li> <p>On the webpage, locate the \"download pipeline\" button and click on it. This will initiate the download of a zip file.</p> </li> <li> <p>Once the zip file is downloaded, extract its contents. Inside the extracted folder, you will find a file named <code>main.dn</code>. This file contains the pipeline definition and can be used for importing the pipeline.</p> </li> <li> <p>Now, go to the \"pipelines\" tab on the ViaFoundry website.</p> </li> <li> <p>Look for the \"Create Pipeline\" button and click on it. This will open the pipeline creation interface.</p> </li> <li> <p>In the pipeline creation interface, you will find an option to import a pipeline. Drag and drop the <code>main.dn</code> file that you extracted earlier into the designated area.</p> </li> <li> <p>The pipeline will be imported, and you can proceed with further customization and execution.</p> </li> </ol>"},{"location":"ViaFoundry/admin-pipeline-testing/#automating-run-environment-creation","title":"Automating Run Environment Creation:","text":"<ol> <li>Access the Foundry profile page and navigate to the \"Run Environment\" section.</li> <li>Identify the ID of the run environment that was utilized in a successful test run.</li> <li>Update the <code>DEFAULT_RUN_ENVIRONMENT</code> parameter located within the <code>[CONFIG]</code> section of the <code>/export/vpipe/config/.sec</code> file. This specific run environment will serve as the template for generating new run environments once a new user registers.</li> </ol> <pre><code>[CONFIG]\nDEFAULT_RUN_ENVIRONMENT=YOUR_RUN_ENV_ID\n</code></pre>"},{"location":"ViaFoundry/api/","title":"API Guide","text":""},{"location":"ViaFoundry/api/#foundry-api-v1","title":"Foundry API v1","text":"<p>This guide is aimed at introducing users to the Foundry API, with which one can interact with Metadata Tracker and Run Submission/Tracking through the command line as opposed to the console/user interface. You'll find sample requests and responses related to runs, projects, collections, fields, and data.</p> <p>The Metadata Tracker API utilizes REST (Representational State Transfer) architecture for its communication. All API requests pull information from HTML pages, and all responses, including errors, are returned in JSON format. HTTP response status codes are used to indicate the success or failure of the API calls.</p>"},{"location":"ViaFoundry/api/#authentication-and-authorization","title":"Authentication and authorization","text":"<p>Requests to the Metadata Tracker API are for both public and private information, so all endpoints require authentication.</p>"},{"location":"ViaFoundry/api/#user-login","title":"User Login","text":"<p>After getting their access token, users can make a request to the Metadata Tracker API with the <code>Authorization</code> HTTP header. This header can be specified with the <code>Bearer &lt;your-access-token</code> flag to authenticate a command as being given by a specific user and to confer the same permissions that that user already has onto the current request.</p> <p>Here's how Metadata Tracker API users can retrieve an access token for the currently logged-in user.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X POST 'https://www.viafoundry.com/vmeta/api/v1/users/login' \\\n-H 'Content-Type: application/json' \\\n-d '\n    {\n        \"email\":\"your-email@mail.com\",\n        \"password\":\"your-password\"\n    }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"token\": \"eyJhbGciOiJSUz....\",\n    \"data\": {\n        \"user\": {\n            \"role\": \"user\",\n            \"_id\": \"b6c9a200168a225f39add38d\",\n            \"email\": \"your-email@mail.com\",\n            \"name\": \"test user\",\n            \"scope\": \"*\",\n            \"username\": \"yukseleo\"\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#runs","title":"Runs","text":"<p>In Foundry, you can submit runs, track their statuses and get their run reports.</p>"},{"location":"ViaFoundry/api/#get-all-runs","title":"Get All Runs","text":"<p>This request retrieves a list of all the runs for the currently logged in user.</p> <p>Example request:</p> <pre><code> bash\n $ curl -X GET 'https://www.viafoundry.com/vpipe/api/service.php?data=getRuns' \\\n-H 'Authorization: Bearer &lt;your-access-token&gt;'\n</code></pre> <p>Example response:</p> <pre><code> {\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": [\n            {\n                \"name\": \"Build index (amazon)\",\n                \"_id\": \"234\"\n            },\n            {\n                \"name\": \"UMIextract\",\n                \"_id\": \"249\"\n            },\n            {\n                \"name\": \"test\",\n                \"_id\": \"250\"\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#get-a-run","title":"Get a Run","text":"<p>Retrieve details of a single run.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET \\\n'https://www.viafoundry.com/vpipe/api/service.php?data=getRun&amp;id=4311' \\\n-H 'Authorization: Bearer &lt;your-access-token&gt;'\n</code></pre> <p>Example response:</p> <pre><code> {\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"inputs\": [\n                {\n                    \"name\": \"gtfFilePath\",\n                    \"type\": \"val\",\n                    \"val\": \"/share/data/umw_biocore/genome_data/human/hg38_gencode_v34/ucsc.gtf\"\n                },\n                {\n                    \"name\": \"mate\",\n                    \"type\": \"val\",\n                    \"val\": \"triple\"\n                },\n                {\n                    \"name\": \"genome_build\",\n                    \"type\": \"val\",\n                    \"val\": \"human_hg38_gencode_v34\"\n                }\n            ],\n            \"dmetaOutput\": [\n                {\n                    \"filename\": \"filename\",\n                    \"feature\": \"row\",\n                    \"target\": \"analysis\",\n                    \"id\": \"g-109\",\n                    \"name\": \"UMI_count_final_after_star\"\n                },\n                {\n                    \"filename\": \"row\",\n                    \"feature\": \"column\",\n                    \"target\": \"sample_summary\",\n                    \"id\": \"g-124\",\n                    \"name\": \"summary\"\n                }\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#create-a-run","title":"Create a Run","text":"<p>In this example, the term <code>tmplt_id</code> is used for template run id, <code>reads</code> represents an illustration for defining a collection as input. The global pipeline inputs include the parameters <code>mate</code>, <code>genome</code>, and <code>run_STAR</code>. Specifically, <code>STAR_Module_Map_STAR.params_STAR</code> is associated with the <code>STAR_Module_Map_STAR</code> process parameter. Notably, the process name (STAR_Module_Map_STAR) and variable name (params_STAR) are separated by the \".\" symbol.</p> <p>In this illustration, the remaining pipeline parameters will be derived from the template run parameters.</p> <p>Example request:</p> <pre><code> bash\n $ curl -X POST \\\n'https://www.viafoundry.com/vpipe/api/service.php?run=startRun' \\\n-H 'Authorization: Bearer &lt;your-access-token&gt;' \\\n-H 'Content-Type: application/json' \\\n-d '\n    {\n        \"doc\": {\n            \"name\": \"New Run Name\",\n            \"tmplt_id\": 1314,\n            \"in\": {\n                \"mate\": \"pair\",\n                \"genome\": \"s3://viafoundry/run_data/genome_data/human/hg38/ensembl_v110/main/genome.fa\",\n                \"run_STAR\": \"yes\",\n                \"STAR_Module_Map_STAR.params_STAR\": \"--outSAMtype BAM SortedByCoordinate\",\n                \"reads\": [\n                      {\n                          \"name\": \"exper_rep3\",\n                          \"file_used\": [[ \"exper_rep3.1.gz\",\"exper_rep3.2.gz\"]],\n                          \"file_dir\": [[ \"/share/data/umw_biocore/genome_data/mousetest/mm10/gz\"]], \n                          \"google_cre_id\": \"\",\n                          \"amazon_cre_id\": \"\",\n                          \"file_type\": \"fastq\",\n                          \"collection_type\": \"pair\",\n                          \"archive_dir\": \"\"\n                      },\n                      {\n                          \"name\": \"exper_rep2\",\n                          \"file_used\": [[\"exper_rep2.1.gz\",\"exper_rep2.2.gz\"]],\n                          \"file_dir\": [[\"/share/data/umw_biocore/genome_data/mousetest/mm10/gz\"]],\n                          \"google_cre_id\": \"\",\n                          \"amazon_cre_id\": \"\",\n                          \"file_type\": \"fastq\",\n                          \"collection_type\": \"pair\",\n                          \"archive_dir\": \"\"\n                      }]\n            }\n        }\n    }'\n</code></pre> <p>Example response:</p> <pre><code> {\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"status\": \"submitted\",\n            \"id\": 354,\n            \"creationDate\": \"2021-04-13T05:32:16.505Z\",\n            \"message\": \"Run successfully submitted.\"\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#projects","title":"Projects","text":"<p>In Metadata Tracker, projects serve as your analysis hubs, acting as silos for your data and collections. Some important and frequently-used API requests include retrieving a user's project(s), as well as creating, updating, and deleting projects.</p>"},{"location":"ViaFoundry/api/#get-all-projects","title":"Get All Projects","text":"<p>This request retrieves a list of all the projects for the currently logged in user.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET 'https://www.viafoundry.com/vmeta/api/v1/projects' \\\n-H 'Authorization: Bearer &lt;your-access-token'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"results\": 1,\n    \"data\": {\n        \"data\": [\n            {\n                \"active\": true,\n                \"creationDate\": \"2020-11-17T19:36:49.048Z\",\n                \"lastUpdateDate\": \"2020-11-17T19:36:49.048Z\",\n                \"_id\": \"5fb2b395c8c1e577fcb8ce6c\",\n                \"restrictTo\": {\n                    \"role\": [\n                        \"admin\"\n                    ]\n                },\n                \"name\": \"vitiligo\",\n                \"label\": \"Vitiligo\",\n                \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n                \"owner\": \"5f39add38db6c9a200168a22\",\n                \"perms\": {\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    },\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    }\n                },\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#get-a-project","title":"Get a Project","text":"<p>Retrieve the metadata associated with a single project.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET \\\n'https://www.viafoundry.com/vmeta/api/v1/projects/5fb2b395c8c1e577fcb8ce6c' \\\n-H 'Authorization: Bearer &lt;your-access-token'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"results\": 1,\n    \"data\": {\n        \"data\": [\n            {\n                \"active\": true,\n                \"creationDate\": \"2020-11-17T19:36:49.048Z\",\n                \"lastUpdateDate\": \"2020-11-17T19:36:49.048Z\",\n                \"_id\": \"5fb2b395c8c1e577fcb8ce6c\",\n                \"restrictTo\": {\n                    \"role\": [\n                        \"admin\"\n                    ]\n                },\n                \"name\": \"vitiligo\",\n                \"label\": \"Vitiligo\",\n                \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n                \"owner\": \"5f39add38db6c9a200168a22\",\n                \"perms\": {\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    },\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    }\n                },\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#create-a-project","title":"Create a Project","text":"<p>POST request for creating a project:</p> <p>Example request:</p> <pre><code>bash\n$ curl -X POST \\\n'https://www.viafoundry.com/vmeta/api/v1/projects' \\\n-H 'Authorization: Bearer &lt;your-access-token' \\\n-H 'Content-Type: application/json' \\\n-d '\n    {\n        \"name\": \"vitiligo\",\n        \"label\": \"Vitiligo\"\n    }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"active\": true,\n            \"creationDate\": \"2021-03-31T02:04:27.474Z\",\n            \"lastUpdateDate\": \"2021-03-31T02:04:27.474Z\",\n            \"_id\": \"6063dbcfa50bb5fa9eb9cfba\",\n            \"name\": \"vitiligo\",\n            \"label\": \"Vitiligo\",\n            \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n            \"owner\": \"5f39add38db6c9a200168a22\"\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#update-a-project","title":"Update a Project","text":"<p>Update one or more metadata fields of an existing project.</p> <p>Example request:</p> <pre><code>bash\n$ curl \\\n  -X PATCH \\\n  -H \"Authorization: Bearer &lt;token\" \\\n  https://www.viafoundry.com/vmeta/api/v1/projects/5fb2b395c8c1e577fcb8ce6c \\\n  -H \"Content-Type: application/json\" \\\n  -d '\n      {\n        \"label\": \"Vitiligo\"\n      }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"active\": true,\n            \"creationDate\": \"2020-11-17T19:36:49.048Z\",\n            \"lastUpdateDate\": \"2020-11-17T19:36:49.048Z\",\n            \"_id\": \"5fb2b395c8c1e577fcb8ce6c\",\n            \"restrictTo\": {\n                \"role\": [\n                    \"admin\"\n                ]\n            },\n            \"name\": \"vitiligo\",\n            \"label\": \"Vitiligo\",\n            \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n            \"owner\": \"5f39add38db6c9a200168a22\",\n            \"perms\": {\n                \"read\": {\n                    \"group\": [\n                        \"5fb4575faa5adff6f407f2d1\"\n                    ]\n                },\n                \"write\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#delete-a-project","title":"Delete a Project","text":"<p>Delete an existing project. NOTE: This action is irreversible, and should only be executed when absolutely certain of its intentions.</p> <p>Example request:</p> <pre><code>bash\n$ curl \\\n  -X DELETE \\\n  -H \"Authorization: Bearer &lt;token\" \\\n  https://www.viafoundry.com/vmeta/api/v1/projects/5fb2b395c8c1e577fcb8ce6c \n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"doc\": \"Deleted!\"\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#collections","title":"Collections","text":""},{"location":"ViaFoundry/api/#get-all-collections","title":"Get All Collections","text":"<p>Retrieve a list of all the collections owned by the currently logged-in user.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET 'https://www.viafoundry.com/vmeta/api/v1/collections' \\\n-H 'Authorization: Bearer &lt;your-access-token'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\"status\": \"success\",\n    \"results\": 10,\n    \"data\": {\n        \"data\": [\n            {\n                \"version\": 1,\n                \"required\": false,\n                \"active\": true,\n                \"creationDate\": \"2020-09-08T21:56:35.301Z\",\n                \"lastUpdateDate\": \"2020-09-08T21:56:35.301Z\",\n                \"_id\": \"5f57ffba35db5980ba020ff3\",\n                \"restrictTo\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                },\n                \"name\": \"exp_series\",\n                \"label\": \"Experiment Series\",\n                \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n                \"owner\": \"5f39add38db6c9a200168a22\",\n                \"projectID\": \"5fb2b395c8c1e577fcb8ce6c\",\n                \"perms\": {\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    },\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    }\n                },\n                \"id\": \"5f57ffba35db5980ba020ff3\"\n            },\n            {\n                \"version\": 1,\n                \"required\": false,\n                \"active\": true,\n                \"creationDate\": \"2020-09-08T21:56:35.301Z\",\n                \"lastUpdateDate\": \"2020-09-08T21:56:35.301Z\",\n                \"_id\": \"5f57ffe635db5980ba020ff4\",\n                \"restrictTo\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                },\n                \"name\": \"exp\",\n                \"label\": \"Experiments\",\n                \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n                \"owner\": \"5f39add38db6c9a200168a22\",\n                \"projectID\": \"5fb2b395c8c1e577fcb8ce6c\",\n                \"perms\": {\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    },\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    }\n                },\n                \"id\": \"5f57ffe635db5980ba020ff4\"\n            }]\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#get-a-collection","title":"Get a Collection","text":"<p>Retrieve the details of a single collection.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET \\\n'https://www.viafoundry.com/vmeta/api/v1/collections/5f57ffe635db5980ba020ff4' \\\n-H 'Authorization: Bearer &lt;your-access-token'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": [\n            {\n                \"version\": 1,\n                \"required\": false,\n                \"active\": true,\n                \"creationDate\": \"2020-09-08T21:56:35.301Z\",\n                \"lastUpdateDate\": \"2020-09-08T21:56:35.301Z\",\n                \"_id\": \"5f57ffe635db5980ba020ff4\",\n                \"restrictTo\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                },\n                \"name\": \"exp\",\n                \"label\": \"Experiments\",\n                \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n                \"owner\": \"5f39add38db6c9a200168a22\",\n                \"projectID\": \"5fb2b395c8c1e577fcb8ce6c\",\n                \"perms\": {\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    },\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    }\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#create-a-collection","title":"Create a Collection","text":"<p>This POST request is only allowed for users with the project-admin role.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X POST \\\n'https://www.viafoundry.com/vmeta/api/v1/collections' \\\n-H 'Authorization: Bearer &lt;your-access-token' \\\n-H 'Content-Type: application/json' \\\n-d '\n    {\n        \"name\": \"analysis\",\n        \"label\": \"Analysis\",\n        \"projectID\":\"5fb2b395c8c1e577fcb8ce6c\",\n        \"restrictTo\": {\"group\":[\"5fb45793aa5adff6f407f2d2\"]},\n    }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"version\": 1,\n            \"required\": false,\n            \"active\": true,\n            \"creationDate\": \"2021-03-31T02:26:17.087Z\",\n            \"lastUpdateDate\": \"2021-03-31T02:26:17.087Z\",\n            \"_id\": \"6063e3a33c195afbe6d5e036\",\n            \"name\": \"analysis\",\n            \"label\": \"Analysis\",\n            \"restrictTo\": {\n                \"group\": [\n                    \"5fb45793aa5adff6f407f2d2\"\n                ]\n            },\n            \"projectID\": \"5fb2b395c8c1e577fcb8ce6c\",\n            \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n            \"owner\": \"5f39add38db6c9a200168a22\",\n            \"perms\": {\n                \"read\": {\n                    \"group\": [\n                        \"5fb4575faa5adff6f407f2d1\"\n                    ]\n                },\n                \"write\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#update-a-collection","title":"Update a Collection","text":"<p>Update an existing collection.</p> <p>Example request:</p> <pre><code>bash\n$ curl \\\n  -X PATCH \\\n  -H \"Authorization: Bearer &lt;your-access-token\" \\\n  https://www.viafoundry.com/vmeta/api/v1/collections/6063e3a33c195afbe6d5e036 \\\n  -H \"Content-Type: application/json\" \\\n  -d '\n      {\n        \"label\": \"Analysis\"\n      }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"version\": 1,\n            \"required\": false,\n            \"active\": true,\n            \"creationDate\": \"2021-03-31T02:26:17.087Z\",\n            \"lastUpdateDate\": \"2021-03-31T02:26:17.087Z\",\n            \"_id\": \"6063e3a33c195afbe6d5e036\",\n            \"name\": \"analysis\",\n            \"label\": \"Analysis\",\n            \"restrictTo\": {\n                \"group\": [\n                    \"5fb45793aa5adff6f407f2d2\"\n                ]\n            },\n            \"projectID\": \"5fb2b395c8c1e577fcb8ce6c\",\n            \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n            \"owner\": \"5f39add38db6c9a200168a22\",\n            \"perms\": {\n                \"read\": {\n                    \"group\": [\n                        \"5fb4575faa5adff6f407f2d1\"\n                    ]\n                },\n                \"write\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#delete-a-collection","title":"Delete a Collection","text":"<p>Delete an existing collection. NOTE: This action is irreversible, and should only be executed when absolutely certain of its intentions.</p> <p>Example request:</p> <pre><code>bash\n$ curl \\\n  -X DELETE \\\n  -H \"Authorization: Bearer &lt;your-access-token\" \\\n  https://www.viafoundry.com/vmeta/api/v1/collections/6063e3a33c195afbe6d5e036 \n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"doc\": \"Deleted!\"\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#fields","title":"Fields","text":""},{"location":"ViaFoundry/api/#get-all-fields","title":"Get All Fields","text":"<p>Retrieve a list of fields for the currently logged in user.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET 'https://www.viafoundry.com/vmeta/api/v1/fields' \\\n-H 'Authorization: Bearer &lt;your-access-token'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"results\": 2,\n    \"data\": {\n        \"data\": [\n            {\n                \"type\": \"String\",\n                \"required\": true,\n                \"active\": true,\n                \"creationDate\": \"2020-09-08T21:56:35.406Z\",\n                \"lastUpdateDate\": \"2020-09-08T21:56:35.406Z\",\n                \"_id\": \"5f58518835db5980ba020ff7\",\n                \"name\": \"name\",\n                \"label\": \"Name\",\n                \"collectionID\": \"5f57ffba35db5980ba020ff3\",\n                \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n                \"owner\": \"5f39add38db6c9a200168a22\",\n                \"perms\": {\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    },\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    }\n                },\n                \"unique\": true\n            },\n            {\n                \"type\": \"String\",\n                \"required\": false,\n                \"active\": true,\n                \"creationDate\": \"2020-09-08T21:56:35.406Z\",\n                \"lastUpdateDate\": \"2020-09-08T21:56:35.406Z\",\n                \"_id\": \"5f58559f35db5980ba020ff8\",\n                \"name\": \"design\",\n                \"label\": \"Design\",\n                \"collectionID\": \"5f57ffba35db5980ba020ff3\",\n                \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n                \"owner\": \"5f39add38db6c9a200168a22\",\n                \"perms\": {\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    },\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    }\n                }\n            }]\n        }\n    }\n</code></pre>"},{"location":"ViaFoundry/api/#get-a-field","title":"Get a Field","text":"<p>Retrieve details of a single field.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET \\\n'https://www.viafoundry.com/vmeta/api/v1/fields/5f58559f35db5980ba020ff8' \\\n-H 'Authorization: Bearer &lt;your-access-token'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": [\n            {\n                \"type\": \"String\",\n                \"required\": false,\n                \"active\": true,\n                \"creationDate\": \"2020-09-08T21:56:35.406Z\",\n                \"lastUpdateDate\": \"2020-09-08T21:56:35.406Z\",\n                \"_id\": \"5f58559f35db5980ba020ff8\",\n                \"name\": \"design\",\n                \"label\": \"Design\",\n                \"collectionID\": \"5f57ffba35db5980ba020ff3\",\n                \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n                \"owner\": \"5f39add38db6c9a200168a22\",\n                \"perms\": {\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    },\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    }\n                }\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#create-a-field","title":"Create a Field","text":"<p>This POST request is only enabled for users with the project-admin role.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X POST \\\n'https://www.viafoundry.com/vmeta/api/v1/fields' \\\n-H 'Authorization: Bearer &lt;your-access-token' \\\n-H 'Content-Type: application/json' \\\n-d '\n    {\n        \"name\": \"clin_pheno\",\n        \"label\": \"Clinical Phenotype\",\n        \"type\": \"String\",\n        \"collectionID\":\"5f74a0e05443973d2bfd870c\"\n    }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"type\": \"String\",\n            \"required\": false,\n            \"active\": true,\n            \"creationDate\": \"2021-03-31T02:57:02.771Z\",\n            \"lastUpdateDate\": \"2021-03-31T02:57:02.771Z\",\n            \"_id\": \"6063e7c91bfc89fd1960ae5b\",\n            \"name\": \"clin_pheno\",\n            \"label\": \"Clinical Phenotype\",\n            \"collectionID\": \"5f74a0e05443973d2bfd870c\",\n            \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n            \"owner\": \"5f39add38db6c9a200168a22\",\n            \"perms\": {\n                \"read\": {\n                    \"group\": [\n                        \"5fb4575faa5adff6f407f2d1\"\n                    ]\n                },\n                \"write\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                }\n            },\n            \"id\": \"6063e7c91bfc89fd1960ae5b\"\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#update-a-field","title":"Update a Field","text":"<p>Update an existing field.</p> <p>Example request:</p> <pre><code>bash\n$ curl \\\n  -X PATCH \\\n  -H \"Authorization: Bearer &lt;your-access-token\" \\\n  https://www.viafoundry.com/vmeta/api/v1/fields/6063e7c91bfc89fd1960ae5b \\\n  -H \"Content-Type: application/json\" \\\n  -d '\n      {\n        \"ontology\": {\n            \"create\": true,\n            \"include\": [\n                \"Dermatomyositis\",\n                \"GVHD\",\n                \"Healthy Control\",\n                \"Lupus\",\n                \"Psoriasis\",\n                \"Vitiligo\"\n            ]\n        }\n      }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"type\": \"String\",\n            \"required\": false,\n            \"active\": true,\n            \"creationDate\": \"2021-03-31T02:57:02.771Z\",\n            \"lastUpdateDate\": \"2021-03-31T02:57:02.771Z\",\n            \"_id\": \"6063e7c91bfc89fd1960ae5b\",\n            \"name\": \"clin_pheno\",\n            \"label\": \"Clinical Phenotype\",\n            \"collectionID\": \"5f74a0e05443973d2bfd870c\",\n            \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n            \"owner\": \"5f39add38db6c9a200168a22\",\n            \"perms\": {\n                \"read\": {\n                    \"group\": [\n                        \"5fb4575faa5adff6f407f2d1\"\n                    ]\n                },\n                \"write\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                }\n            },\n            \"ontology\": {\n                \"create\": true,\n                \"include\": [\n                    \"Dermatomyositis\",\n                    \"GVHD\",\n                    \"Healthy Control\",\n                    \"Lupus\",\n                    \"Psoriasis\",\n                    \"Vitiligo\"\n                    ]\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#delete-a-field","title":"Delete a Field","text":"<p>Delete an existing field. NOTE: This action is irreversible, and should only be executed when absolutely certain of its intentions.</p> <p>Example request:</p> <pre><code>bash\n$ curl \\\n  -X DELETE \\\n  -H \"Authorization: Bearer &lt;your-access-token\" \\\n  https://www.viafoundry.com/vmeta/api/v1/fields/6063e7c91bfc89fd1960ae5b \n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"doc\": \"Deleted!\"\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#data","title":"Data","text":""},{"location":"ViaFoundry/api/#pull-all-data-of-a-projects-collection","title":"Pull All Data of a Project's Collection","text":"<p>Retrieve all data associated with a given collection in a project for the currently  logged-in user.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET 'https://www.viafoundry.com/vmeta/api/v1/projects/vitiligo/data/sample' \\\n-H 'Authorization: Bearer &lt;your-access-token'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"results\": 2,\n    \"data\": {\n        \"data\": [\n            {\n                \"creationDate\": \"2020-12-17T16:42:06.252Z\",\n                \"lastUpdateDate\": \"2020-12-17T16:42:06.252Z\",\n                \"_id\": \"5fdb8c6ad6330eb80d503fe2\",\n                \"name\": \"CL067_L2_V1_Bst_sc_iD\",\n                \"sample_type\": \"scRNAseq\",\n                \"technology\": \"inDrop\",\n                \"status\": \"Processed\",\n                \"contract\": \"scRNAseq\",\n                \"bead_occup\": \"65-70%\",\n                \"biosamp_id\": \"5fdb8820d6330eb80d503a31\",\n                \"lastUpdatedUser\": \"5f92529b89c7d0b3bf31ac27\",\n                \"owner\": \"5f92529b89c7d0b3bf31ac27\",\n                \"perms\": {\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    },\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    }\n                },\n                \"DID\": 1\n            },\n            {\n                \"creationDate\": \"2020-12-17T16:42:06.252Z\",\n                \"lastUpdateDate\": \"2020-12-17T16:42:06.252Z\",\n                \"_id\": \"5fdb8c6ad6330eb80d503fe4\",\n                \"name\": \"VB071_L1_V1_Bst_sc_iD\",\n                \"sample_type\": \"scRNAseq\",\n                \"technology\": \"inDrop\",\n                \"status\": \"Processed\",\n                \"contract\": \"scRNAseq\",\n                \"bead_occup\": \"33/50 (~65%)\",\n                \"biosamp_id\": \"5fdb8820d6330eb80d503a33\",\n                \"lastUpdatedUser\": \"5f92529b89c7d0b3bf31ac27\",\n                \"owner\": \"5f92529b89c7d0b3bf31ac27\",\n                \"perms\": {\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    },\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    }\n                },\n                \"DID\": 2\n            }]\n        }\n    }\n</code></pre>"},{"location":"ViaFoundry/api/#get-a-data-value","title":"Get a Data Value","text":"<p>Retrieve details of a single data field.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X GET \\\n'https://www.viafoundry.com/vmeta/api/v1/projects/vitiligo/data/sample/5fdb8c6ad6330eb80d503fe2' \\\n-H 'Authorization: Bearer &lt;your-access-token'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": [\n                    {\n                \"creationDate\": \"2020-12-17T16:42:06.252Z\",\n                \"lastUpdateDate\": \"2020-12-17T16:42:06.252Z\",\n                \"_id\": \"5fdb8c6ad6330eb80d503fe2\",\n                \"name\": \"CL067_L2_V1_Bst_sc_iD\",\n                \"sample_type\": \"scRNAseq\",\n                \"technology\": \"inDrop\",\n                \"status\": \"Processed\",\n                \"contract\": \"scRNAseq\",\n                \"bead_occup\": \"65-70%\",\n                \"biosamp_id\": \"5fdb8820d6330eb80d503a31\",\n                \"lastUpdatedUser\": \"5f92529b89c7d0b3bf31ac27\",\n                \"owner\": \"5f92529b89c7d0b3bf31ac27\",\n                \"perms\": {\n                    \"write\": {\n                        \"group\": [\n                            \"5fb45793aa5adff6f407f2d2\"\n                        ]\n                    },\n                    \"read\": {\n                        \"group\": [\n                            \"5fb4575faa5adff6f407f2d1\"\n                        ]\n                    }\n                },\n                \"DID\": 1\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#create-a-data-value","title":"Create a Data Value","text":"<p>This POST request is only allowed for the users with Write  permission.</p> <p>Example request:</p> <pre><code>bash\n$ curl -X POST \\\n'https://www.viafoundry.com/vmeta/api/v1/projects/vitiligo/data/sample' \\\n-H 'Authorization: Bearer &lt;your-access-token' \\\n-H 'Content-Type: application/json' \\\n-d '\n    {\n        \"name\": \"CL070_L2_V1_Bst_sc_iD\",\n        \"sample_type\": \"scRNAseq\",\n        \"technology\": \"inDrop\",\n        \"status\": \"Sequenced\",\n        \"contract\": \"scRNAseq\",\n        \"bead_occup\": \"65-70%\",\n        \"biosamp_id\":\"5fdb8820d6330eb80d503a31\"\n    }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"creationDate\": \"2021-03-31T03:16:58.503Z\",\n            \"lastUpdateDate\": \"2021-03-31T03:16:58.503Z\",\n            \"_id\": \"606453c0cb44bcfdbb84c6a2\",\n            \"name\": \"CL070_L2_V1_Bst_sc_iD\",\n            \"sample_type\": \"scRNAseq\",\n            \"technology\": \"inDrop\",\n            \"status\": \"Sequenced\",\n            \"contract\": \"scRNAseq\",\n            \"bead_occup\": \"65-70%\",\n            \"biosamp_id\": \"5fdb8820d6330eb80d503a31\",\n            \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n            \"owner\": \"5f39add38db6c9a200168a22\",\n            \"perms\": {\n                \"read\": {\n                    \"group\": [\n                        \"5fb4575faa5adff6f407f2d1\"\n                    ]\n                },\n                \"write\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                }\n            },\n            \"DID\": 135\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#update-a-data-field","title":"Update a Data Field","text":"<p>Update an existing data field.</p> <p>Example request:</p> <pre><code>bash\n$ curl \\\n  -X PATCH \\\n  -H \"Authorization: Bearer &lt;your-access-token\" \\\n  https://www.viafoundry.com/vmeta/api/v1/projects/vitiligo/data/sample/606453c0cb44bcfdbb84c6a2 \\\n  -H \"Content-Type: application/json\" \\\n  -d '\n      {\n        \"status\":\"Processed\"\n      }'\n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": {\n            \"creationDate\": \"2021-03-31T03:16:58.503Z\",\n            \"lastUpdateDate\": \"2021-03-31T03:16:58.503Z\",\n            \"_id\": \"606453c0cb44bcfdbb84c6a2\",\n            \"name\": \"CL070_L2_V1_Bst_sc_iD\",\n            \"sample_type\": \"scRNAseq\",\n            \"technology\": \"inDrop\",\n            \"status\": \"Processed\",\n            \"contract\": \"scRNAseq\",\n            \"bead_occup\": \"65-70%\",\n            \"biosamp_id\": \"5fdb8820d6330eb80d503a31\",\n            \"lastUpdatedUser\": \"5f39add38db6c9a200168a22\",\n            \"owner\": \"5f39add38db6c9a200168a22\",\n            \"perms\": {\n                \"read\": {\n                    \"group\": [\n                        \"5fb4575faa5adff6f407f2d1\"\n                    ]\n                },\n                \"write\": {\n                    \"group\": [\n                        \"5fb45793aa5adff6f407f2d2\"\n                    ]\n                }\n            },\n            \"DID\": 135\n        }\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/api/#delete-a-data-field","title":"Delete a Data Field","text":"<p>Delete an existing data value. NOTE: This action is irreversible, and should only be executed when absolutely certain of its intentions.</p> <p>Example request:</p> <pre><code>bash\n$ curl \\\n  -X DELETE \\\n  -H \"Authorization: Bearer &lt;your-access-token\" \\\n  https://www.viafoundry.com/vmeta/api/v1/projects/vitiligo/data/sample/606453c0cb44bcfdbb84c6a2 \n</code></pre> <p>Example response:</p> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"doc\": \"Deleted!\"\n    }\n}\n</code></pre>"},{"location":"ViaFoundry/app/","title":"App Guide","text":""},{"location":"ViaFoundry/app/#app-guide","title":"App Guide","text":""},{"location":"ViaFoundry/app/#basics","title":"Basics","text":"<p>In the top navigation bar, you will find the \"App\" button. Clicking on this button allows you to access all the shared or public apps available. If you have a specific app in mind that you would like to create, we are here to assist you. Simply contact us at support@viascientific.com for guidance and support.</p> <p></p>"},{"location":"ViaFoundry/app/#-shiny-app-debrowser","title":"-   Shiny App - DEBrowser","text":"<p>DEBrowser is an R library which provides an easy way to perform and visualize DE (Differential Expression) analysis. This module takes count matrices as input and allows interactive exploration of the resulting data. You can find their documentation here.</p> <p></p>"},{"location":"ViaFoundry/app/#-r-studio-r-markdown","title":"-   R-Studio - R-Markdown","text":"<p>The R-Studio launcher facilitates interactive analysis of the data generated from a run. We have prepared a set of R-Markdown reports that provide access to your report in HTML or PDF format immediately after the run is completed.</p> <p>For instance, the code below performs differential expression analysis for each comparison listed in the compare file. It generates volcano and MA plots for differentially expressed genes in each comparison:</p> <p></p>"},{"location":"ViaFoundry/app/#-jupyter-notebook","title":"-   Jupyter Notebook","text":"<p>The Jupyter Notebook app, due to its interactive and flexible nature, allows bioinformatics researchers to combine code, visualizations, and explanatory text in a single document. Bioinformaticians can write and execute code snippets in real-time, visualize data using various plotting libraries, and document their analyses step-by-step.</p> <p></p>"},{"location":"ViaFoundry/app/#-shiny-app-gsea-explorer","title":"-   Shiny App - GSEA Explorer","text":"<p>GSEA Explorer is an R library that offers a convenient method for conducting and visualizing Gene Set Enrichment Analysis (GSEA). GSEA aims to assess whether a specific gene set or pathway is enriched in gene expression data, indicating its potential biological significance in the studied condition. The GSEA Explorer application can be accessed after executing Foundry's complete RNA-sequencing pipeline or the standalone Differential Expression module. By leveraging GSEA Explorer, researchers can gain valuable insights into the functional implications of gene sets and pathways, aiding in the interpretation of RNA-seq results and facilitating a deeper understanding of biological mechanisms.</p> <p></p>"},{"location":"ViaFoundry/app/#-shiny-app-network-explorer","title":"-   Shiny App - Network Explorer","text":"<p>The Network Explorer allows bioinformaticians to explore and analyze these complex networks, helping them uncover hidden patterns, identify key players, and understand the underlying biological mechanisms. The Network Explorer application can be launched after running Foundry's full RNA-sequencing pipeline or the stand-alone Differential Expression module.</p> <p></p>"},{"location":"ViaFoundry/app/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"},{"location":"ViaFoundry/app_developer_guide/","title":"Application Guide","text":""},{"location":"ViaFoundry/app_developer_guide/#recommended-practices-for-app-development","title":"Recommended Practices for App Development","text":"<p>The following section contains guidelines to help you develop your own apps. Although nothing in this section is required, we find these guidelines helpful to create applications that are both reproducible and easy to maintain.</p>"},{"location":"ViaFoundry/app_developer_guide/#directory-structure","title":"Directory Structure","text":"<p>A directory structure similar to below can be tracked on Github and Dockerhub for easy version control and containerization.</p> <pre><code>|-- parent-directory/\n    |-- README.md\n    |-- Dockerfile\n    |-- app-directory/\n        |-- app.R\n        |-- R/\n            |-- # additional scripts sourced by app\n            |--\n            |--\n        |-- www/\n            |-- # CSS files\n            |--\n            |--\n        |-- data/\n            |-- # Data files used for app\n            |--\n            |--\n</code></pre>"},{"location":"ViaFoundry/app_developer_guide/#dockerfile","title":"Dockerfile","text":"<p>Docker is an open-source platform that allows developers to automate the deployment and management of applications within lightweight, isolated containers. It provides an additional layer of abstraction and standardization, enabling software to be packaged with all its dependencies and run consistently over time across different environments.</p> <p>When building a docker, it is best to aim for a minimalistic approach - only including the necessary components and dependencies required for your application.</p> <pre><code>FROM rocker/shiny:4.3.0\nLABEL author=\"zach@viascientific.com\" description=\"Docker image containing all requirements for the Via Scientific GSEA explorer App\"\n\nENV LANG=C.UTF-8 LC_ALL=C.UTF-8\n\n# Install System libraries\nRUN apt-get update --fix-missing &amp;&amp; \\\n    apt-get install -y gcc unzip curl make zlib1g-dev libglpk-dev libgmp3-dev libxml2-dev pandoc libicu-dev vim\n\n# Install required R packages\nRUN R -e 'install.packages(c(\"shiny\", \"BiocManager\", \"dplyr\", \"DT\", \"ggplot2\", \"shinydashboard\", \"shinydashboardPlus\", \"shinycssloaders\", \"igraph\", \"shinyjs\"), \\\n    repos=\"https://packagemanager.posit.co/cran/__linux__/focal/2023-06-01\")'\nRUN R -e \"BiocManager::install(version = '3.17')\"\nRUN R -e \"BiocManager::install(c('fgsea', 'clusterProfiler', 'org.Hs.eg.db', 'org.Mm.eg.db'))\"\n\n# Copy app directory onto image\nADD gsea-explorer /gsea-explorer/\n\n# Run the app when the container is started\nCMD [\"R\", \"-e\", \"shiny::runApp('/gsea-explorer')\"]\n</code></pre> <p>In the above example, a minimal base image (rocker/shiny:4.3.0) was selected that supports shiny applications. Only the necessary system libraries and R packages are installed. The entire app directory (gsea-explorer) is copied onto the image. Finally, the app is automatically started when the app is run.</p> <p>Posit's Package Manager is a useful tool in determining which system libraries are necessary for each R package. Navigate to the cran or bioconductor package of interest and look for the \"Install System Prerequisites\" section. Select the distribution that matches your base image.</p> <p>Additionally, when using <code>install.packages()</code>, setting the <code>repos</code> argument to use posit's package manager allows you to both download pre-built binaries (significantly speeding up docker build time) as well as freezing the version of each library to a specific date in time.</p>"},{"location":"ViaFoundry/app_developer_guide/#port-selection","title":"Port Selection","text":"<p>The shiny app file should set the host to <code>0.0.0.0</code> and specify the port. An example skeleton is below:</p> <pre><code>ui = dashboardPage(\n\n    # UI Code\n\n)\n\nserver = function(input, output, session) {\n\n    # Sever Code\n\n}\n\noptions(shiny.host = \"0.0.0.0\", shiny.port = 8789)\nshinyApp(ui = ui, server = server)\n</code></pre>"},{"location":"ViaFoundry/app_developer_guide/#host-app-on-foundry","title":"Host App on Foundry","text":"<p>To host the app on Foundry, navigate to the <code>Apps</code> page and select the <code>My Apps</code> tab and then click <code>Create App</code></p> <p></p>"},{"location":"ViaFoundry/app_developer_guide/#app-definition-file","title":"App Definition File","text":"<p>In the app definition file, set the following values:</p> <p> </p> <ol> <li>Set the name of the app</li> <li>If the docker image is hosted on Dockerhub, provide the path</li> <li>Select the container</li> <li>Set status to active</li> <li>The command to run when the container is activated. This can be left blank if the run command was set in the dockerfile. (as above <code>CMD [\"R\", \"-e\", \"shiny::runApp('/gsea-explorer')\"]</code>)</li> <li> <p>Set the port number. This should match the port that is set in the app.R file.</p> <pre><code>options(shiny.host = \"0.0.0.0\", shiny.port = 8789)\n</code></pre> </li> <li> <p>Set Websocket-reconnection-mode to None (default)</p> </li> <li>Set the Container Environment to     <pre><code>DISABLE_AUTH: true\nWWW_ROOT_PATH: \"/api/route/#{proxy.getRuntimeValue('SHINYPROXY_PROXY_ID')}/\"\n</code></pre></li> </ol>"},{"location":"ViaFoundry/app_developer_guide/#how-to-cite-us","title":"How To Cite Us","text":"<p>If you use Via Foundry (formerly DolphinNext) in your research, please cite:</p> <p>Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x</p>"},{"location":"ViaFoundry/app_developer_guide/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"},{"location":"ViaFoundry/cite/","title":"How To Cite Us","text":""},{"location":"ViaFoundry/cite/#how-to-cite-us","title":"How To Cite Us","text":"<p>If you use Via Foundry (formerly DolphinNext) in your research, please cite:</p> <p>Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x</p>"},{"location":"ViaFoundry/demo_TETranscripts_tutorial/","title":"demo TETranscripts tutorial","text":""},{"location":"ViaFoundry/demo_TETranscripts_tutorial/#expected-learning-outcome","title":"Expected learning outcome","text":"<p>To understand the basics of running a pipeline in Foundry by running the TE Transcripts analysis pipeline on sample mouse data.</p>"},{"location":"ViaFoundry/demo_TETranscripts_tutorial/#before-you-start","title":"Before you start","text":"<p>Please go to https://www.viafoundry.com and login into your account. If you have any issues logging in, please let us know (support@viascientific.com) and we will help to create an account for you.</p>"},{"location":"ViaFoundry/demo_TETranscripts_tutorial/#creating-a-project","title":"Creating a Project","text":"<p>In Foundry, analysis is organized by project. Each run belongs to a project and a project can consist of multiple runs.</p> <p>Once logged in, to create and configure a new project click on the <code>Projects</code> tab in the top menu and select <code>Add a New Project</code> button in the dropdown. In the pop-up, give the project a name (e.g. <code>TE Transcripts Tutorial</code>) and click save.</p> <p></p>"},{"location":"ViaFoundry/demo_TETranscripts_tutorial/#attaching-pipeline-to-project","title":"Attaching Pipeline to Project","text":"<p>To help with organization, pipelines used in a project are attached to that project.</p> <p>Note: The same pipeline can be attached to multiple projects.</p> <p>To attach a pipeline select the <code>Pipelines</code> tab and then click the <code>Add Pipeline</code> button. </p> <p></p> <p>Locate <code>TE Transcripts</code>, click on the <code>Add</code> button, and then close the window.</p> <p></p>"},{"location":"ViaFoundry/demo_TETranscripts_tutorial/#creating-a-run","title":"Creating a Run","text":"<p>Once the project is created and a pipeline is attached, you are ready to create a run:</p> <ol> <li> <p>Click the <code>Run</code> button next to the <code>TE Transcripts</code> entry in the table to load the \"Run Page\"</p> <p></p> </li> <li> <p>On the run page, under \"Run Environment\" select <code>viafoundry</code></p> </li> <li>In the Inputs section, next to <code>FASTQ Input</code>, click <code>Enter File</code></li> <li>In the files tab, click <code>Add File</code> button to enter new files.</li> <li> <p>Next to \"1. File Location\", enter:</p> <pre><code>gs://via-scientific-nprd-bucket/viafoundry/run_data/test_data/fastq_mouse\n</code></pre> </li> <li> <p>Click the magnifying glass icon. The box below will populate with files like so:</p> <p></p> </li> <li> <p>In the <code>3. Collection Type</code> dropdown, select <code>Paired List</code></p> </li> <li> <p>Under <code>4. File Pattern</code>, next to <code>Forward Pattern</code>, enter <code>.1</code>. Similarly, enter <code>.2</code> for <code>Reverse Pattern</code>.</p> <p></p> </li> <li> <p>Click <code>Add All Files</code> button. You should now see 6 entries below.</p> <p></p> </li> <li> <p>Next to <code>5. Collection Name</code>, enter <code>mousetest paired</code>. The final three boxes can be left blank. Click <code>Save Files</code></p> </li> <li>The \"Select/Add Input File\" screen will now have 6 entries. Click \"Save\".</li> <li>For \"Library Type\", select <code>pair</code></li> <li>For \"Genome Build\", select <code>mouse</code></li> <li>For \"Groups file\", click <code>Enter File</code></li> <li> <p>In the \"File Location\", enter:</p> <pre><code>gs://via-scientific-nprd-bucket/viafoundry/run_data/test_data/fastq_mouse_metadata/groups.tsv\n</code></pre> </li> <li> <p>Click <code>Save</code></p> </li> <li>For \"Comparison file\", click <code>Enter File</code></li> <li> <p>In the \"File Location\", enter:</p> <pre><code>gs://via-scientific-nprd-bucket/viafoundry/run_data/test_data/fastq_mouse_metadata/comparisons.tsv\n</code></pre> </li> <li> <p>Click <code>Save</code></p> </li> <li>Leave the rest of the inputs as defaults</li> <li>Click <code>Run</code> in the top right and then select <code>Start</code>. For this dataset, the TE Transcripts pipeline run typically takes several minutes to complete.</li> <li>Navigate to the Log tab and click on log.txt to follow the progress of your run.</li> <li>Once the status bar in the top right changes from a blue \"Running\" status to a green \"Completed\" status go to the Report tab to see the final reports.</li> <li> <p>Click on <code>Deseq2</code> to open the \"Differential Expression\" section. Select the first file on the left tab (exper_vs_control_DE.html). This is a differential expression report for the comparison. The report can be expanded to fit the full screen with the button on the top left.</p> <p></p> </li> <li> <p>Launch the <code>GSEA Explorer</code> application by clicking the second file on the left tab (exper_vs_control_DE.Rmd) and then click <code>Launch</code></p> </li> </ol> <p>Note: For the purposes of speeding up the runtime of this tutorial, the demo dataset has been downsampled to only include a few genes. Differential expression analysis graphs and Gene Set Enrichment Analysis graphs will look very sparse with this dataset.</p> <p>Congratulations! You have run and tested the TE Transcripts pipeline on Foundry!</p>"},{"location":"ViaFoundry/demo_developer_tutorial/","title":"Developer Tutorial - Creating Pipeline","text":""},{"location":"ViaFoundry/demo_developer_tutorial/#introduction","title":"Introduction","text":"<p>Via Foundry is an easy-to-use platform for creating, deploying, and executing complex nextflow pipelines for high throughput data processing.</p>"},{"location":"ViaFoundry/demo_developer_tutorial/#foundry-provides","title":"Foundry provides:","text":"<ol> <li>A drag and drop user interface to build nextflow pipelines</li> <li>Reproducible pipelines with version tracking </li> <li>Seamless portability to different computing environments with containerization</li> <li>Simplified pipeline sharing using GitHub (github.com)</li> <li>Support for continuous integration and tests (travis-ci.org)</li> <li>Easy re-execution of pipelines by copying previous runs settings</li> <li>Integrated data analysis and reporting interface with R markdown support</li> </ol> <p>Our aim is;</p> <ol> <li>Reusability</li> <li>Reproducibility</li> <li>Shareability</li> <li>Easy execution</li> <li>Easy monitoring</li> <li>Easy reporting</li> </ol>"},{"location":"ViaFoundry/demo_developer_tutorial/#expected-learning-outcome","title":"Expected learning outcome","text":"<p>To understand the basics of Foundry, how to use pipeline builder for different objectives and to familiarize yourself with Nextflow and some standard software packages for such analysis. This guide will walk you through how to start using Foundry pipelines and creating new pipelines.</p>"},{"location":"ViaFoundry/demo_developer_tutorial/#before-you-start","title":"Before you start","text":"<p>Please go to https://viafoundry.com and login into your account. If you have an issue about login, please let us know about it (support@viascientific.com). We will set an account for you.</p>"},{"location":"ViaFoundry/demo_developer_tutorial/#exercise-1-creating-processes","title":"Exercise 1 - Creating processes","text":"<p>Once logged in, click on the <code>Projects</code> section at the top menu and click <code>Add a New Project</code> button. This is the place to configure your project. To access pipeline builder page, click <code>Pipelines</code> tab and then click <code>Create Pipeline</code> button. </p> <p></p> <p>Now you can write a descripton about your pipeline using <code>Description</code> tab, start developing your pipeline using <code>Workflow</code> tab, and adding extra files or setting some extra parameters using <code>Advanced</code> tab. Let's get into some details about the pipeline elements.</p> <p></p>"},{"location":"ViaFoundry/demo_developer_tutorial/#what-is-a-process","title":"What is a \"process\"?","text":"<p>Process is a basic programming element in Nextflow to run user scripts. Please click here to learn more about Nextflow's processes.</p> <p>A process usually has inputs, outputs and script sections. In this tutorial, you will see sections that include necesseary information to define a process shown in the left side of the picture below. Please, use that information to fill \"Add new process\" form shown in the middle section in the picture below. Foundry will then convert this information to a nextflow process shown in the left side of the picture. Once a process created, it can be used in the pipeline builder. The example how it looks is shown in the bottom left side in the picture. The mapping between the sections shown in colored rectangles.      </p> <p></p>"},{"location":"ViaFoundry/demo_developer_tutorial/#the-process-we-will-create-in-this-exercise","title":"The process we will create in this exercise;","text":"<ol> <li>FastQC process</li> <li>Hisat2 process</li> <li>RSeQC process</li> </ol> <p>You\u2019ll notice several buttons at the left menu. New processes are created by clicking blue <code>New process</code> button .</p>"},{"location":"ViaFoundry/demo_developer_tutorial/#1-fastqc-process","title":"1. FastQC process","text":"<p>a. First, please click, blue <code>New process</code> button  in the left menu to open \"Add New Process\" window.</p> <p>b. Please enter FastQC for the process name and define a new \"Menu Group\". </p> <p></p> <p>c. In the FastQC process, we have an input, an output and a line of a command we are going to use to execute the fastqc process.</p> <pre><code>Name: \"FastQC\"\nMenu Group: \"Tutorial\"\nInputs: \n  reads(fastq,set) name: val(name),file(reads)\n\nOutputs: \n  outputFileHTML(html,file) name: \"*.html\"\n\nScript:\n  fastqc ${reads}\n</code></pre> <p>d. Lets select input and output parameters (<code>reads</code> and <code>outputFileHTML</code>) and define their \"Input Names\" that we are going to use in the script section.</p> <p></p> <p>e. Let's enter the script section</p> <p></p> <p>f. Press \"Save changes\" button at the bottom of the modal to create the process. Now this process is ready to use. We will use it in the Exercise 2.</p>"},{"location":"ViaFoundry/demo_developer_tutorial/#2-hisat2-process","title":"2. Hisat2 process","text":"<p>Let's create Hisat2 process. </p> <p>a. First, please click, blue \u201cNew process\u201d button to open \"Add New Process\" modal.</p> <p>b. Inputs, outputs and scripts should be defined like below;</p> <pre><code>Name: \"Hisat2\"\nMenu Group: \"Tutorial\"\nInputs: \n  reads(fastq,set) name: val(name),file(reads)\n  hisat2Index(file) name: hisat2Index\n\nOutputs: \n  mapped_reads(bam,set) name: val(name), file(\"${name}.bam\")\n  outputFileTxt(txt,file) name: \"${name}.align_summary.txt\"\n\nScript:\n  basename=\\$(basename ${hisat2Index}/*.8.ht2 | cut -d. -f1)\n  hisat2 -x ${hisat2Index}/\\${basename} -U ${reads} -S ${name}.sam &amp;&gt; ${name}.align_summary.txt\n  samtools view -bS ${name}.sam &gt; ${name}.bam \n</code></pre> <p>c. After you select input(<code>reads</code> and <code>hisat2Index</code>) and output parameters (<code>mapped_reads</code> and <code>outputFileTxt</code>), add their names and enter the script. The page should look like this;</p> <p></p> <p>d. Please save changes before you close the screen.</p>"},{"location":"ViaFoundry/demo_developer_tutorial/#3-rseqc-process","title":"3. RSeQC process","text":"<p>a. First, please click, blue \u201cNew process\u201d button to open \"Add New Process\" modal.</p> <p>b. The form should be filled using the information below;</p> <pre><code>Name: \"RSeQC\"\nMenu Group: \"Tutorial\"\nInputs:\n  mapped_reads(bam,set) name: val(name), file(bam)\n  bedFile(bed,file) name: bed\n\nOutputs: \n  outputFileTxt(txt,file) name: \"RSeQC.${name}.txt\"\n\nScript:\n  read_distribution.py  -i ${bam} -r ${bed}&gt; RSeQC.${name}.txt\n</code></pre> <p>c. After you select input output parameters, enter their names and the script. The page should look like this;</p> <p></p> <p>d. Please, save changes before you close the screen.</p> <p>Here Exercise 1 is finished. Please move to Exercise 2 to build the pipeline using the processes you created in Exercise 1. </p>"},{"location":"ViaFoundry/demo_developer_tutorial/#exercise-2-building-a-pipeline","title":"Exercise 2 - Building a pipeline","text":"<p>Before you start building the pipeline make sure you have the processes available in your Process Menu.</p> <p></p> <p>a. At the top of the page, you\u2019ll notice <code>Pipeline Name</code> box. You can rename your pipeline by clicking here. Please enter a name to your pipeline. E.g. \"RNA-Seq-Tutorial\" and press save button.</p> <p></p> <p>b. Please drag and drop FastQC, Hisat2 and RSeQC to your workspace;</p> <p></p> <p>c. Please drag and drop three <code>Input parameters</code> and change their names to <code>Input_Reads</code>, <code>Hisat2_Index</code> and <code>bedFile</code> and connect them to their processes;  </p> <p> </p> <p>d. Connect your Hisat2 process with RSeQC process using mapped_reads parameter in both. You will observe that, when the types match you can connect the two processes using their matching input and output parameters.</p> <p>e. Drag &amp; Drop three <code>output parameters</code> from the sidebar </p> <p></p> <p>and name them <code>FastQC_output</code>, <code>Hisat2_Summary</code>, and <code>RSeQC_output</code> and connect them to their corresponding processes. While naming, click their \"Publish to Web Directory\" and choose the right output format according to the output type of the process.</p> <p> </p> <p>f. Overall pipeline should look like below;</p> <p></p>"},{"location":"ViaFoundry/demo_developer_tutorial/#exercise-3-executing-a-pipeline","title":"Exercise 3 - Executing a pipeline","text":"<p>1. Once a pipeline is created, you will notice \u201cRun\u201d button at the right top of the page.</p> <p></p> <p>2. This button opens a new window where you can select your project by clicking on the project. You will then proceed by entering run name which will be added to your run list of the project. Clicking \u201cSave run\u201d will redirect you to the \u201crun page\u201d where you can initiate your run.</p> <p></p> <p>3. Here, please choose your <code>Run Environment</code> (Via Demo Environment(AWS Batch))</p> <p></p> <p>4. Then click the <code>Advanced</code> tab and go to <code>Run Container</code> section. Click <code>Use Docker Image</code> and enter the<code>Image Path</code> below;</p> <p></p><pre><code>Run Container:\nUse Docker Image: Checked\nImage Path: public.ecr.aws/t4w5x8f2/viascientific/rnaseq:3.0\n</code></pre> <p>5. Now, we are ready to enter the inputs we defined for the pipeline.    Click the <code>Run Settings</code> tab to enter bed file. Please use the Manually tab.</p> <pre><code>bedFile:  s3://viascientific/run_data/genome_data/mousetest/mm10/refseq_170804/genes/genes.bed\n</code></pre> <p></p> <p>6. Second, enter the hisat2 index directory. Please use the Manually tab.\u00a0 </p><pre><code>Hisat2_Index: s3://viascientific/run_data/genome_data/mousetest/mm10/refseq_170804/Hisat2Index\n</code></pre> <p></p>"},{"location":"ViaFoundry/demo_developer_tutorial/#creating-collection","title":"Creating Collection","text":"<p>7.  To enter Input_Reads, click <code>Enter File</code> button. Then go to <code>Files</code> Tab and click \"Add File\" button.</p> <p></p> <p>8.  Enter the location of your files and click Search button to get the list of files: </p> <p>File Location: </p><pre><code>s3://viascientific/run_data/test_data/fastq_mouse_single\n</code></pre> <p></p> <p>8. Then please choose <code>Single List</code> for the Collection Type and press <code>add all files</code> button.</p> <p>9. Here there is an option to change the names but we will keep them as they are. Enter a collection name and click \"save files\". </p><pre><code>collection name: test collection\n</code></pre> <p>10. In the next screen, the user can still add or remove some samples. Let's click \"Save file\" button to process all samples.</p> <p></p>"},{"location":"ViaFoundry/demo_developer_tutorial/#running-pipeline","title":"Running Pipeline","text":"<p>11. After we fill the inputs, the orange \"Waiting\" button at the top right should turn to green \"Run\" button. Now, you can press that button to start your run.</p> <p>12. All run should finish in a couple of minutes. When the run finalized the log section will be look like below;</p> <p>a. Logs:</p> <p></p> <p>b. Timeline:   </p>"},{"location":"ViaFoundry/demo_developer_tutorial/#reports","title":"Reports","text":"<p>13. In the report section, you can monitor all defined reports in the pipeline;</p> <p>a. FastQC </p> <p>b. Hisat2 </p> <p>c. RSeQC </p>"},{"location":"ViaFoundry/demo_metadata-basics/","title":"Metadata Tracker - Basics","text":""},{"location":"ViaFoundry/demo_metadata-basics/#expected-learning-outcome","title":"Expected learning outcome","text":"<p>To understand the basics of Metadata Tracking System.</p>"},{"location":"ViaFoundry/demo_metadata-basics/#before-you-start","title":"Before you start","text":"<p>Please go to https://www.viafoundry.com and login into your account. If you have an issue about login, please let us know about it (support@viascientific.com). We will set an account for you.</p>"},{"location":"ViaFoundry/demo_metadata-basics/#creating-project-and-adding-metadata-tracker","title":"Creating Project and Adding Metadata Tracker","text":"<p>Once logged in, click on the <code>Projects</code> section at the top menu and click <code>Add a New Project</code> button. Enter your project name and click OK. This is the place to configure your project. Click on the <code>Add Metadata Tracker</code> icon to add a new <code>Metadata</code> tab into your project. </p> <p></p> <p>Click on the <code>Metadata</code> tab. This window is the <code>Data View</code> section of the Metadata tracker where you will insert your data. Before inserting new data, we need to configure the database structure. To start configuring click on the \"Configure Metadata\" button at the right. </p> <p></p>"},{"location":"ViaFoundry/demo_metadata-basics/#configuring-metadata-tracker","title":"Configuring Metadata Tracker","text":"<ol> <li> <p>In this configuration window there are a couple of tabs available:</p> <ul> <li>All Collections: List of project collections(tables).</li> <li>All Events: List of events that are defined for Data view.</li> <li>Tree View: Shows your project collections(tables) in tree visualization.</li> <li>Templates: Predefined collections to import into your project</li> </ul> <p></p> </li> <li> <p>Please click the <code>Templates</code> tab to import predefined collections. Select all the collections by clicking checkboxes. After choosing them, click the <code>Import Collection</code> Button.</p> <p></p> </li> <li> <p>Now you can revisit <code>All Collections</code> and <code>Tree View</code> Tabs to see imported collections and their relationships.</p> <p> </p> </li> <li> <p>If you click each <code>Series</code> tab, you will see the columns defined for the <code>Series</code> collection. The NamingPattern feature is defined for the id column to set a unique id for each inserted value. Since the pattern is <code>SE-${AUTOINCREMENT}</code>, new data will have the following ids: SE-1, SE-2, etc.</p> <p></p> </li> <li> <p>Now click on the <code>Biosamples</code> tab to see its columns. Type of the series_id column is <code>mongoose.Schema.ObjectId</code> which means that this value will be referencing data in another collection. If you scroll to the right, you will see the ref column is set to <code>{yourProjectID}_series</code>, therefore it is referencing data in series collection in this project. By using this referencing method, we don't need to enter series details each time we insert data into the <code>Biosamples</code> collection. Instead, we will reference the data in the <code>Series</code> collection.</p> <p></p> </li> <li> <p>Similarly <code>Samples</code> collection referencing data in <code>Biosamples</code> collection and <code>Files</code> collection referencing <code>Samples</code> collection. You can visualize their relationship in the <code>Tree View</code> tab.</p> <p></p> </li> <li> <p>To add new columns to the Biosamples collection, click the plus button located at the top left corner.</p> New Column Name New Column Label Organism Organism Treatment Treatment Tissue Tissue <p>Enter new name and label values using the table above and click save button. You can see the example for the <code>Organism</code> column below.</p> <p></p> <p>This should create the following database structure:</p> <p></p> <pre><code>* Note: The name column has been normalized to lowercase.\n</code></pre> </li> <li> <p>Lets, enter new columns into Samples collection by clicking the plus button at the top left. Enter new name and label values using the table below.</p> New Column Name New Column Label Assay Type Assay Type Instrument Instrument <p>Expected database structure of sample collection:</p> <p></p> </li> <li> <p>Now we're ready to insert metadata. Click on the \"Data View\" Button, Located right handside of the tabs menu. </p> </li> </ol>"},{"location":"ViaFoundry/demo_metadata-basics/#importing-metadata","title":"Importing Metadata","text":"<ol> <li> <p>Click the <code>Collections</code> Tab and select <code>Series</code> Collection from dropdown. </p> <p></p> </li> </ol> <p>2.Click the Insert button to enter Demo Project into the name field using the form. After that, click the save button.</p> <pre><code>  ![image](../metadata_basic_images/series_insert.png)\n</code></pre> <ol> <li> <p>Importing Biosamples: Go to the collection dropdown and change its value to <code>Biosamples</code>. This time we will insert 6 Biosamples using Spreadsheet view. Click the <code>Edit in Spreadsheet</code> button. </p> <p></p> </li> <li> <p>You will see an empty Excel sheet with table headers. </p> <p></p> </li> <li> <p>Copy and paste the example data from here:  biosamples data to this empty sheet as shown below:</p> <p></p> <pre><code>Note: `series_id` column values belong to Series Collection IDs.\n</code></pre> </li> <li> <p>It will show the rows that are going to be inserted/updated in yellow. Click the <code>Save</code> button at the top left.</p> <p></p> </li> <li> <p>It will insert the rows as follows and the status of the operation will be shown at the left side.</p> <p></p> </li> <li> <p>Now you can return to table view by clicking the button below:</p> <p></p> </li> <li> <p>Importing Samples: Switch to <code>Samples</code> collection by using collection dropdown. Similar to the biosamples collection, click the <code>Edit in Spreadsheet</code> button. Copy and paste the example  here.:</p> <p></p> <pre><code>Note: `Biosamp_id` column values belong to Biosample Collection IDs.\n</code></pre> </li> <li> <p>Go to the collection dropdown and change its value to <code>Files</code>. This time click on the <code>Insert</code> Button. Then enter the following information:</p> <ul> <li>Enter the Directory: <code>https://web.dolphinnext.com/umw_biocore/dnext_data/tutorial/fastq_data/pair</code> and click the search button. This will list all the files on that location.</li> <li>File Type: FASTQ</li> <li>Collection Type: <code>Paired List</code></li> <li>Enter the R1 Pattern: <code>.1</code></li> <li>Enter the R2 Pattern: <code>.2</code></li> </ul> <p></p> <ul> <li>Click <code>All Files</code> button</li> <li>Select the Sample1 (SA-1)</li> <li>Click Save button.</li> </ul> <p></p> </li> <li> <p>Here we have assocaited all of the files with Sample1 (SA-1). To edit these files, you can click <code>Edit in Spreadsheet</code> button as follows and click the <code>Save</code> button:</p> <p></p> </li> </ol>"},{"location":"ViaFoundry/demo_metadata-basics/#submitting-a-foundry-run","title":"Submitting a Foundry Run","text":"<ol> <li> <p>You can use entered files in run submission. To simplify new run entry, we will create new Event in configuration section. While creating this event we will select only subset of Run Collection fields which will simplify the run submission process. To do that, click <code>Configure Metadata</code> button at the right. </p> </li> <li> <p>Click <code>All Events</code> tab and click <code>Insert</code> button. Enter <code>Event Name</code> as <code>New Run</code> and fill up the Event Form like following:</p> <ul> <li>Collection: Runs</li> <li>Field: Name</li> <li>Field: Server</li> <li>Field: Run Environment</li> <li>Field: Template Run ID</li> <li>Field: Project ID</li> <li>Field: Inputs</li> <li>Field: Outputs</li> <li>Field: Run Url</li> </ul> <p></p> </li> <li> <p>Click the <code>Save</code> Button to save new event. Now we're ready to submit a run. Click on the \"Data View\" Button, located right handside of the tabs menu. </p> </li> <li> <p>Under Events tab, select <code>New Run</code> option. Enter the following information:</p> <ul> <li>Name: Your run name</li> <li>Submission Type: Standard</li> <li>Server: Viafoundry Server</li> <li>Run Environment: Via Run Environment (AWS Batch)</li> <li>Template run id: Enter your previous run id or enter 1 to use our demo run as template.</li> <li>Inputs: Click <code>Select File</code> Button and use checkboxes to select your files.</li> <li>You can change other input values according to your needs.</li> </ul> <p></p> </li> <li> <p>Click the <code>Save</code> button. This will submit your run. You can track your run status on the dashboard.</p> </li> </ol> <p>Congratulations! You have configured a metadata tracker for your project and submit a run into Foundry!</p>"},{"location":"ViaFoundry/demo_metadata-geo/","title":"Metadata Tracker - NCBI-GEO","text":""},{"location":"ViaFoundry/demo_metadata-geo/#expected-learning-outcome","title":"Expected learning outcome","text":"<p>To understand the basics of Metadata Tracking System, and import sample GEO metadata.</p>"},{"location":"ViaFoundry/demo_metadata-geo/#before-you-start","title":"Before you start","text":"<p>Please go to https://www.viafoundry.com and login into your account. If you have an issue about login, please let us know about it (support@viascientific.com). We will set an account for you.</p>"},{"location":"ViaFoundry/demo_metadata-geo/#creating-project-and-adding-metadata-tracker","title":"Creating Project and Adding Metadata Tracker","text":"<p>Once logged in, click on the <code>Projects</code> section at the top menu and click <code>Add a New Project</code> button. Enter your project name and click OK. This is the place to configure your project. Click on the <code>Add Metadata Tracker</code> icon to add a new <code>Metadata</code> tab into your project. </p> <p></p> <p>Click on the <code>Metadata</code> tab. This window is the <code>Data View</code> section of the Metadata tracker where you will insert your data. Before inserting new data, we need to configure the database structure. To start configuring click on the \"Configure Metadata\" button at the right. </p> <p></p>"},{"location":"ViaFoundry/demo_metadata-geo/#configuring-metadata-tracker","title":"Configuring Metadata Tracker","text":"<ol> <li> <p>In this configuration window there are a couple of tabs available:</p> <ul> <li>All Collections: List of project collections(tables).</li> <li>All Events: List of events that are defined for Data view.</li> <li>Tree View: Shows your project collections(tables) in tree visualization.</li> <li>Templates: Predefined collections to import into your project</li> </ul> <p></p> </li> <li> <p>Please click the <code>Templates</code> tab to import predefined collections. Select all the collections by clicking checkboxes. After choosing them, click the <code>Import Collection</code> Button.</p> <p></p> </li> <li> <p>Now you can revisit <code>All Collections</code> and <code>Tree View</code> Tabs to see imported collections and their relationships.</p> <p> </p> </li> <li> <p>If you click each <code>Series</code> tab, you will see the columns defined for the <code>Series</code> collection. The NamingPattern feature is defined for the id column to set a unique id for each inserted value. Since the pattern is <code>SE-${AUTOINCREMENT}</code>, new data will have the following ids: SE-1, SE-2, etc.</p> <p></p> </li> <li> <p>Now click on the <code>Biosamples</code> tab to see its columns. Type of the series_id column is <code>mongoose.Schema.ObjectId</code> which means that this value will be referencing data in another collection. If you scroll to the right, you will see the ref column is set to <code>{yourProjectID}_series</code>, therefore it is referencing data in series collection in this project. By using this referencing method, we don't need to enter series details each time we insert data into the <code>Biosamples</code> collection. Instead, we will reference the data in the <code>Series</code> collection.</p> <p></p> </li> <li> <p>Similarly <code>Samples</code> collection referencing data in <code>Biosamples</code> collection and <code>Files</code> collection referencing <code>Samples</code> collection. You can visualize their relationship in the <code>Tree View</code> tab.</p> <p></p> </li> <li> <p>Let's visit NCBI SRA Run Selector (https://www.ncbi.nlm.nih.gov/Traces/study/) to download sample project metadata. Enter <code>GSE196908</code> into the <code>Accession</code> field and click the search button.</p> <p></p> </li> <li> <p>Click on the <code>Metadata</code> button to download comma separated metadata files. </p> <p></p> </li> <li> <p>To visualize the file in excel or another spreadsheet viewer, change the file extension to csv.</p> <pre><code>SraRunTable.txt -&gt; SraRunTable.csv\n</code></pre> </li> <li> <p>Open the file to check its content.</p> <p></p> </li> <li> <p>Lets distribute these column headers into 4 groups (series, biosamples, samples, file) to prevent repetition and save into our project.</p> series biosamples samples files BioProject BioSample Sample Name Run Organism Assay Type source_name Library Name TREATMENT LibraryLayout weeks_treatment LibrarySelection LibrarySource Instrument Platform </li> <li> <p>Return back to Foundry Metadata tracker, and click the Series tab. Instead of auto increment values (SE-1, SE-2 etc,) using BioProject-ID for referencing would simplify our referencing strategy. While entering data, we will enter BioProject-ID into the name column and we want to save this entered value as <code>id</code>. Change the <code>NamingPattern</code> of <code>id</code> column as follows to save <code>name</code> value as <code>id</code>:</p> Column Field of change Old Value New Value id NamingPattern <code>SE-${AUTOINCREMENT}</code> <code>${series.name}</code> <p>Note</p> <p>'series' is the name of the Series collection. Please check <code>All Collections</code> tab and Name and Label columns.</p> </li> <li> <p>Similarly, to make <code>name</code> column a identifier for biosamples, click on Biosamples Tab and change the <code>NamingPattern</code> of <code>id</code> column as follows: </p> Column Field of change Old Value New Value id NamingPattern <code>B-${AUTOINCREMENT}</code> <code>${biosamp.name}</code> <p>Note</p> <p>'biosamp' is the name of the Biosamples collection. All details listed in <code>All Collections</code> tab.</p> </li> <li> <p>Insert new columns into Biosamples collection by clicking the plus button at the top left while in the Biosamples tab.</p> New Column Name New Column Label Organism Organism source_name source_name TREATMENT TREATMENT weeks_treatment weeks_treatment <p>Enter new name and label values using the table above and click save button. You can see the example for the <code>Organism</code> column below.</p> <p></p> <p>This should create the following database structure:</p> <p></p> <pre><code>* Note: the id column has been normalized to lower case.\n</code></pre> </li> <li> <p>Click Samples Tab and change the <code>NamingPattern</code> of <code>id</code> column as follows: </p> Column Field of change Old Value New Value id NamingPattern <code>SA-${AUTOINCREMENT}</code> <code>${sample.name}</code> <pre><code>* Note: 'sample' is the name of the Samples collection. \n</code></pre> </li> <li> <p>Insert new columns into Samples collection by clicking the plus button at the top left. Enter new name and label values using the table below.</p> New Column Name New Column Label Assay Type Assay Type Library Name Library Name LibraryLayout LibraryLayout LibrarySelection LibrarySelection LibrarySource LibrarySource Instrument Instrument Platform Platform <p>Expected database structure of sample collection:</p> <p></p> </li> <li> <p>Click Files Tab and change the <code>NamingPattern</code> of <code>id</code> column as follows: </p> Column Field of change Old Value New Value id NamingPattern <code>F-${AUTOINCREMENT}</code> <code>${file.name}</code> </li> <li> <p>Now we're ready to insert metadata. Click on the \"Data View\" Button, Located right handside of the tabs menu. </p> </li> </ol>"},{"location":"ViaFoundry/demo_metadata-geo/#importing-metadata","title":"Importing Metadata","text":"<ol> <li> <p>Click the <code>Collections</code> Tab and select <code>Series</code> Collection from dropdown. </p> <p></p> </li> <li> <p>Click on the <code>Insert</code> button to enter <code>PRJNA807693</code> into the name field using form. After that click the save button. </p> <p></p> </li> <li> <p>Go to the collection dropdown and change its value to <code>Biosamples</code>. Since we will insert 99 Biosamples we will use excel import. Let's download the template excel file to facilitate import. Click the <code>Download Last Viewed Page as Excel file</code> button. You will see an empty excel sheet with table headers. If you like you can download the biosamples template here.</p> <p></p> <p></p> </li> <li> <p>Copy and paste the data in SraRunTable into this empty sheet as follows:</p> <p></p> <pre><code>Note: `BioSample` column in SraRunTable is copied to the `Name` column.\n</code></pre> </li> <li> <p>After saving this file, click on the <code>Import on Excel File</code> button. Drag and drop your Biosamples.xlsx file and click the <code>Load Table</code> button. If you prefer you can download and use  the pre-filled biosamples table here.</p> <p></p> </li> <li> <p>It will show the rows that are going to be inserted/updated in yellow color. Click the Save button at the top left.</p> <p></p> </li> <li> <p>It will insert the rows as follows and the status of the operation will be shown at the left side.</p> <p></p> </li> <li> <p>Now you can return to table view by clicking the button below:</p> <p></p> </li> <li> <p>Switch to <code>Samples</code> collection by using collection dropdown. Similar to the biosamples collection, click the <code>Download Last Viewed Page as Excel file</code> button. Copy and paste the data in SraRunTable into this empty sheet as follows or you can download the samples template   here.:</p> <p></p> <pre><code>Note: `Sample Name` column in SraRunTable is copied to the `Name` column.\n</code></pre> </li> <li> <p>After saving this file, click on the <code>Import on Excel File</code> button. If you prefer you can use samples table by  clicking here. Drag and drop your Samples.xlsx file and click the <code>Load Table</code> button. Now you're ready to import sample data with the save button. Return back to the table view by clicking the <code>Show Table Format</code> button.</p> </li> <li> <p>Go to the collection dropdown and change its value to <code>Files</code>. Click the <code>Download Last Viewed Page as Excel file</code> button or download  here  Copy and paste the data in SraRunTable into this empty sheet as follows or  download here :</p> <p></p> </li> <li> <p>If you prefer you can use entered files in run submission. Select the <code>Runs</code> collection and click <code>Insert</code> button. </p> <ul> <li>Name: Your run name</li> <li>Submission Type: Standard</li> <li>Server: Viafoundry Server</li> <li>Run Environment: Via Run Environment (AWS Batch)</li> <li>Template run id: Enter your previous run id or enter 1 to use our demo run as template.</li> <li>Inputs: Click <code>Select File</code> Button and use checkboxes to select your files.</li> <li>You can change other input values according to your needs.</li> </ul> <p></p> </li> <li> <p>Click the save button. This will submit your run. You can track your run status on the dashboard.</p> </li> </ol> <p>Congratulations! You have configured a metadata tracker for your project and imported GEO data into Foundry!</p>"},{"location":"ViaFoundry/demo_metadata-portal/","title":"Metadata Portal","text":""},{"location":"ViaFoundry/demo_metadata-portal/#expected-learning-outcome","title":"Expected learning outcome","text":"<p>To understand the basics of Metadata Portal.</p>"},{"location":"ViaFoundry/demo_metadata-portal/#before-you-start","title":"Before you start","text":"<p>Please go to https://www.viafoundry.com and login into your account. If you have an issue about login, please let us know about it (support@viascientific.com). We will set an account for you.</p>"},{"location":"ViaFoundry/demo_metadata-portal/#adding-metadata-portal","title":"Adding Metadata Portal","text":"<p>Once logged in, first complete Metadata Tracker - Basics Tutorial. After that Click on the <code>Add Metadata Portal</code> icon to add a new <code>Portal</code> tab into your project. </p> <p></p> <p>To visualize Metadata information, we need to define two API routes in the Metadata Tracker Section. To start configuring API routes click on the <code>Metadata</code> tab and \"Configure Metadata\" button at the right. </p> <p></p>"},{"location":"ViaFoundry/demo_metadata-portal/#configuring-metadata-tracker","title":"Configuring Metadata Tracker","text":"<ol> <li>In this configuration section, click on <code>API Config</code> tab and Click Insert Button.</li> <li> <p>Add Samples Route and its Required Fields</p> <ul> <li>Target Collection: sample</li> <li>Route Description: detailed</li> <li>Config: Click New Collection Group, Select <code>Samples</code> Collection and add the following required fields. You might choose to add other fields to visualize in Portal.</li> </ul> <p>Collection: Samples</p> Field Rename _id  (required) _id creationDate (required) date_created Name  (required) name owner owner Assay Type assay_type <ul> <li>Click <code>New Collection Group</code>, Select <code>Biosamples</code> Collection and add the following fields.</li> </ul> <p>Collection: Biosamples</p> Field Rename Name (required) biosample_name Organism organism Tissue tissue Treatment treatment <ul> <li>Click <code>New Collection Group</code>, Select <code>Series</code> Collection and add the following fields.</li> </ul> <p>Collection: Series</p> Field Rename Name (required) series </li> <li> <p>Click <code>Save API config</code> button to create <code>/api/v1/projectid/${project_id}/data/sample/format/detailed</code> API route.</p> </li> <li> <p>Add File Route and it's Required Fields</p> <ul> <li>Target Collection: file</li> <li>Route Description: summary</li> <li>Config: Click <code>New Collection Group</code> and select <code>Files</code> Collection and add the following required fields.</li> </ul> <p>Collection: Files</p> Field Rename _id (required) _id Name name owner owner Used Files file_used Directory file_dir Archive Directory archive_dir S3 Archive Directory s3_archive_dir Google Storage Archive Directory gs_archive_dir Collection Type collection_type File Type file_type creationDate date_created <ul> <li>Click New Collection Group, Select <code>Samples</code> Collection and add the following fields.</li> </ul> <p>Collection: Samples</p> Field Rename _id (required) sample_id <p>Click New Collection Group, Select <code>Series</code> Collection and add the following fields.</p> <p>Collection: Series</p> Field Rename Name collection_name  (required) Name project_name (required) </li> <li> <p>Click <code>Save API config</code> button to create <code>/api/v1/projectid/${project_id}/data/file/format/summary</code> API route.</p> </li> </ol>"},{"location":"ViaFoundry/demo_metadata-portal/#portal-configuration","title":"Portal Configuration","text":"<p>To visualize the Metadata in Portal, you need to configure the retrieved API data from Metadata Tracker. </p> <p></p> <ol> <li>Click the <code>Configure Portal</code> button at the top right and then click the <code>Add Config</code> button</li> <li>Choose a project</li> <li>Choose Sample API route: e.g. (<code>/api/v1/projectid/${project_id}/data/sample/format/detailed</code>)</li> <li>Graph Settings: Choose three fields and labels that you want to visualize on the three bar graphs.</li> <li> <p>Column Settings:</p> <ul> <li>Column Label: You might rename the received data column name here. </li> <li>Main Table Column: Check the checkbox if you want to show the data in the main Portal table.</li> <li>Visible on Load: Check the checkbox if you want to show column data on the initial load of the page.</li> <li>Allow Toggle: Add field into Toggle section so user might show/hide that column.</li> <li>Show in the sidebar: Show field in the left sidebar.</li> </ul> </li> <li> <p>Column Order: You might change the Main Portal column order here by drag and drop.</p> </li> </ol> <p></p> <p>Congratulations! You have configured a metadata portal for your project!</p>"},{"location":"ViaFoundry/demo_rna-seq_tutorial/","title":"Running RNA-Seq Pipeline","text":""},{"location":"ViaFoundry/demo_rna-seq_tutorial/#expected-learning-outcome","title":"Expected learning outcome","text":"<p>To understand the basics of running a pipeline in Foundry by performing RNA-seq analysis on sample mouse data.</p>"},{"location":"ViaFoundry/demo_rna-seq_tutorial/#before-you-start","title":"Before you start","text":"<p>Please go to https://www.viafoundry.com and login into your account. If you have any issues logging in, please let us know (support@viascientific.com) and we will create an account for you.</p>"},{"location":"ViaFoundry/demo_rna-seq_tutorial/#creating-a-project","title":"Creating a Project","text":"<p>In Foundry, analysis is organized by project. Each run belongs to a project and a project can consist of multiple runs.</p> <p>Once logged in, to create and configure a new project click on the <code>Projects</code> tab in the top menu and select <code>Add a New Project</code> button in the dropdown. In the pop-up, give the project a name (e.g. <code>RNA-seq Tutorial</code>) and click save.</p> <p></p>"},{"location":"ViaFoundry/demo_rna-seq_tutorial/#attaching-pipeline-to-project","title":"Attaching Pipeline to Project","text":"<p>To help with organization, pipelines used in a project are attached to that project.</p> <p>Note: The same pipeline can be attached to multiple projects.</p> <p>To attach a pipeline select the <code>Pipelines</code> tab and then click the <code>Add Pipeline</code> button. </p> <p></p> <p>Locate the <code>RNA-seq Pipeline</code>, click on the <code>Add</code> button, and then close the window.</p> <p></p> <p>Note: For the purposes of this tutorial, be sure to add <code>RNA-seq Pipeline</code>, and not <code>RNA-seq Pipeline (NF-Core)</code></p>"},{"location":"ViaFoundry/demo_rna-seq_tutorial/#creating-a-run","title":"Creating a Run","text":"<p>Once the project is created and a pipeline is attached, you are ready to create a run:</p> <ol> <li> <p>Click the <code>Run</code> button next to the <code>RNA-seq Pipeline</code> entry in the table to load the \"Run Page\"</p> <p></p> </li> <li> <p>On the run page, under \"Run Environment\" select <code>Via Demo Environment (AWS Batch)</code></p> </li> <li>In the Inputs section, next to <code>reads</code>, click <code>Enter File</code></li> <li>In the files tab, click <code>Add File</code> button to enter new files.</li> <li> <p>Next to \"1. File Location\", enter:</p> <pre><code>s3://viascientific/run_data/test_data/fastq_mouse\n</code></pre> </li> <li> <p>Click the magnifying glass icon. The box below will populate with files like so:</p> <p></p> </li> <li> <p>In the <code>3. Collection Type</code> dropdown, select <code>Paired List</code></p> </li> <li> <p>Under <code>4. File Pattern</code>, next to <code>Forward Pattern</code>, enter <code>.1</code>. Similarly, enter <code>.2</code> for <code>Reverse Pattern</code>.</p> <p></p> </li> <li> <p>Click <code>Add All Files</code> button. You should now see 6 entries below.</p> <p></p> </li> <li> <p>Next to <code>5. Collection Name</code>, enter <code>rna-seq mousetest paired</code>. The final three boxes can be left blank. Click <code>Save Files</code></p> </li> <li>The \"Select/Add Input File\" screen will now have 6 entries. Click \"Save\".</li> <li>For \"mate\", select <code>pair</code></li> <li>For \"genome_build\", select <code>mousetest_mm10</code></li> <li>Leave the rest of the inputs as defaults</li> <li>Click <code>Run</code> in the top right and then select <code>Start</code>. For this dataset, the RNA-seq pipeline run typically takes several minutes to complete.</li> <li>Navigate to the Log tab and click on log.txt to follow the progress of your run.</li> <li>Once the status bar in the top right changes from a blue \"Running\" status to a green \"Completed\" status go to the Report tab to see the final reports.</li> <li> <p>Click on <code>Mulitiqc</code> to open the \"MultiQC\" section. Scroll down to find this plot, which shows aligned reads per library:</p> <p></p> </li> <li> <p>Open the <code>Overall Summary</code> section to check mapping rates:</p> <p></p> </li> <li> <p>Open the <code>RSEM Module</code> section. that has a \"View Format\" of \"Table\" to download a count table:</p> <p></p> </li> <li> <p>Open the <code>RSEM Module</code> section. that has a \"View Format\" of \"App\" and select the \"DEBrowser App\" and click \"Launch App\" to perform Differential Expression Analysis with DEBrowser.</p> <p></p> </li> </ol> <p>Note: For the purposes of speeding up the runtime of this tutorial, the demo dataset has been downsampled to only include a few genes. Differential expression analysis graphs will look very sparse with this dataset.</p> <p>Congratulations! You have run and tested a RNA-seq pipeline on Foundry!</p>"},{"location":"ViaFoundry/demo_run_submission_api/","title":"Run Submission through API","text":""},{"location":"ViaFoundry/demo_run_submission_api/#tutorial-run-submission-through-api","title":"Tutorial: Run Submission through API","text":"<p>In this tutorial, we will guide you through the process of submitting runs using the provided Python scripts <code>triggerRun.py</code> and <code>getRuns.py</code>. These scripts interact with the ViaFoundry API to trigger a pipeline run and retrieve the list of runs, respectively.</p>"},{"location":"ViaFoundry/demo_run_submission_api/#prerequisites","title":"Prerequisites","text":"<p>Before you start, make sure you have the following:</p> <ul> <li>Python installed on your machine (version 3.6 or later).</li> <li>ViaFoundry account credentials (email and password).</li> </ul>"},{"location":"ViaFoundry/demo_run_submission_api/#step-1-download-the-scripts","title":"Step 1: Download the Scripts","text":"<p>Download the following Python scripts from the provided links:</p> <ul> <li>triggerRun.py</li> <li>getRuns.py</li> </ul> <p>Save these files to your local machine.</p>"},{"location":"ViaFoundry/demo_run_submission_api/#step-2-set-environment-variables","title":"Step 2: Set Environment Variables","text":"<p>Open a terminal and set the required environment variables for ViaFoundry credentials:</p> <pre><code>export VIAFOUNDRY_EMAIL=your_email@example.com\nexport VIAFOUNDRY_PASSWORD=your_password\n</code></pre> <p>Replace <code>your_email@example.com</code> and <code>your_password</code> with your ViaFoundry account credentials.</p>"},{"location":"ViaFoundry/demo_run_submission_api/#step-3-update-triggerrunpy-with-run-details","title":"Step 3: Update <code>triggerRun.py</code> with Run Details","text":"<p>Open the <code>triggerRun.py</code> file and update the <code>run_settings</code> dictionary with your specific run details:</p> <pre><code>run_settings = {\n    \"doc\": {\n        \"name\": \"New Run Name\",\n        \"tmplt_id\": 861,\n        \"in\": {\n            \"testFile\": \"s3://viafoundry/run_data/test_data/models/data/AmpC_screen_table_subset_10K.csv\",\n            \"numLine\": \"3000\",\n            \"Header\": \"smiles\"\n        }\n    }\n}\n</code></pre> <p>Replace the values in the <code>run_settings</code> dictionary with the specific configuration for your run. In this context, the term <code>tmplt_id</code> refers to the template run ID. The global pipeline inputs include parameters such as <code>testFile</code>, <code>numLine</code>, and <code>Header</code>. Generally, the remaining pipeline parameters will be automatically derived from the template run parameters.</p>"},{"location":"ViaFoundry/demo_run_submission_api/#step-4-run-triggerrunpy","title":"Step 4: Run <code>triggerRun.py</code>","text":"<p>Execute the following command in the terminal to trigger the pipeline run:</p> <pre><code>python triggerRun.py\n</code></pre> <p>This will log in to ViaFoundry, obtain an access token, and trigger the specified pipeline run.</p>"},{"location":"ViaFoundry/demo_run_submission_api/#step-5-run-getrunspy","title":"Step 5: Run <code>getRuns.py</code>","text":"<p>Execute the following command in the terminal to retrieve the list of runs:</p> <pre><code>python getRuns.py\n</code></pre> <p>This will log in to ViaFoundry, obtain an access token, and print the list of runs.</p> <p>Congratulations! You have successfully triggered a pipeline run and retrieved the list of runs using the ViaFoundry API.</p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/","title":"Running Cellranger Pipeline","text":""},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#expected-learning-outcome","title":"Expected learning outcome","text":"<p>To understand the basics of Foundry and run a single-cell RNA-Seq pipeline with sample data.</p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#before-you-start","title":"Before you start","text":"<p>Please go to https://www.viafoundry.com and log in to your account. If you have any issues with logging in, please let us know (support@viascientific.com). We will set-up an account for you.</p> <p></p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#creating-a-project","title":"Creating a Project","text":"<p>Once logged in, (1) click on the <code>Projects</code> drop down on the top menu. This is the place to configure your projects.</p> <p>To create a new project, (2) select <code>Add a New Project</code>.</p> <p></p> <p>In the <code>New Project</code> pop-up, (3) give the new project a name and (4) click save.</p> <p></p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#creating-a-run","title":"Creating a Run","text":"<p>Once a project is created, to access pipelines (1) click the <code>Pipelines</code> tab and then (2) click <code>Add Pipeline</code> in the top right. </p> <p></p> <p>In the <code>Add pipeline</code> window, (3) click the <code>Add</code> button for the \"Cell Ranger Pipeline\" and close the window (bottom right). </p> <p></p> <p>To bring up the run page, (4) click the <code>Run</code> button of the Cell Ranger pipeline on the table.</p> <p></p> <p>[Optional] To provide a run name, (5) click inside the box next to <code>Run</code>. Provide a new name and click anywhere outside the box to exit.</p> <p>[Optional] To provide a run description, (6) click the pencil next to <code>Run Description</code>. Provide a description and (7) click on the checkbox to finish editing.</p> <p>Under <code>Run Environment</code>, (8) select \"Via Run Environment (AWS Batch)\"</p> <p></p> <p>To add the source data, under <code>User Inputs</code> next to <code>Reads</code> (9) click <code>Enter File</code></p> <p></p> <p>In the <code>Select/Add Input File</code> window, (10) select the <code>Files</code> tab and then (11) click <code>+ Add File</code></p> <p></p> <p>Here if you need to upload your files to AWS S3, please check out the file upload video below:</p> <p></p> <p>In the <code>Add File</code> window, (12) select the <code>Remote Files</code> tab and in the <code>1. File Location</code> box (13) paste the following path: </p> <pre><code>s3://viafoundry/run_data/test_data/fastq_10x_pbmc_1k_v3\n</code></pre> <p>The <code>Amazon Keys for S3</code> box should appear. The <code>Amazon Keys for S3</code> should automatically set itself to \"Via Scientific AWS Key\". <code>If you prefer to use another AWS Key for another s3 bucket, please change \"Amazon Keys for S3\" dropdown option.</code> </p> <p>Click (14) search button to see the content of the s3 directory. Leave <code>2. File Type</code> as \"FASTQ\" and in the <code>3. Collection Type</code> dropdown, (15) select 'Paired List'. </p> <p></p> <p>Under <code>4. File Pattern</code>, check that <code>R1 Pattern</code> is set to <code>_R1</code> and similarly that the <code>R2 Pattern</code> is set to <code>_R2</code>. For this sample, there were two sequencing lanes run (L001 and L002). To merge the two lanes, (16) hold shift while clicking on <code>pbmc_1k_v3_S1_L001_R1_001.fastq.gz</code> and (17) <code>pbmc_1k_v3_S1_L002_R1_001.fastq.gz</code>. The corresponding R2 will be automatically selected. (18) Click \"Merge Selected Files\" to combine the two lanes.</p> <p>Tip</p> <ul> <li>For other datasets, if you don't need to merge samples, you can select the samples you want to add and click  <code>Add Selected Files</code> button. </li> <li>If you prefer to add all files that match the pattern, you can click <code>Add All Files</code> button.</li> </ul> <p></p> <p>(19) Update the 'Name' to <code>pmbc_1k_v3_S1</code>. And input <code>fastq_10x_pbmc_1k_v3</code> as the <code>5. Collection Name</code>. The final three boxes can be left blank. (21) Click \"Save Files\".</p> <p></p> <p>This will return to the <code>Change Input File</code> window. (22) Click \"Save\" again.</p> <p></p> <p>Since this sample has paired end reads, (23) ensure the <code>mate</code> dropdown is set to \"pair\". To finish the <code>Metadata</code> section (24) click \"Enter File\".</p> <p></p> <p>Metadata can be entered in three ways: Via a path to a file in the cloud, dropping a local file into the box, or in simple cases directly in the table. Notice the sample name is pulled in from the sample selection. To finish the table (25) fill in the \"Condition\" column with <code>pmbc_1k_v3</code> and (26) click \"Save\".</p> <p></p> <p>Pick the genome by (27) selecting \"human_hg38_gencode_v32_cellranger_v6\" in the <code>genome_build</code> dropdown. </p> <p>Optional: Adding custom genome sequence to selected genome_build</p> <p>If you prefer to add custom genome sequence to selected genome_build, enable three inputs:</p> <ul> <li> <p>set <code>run_mkref</code> to \"Yes\" (28)</p> </li> <li> <p>set <code>add_sequences_to_reference</code> to \"Yes\" (29)</p> </li> <li> <p>set <code>run_Download_Genomic Sources</code> to \"Yes\" (30)</p> </li> </ul> <p>Warning</p> <p>This change will add approx. 1 hour to run time since it will build new indexes with entered custom sequences.  If you select no to these inputs it will use prebuilt indexes. </p> <p>To supply the new sequences (31) click the wrench in the <code>add_sequences_to_reference</code> section.</p> <p></p> <p>In the <code>Process Settings</code> window (32) click \"Enter File\"</p> <p></p> <p>In the <code>Enter File</code> window (33) you can drag and drop your local genome file or enter a s3 path into <code>File Location</code>:</p> <pre><code>s3://viafoundry/run_data/test_data/fasta_EGFP/EGFP.fa\n</code></pre> <p>and (34) click \"Save\" to return to the <code>Process Settings</code> window.</p> <p></p> <p>(35) Click \"OK\" to submit the changes.</p> <p></p> <p>In conclusion, all settings should be set to \"yes\", except <code>run_Aggregate_Libraries</code> and <code>genome_build</code> which was set to \"human_hg38_gencode_v32_cellranger_v6\".</p> <p></p> <p>Finally, to submit the run (36) click \"Run\" in the top right and (37) select \"Start\"</p> <p></p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#analyzing-reports","title":"Analyzing Reports","text":"<ul> <li>RNA-Seq pipeline runs typically take 2.5 hours (if you enter custom genome sequence) to complete for this dataset.</li> <li>Navigate to the Log tab and click on log.txt to see progress on your run.</li> <li>Once the blue \"Running\" in the top right changes to a green \"Completed\" go to the Report tab to see the final reports.</li> <li>While waiting for the run to finish, you can check the example finalized run: https://www.viafoundry.com/run/101</li> </ul>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#executing-scrna-seurat-10x-module-optional","title":"Executing scRNA Seurat 10x Module (Optional)","text":"<ul> <li>Instead of running whole 10x pipeline, you might execute only scRNA Seurat 10x Module too. This module would take 10 minutes to run.</li> <li>Go to your <code>new project</code> by clicking on the <code>Projects</code> drop down on the top menu and clicking on your <code>new project</code>.</li> <li>Click <code>Pipeline Tab</code> and click <code>Add Pipeline</code> Button.</li> <li>In the <code>Add pipeline</code> window, click the <code>Add</code> button for the <code>scRNA Seurat 10x</code> and close the window. </li> <li>Click the <code>Run</code> button of the <code>scRNA Seurat 10x</code> on the table.</li> <li>Choose Run Environment as <code>Via Run Environment (AWS Batch)</code></li> <li>Choose genome_build as <code>human_hg38_gencode_v32_cellranger_v6</code></li> <li>Click <code>Enter File</code> for Metadata. Fill in the \"Sample\" column with \"pbmc_1k_v3_S1\" and \"Condition\" column with <code>pmbc_1k_v3</code> click \"Save\".</li> <li>Enter 101 for Data_Path which is the already finished 10x pipeline run id.</li> <li>Click Run Button at the top right.</li> </ul>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#multiqc","title":"Multiqc","text":"<p>multiQC will aggregate the FastQC quality control report of each fastq file in the pipeline. The aggregation of multiple FastQC reports helps users to view the quality control of multiple fastq files easily.</p> <p></p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#count-web-summary","title":"Count Web Summary","text":"<p>The output file in this section is a html file generated by cellranger pipeline.This html file is a quality control report of single cell RNA-Seq Count Matrix, which include estimated number of cells, alignment/quantification results and preliminary clustering analysis. </p> <p>In this report there is also a preliminary clustering analysis of the data. The preliminary analysis result can be used as means of quality control because it shows likely cell populations and their gene markers. However, the results from this report may not be accurate.</p> <p></p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#qc-plots","title":"Qc Plots","text":"<p>The cellranger software in the pipeline will generate a count matrix from input fastq files. The count matrix will be analyzed using R package Seurat. In this html file the number of genes and number of UMIs (Unique Molecular Identifiers) for each cell is plotted using violin plot and scatter plots. </p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#filtering-reports","title":"Filtering Reports","text":"<p>In this html file the filtering criteria will be shown and the number of cells before and after the filtering will be shown.  </p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#embedding-report","title":"Embedding Report","text":"<p>In this html file the dimension reduction results are shown. In the single cell RNA-Seq analysis there are two layers of dimension reduction: Principal component analysis (PCA) and UMAP/tSNE.</p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#prinicple-component-analysis-pca","title":"Prinicple component analysis (PCA)","text":"<p>Prinicple component analysis (PCA) is used to reduce the dimensionality of the data. The rationale of this analysis is that not all the genes in the scRNA-Seq data are contributing to the variations in gene expression profiles of cells due to the sparsity of data. PCA uses all the genes in the data and constructs a series of artificial dimensions (\"Principal components\") which are linear combinations of the gene expression profiles.  For each Principal component the amount of variation explained and how genes contributed to the Principal component are calculated. In the figures below you can see an elbow plot of amount of variation explained: it shows that the amount of variation explained by Principal component decreases \"exponentially\", which means that using the top 15-20 Principal components can capture enough differences in the dataset and we can just use these Principal components in the downstream analysis. The heatmap in the figure below shows what and how genes contributed to different Principal components. Most of these genes in the figure are immune cell type markers, which means that the Principal components are capturing differences between immune cell populations.  </p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#tsne-and-umap","title":"tSNE and UMAP","text":"<p>Although PCA is very helpful and robust in the dimension reduction and generates biologically meaningful results, it is not good enough for the visualization. For visualization the pipeline generates tSNE and UMAP, which is shown in this section.</p> <p></p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#cluster-reports-marker-reports-and-markers","title":"Cluster Reports, Marker Reports and Markers","text":"<p>Cluster reports and Marker Reports are two html files generated by Rmarkdown. The Markers section provided a tsv file of marker genes for each cluster.</p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#description","title":"Description","text":"<p>The unsupervised clustering is used to group cells into populations so users do not need to look at a few thousand cells one at a time. After the clustering, genes that are significantly up and down regulated in each cell population are calculated: these genes can help users to identify the identity of cell populations.</p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#cluster-reports","title":"Cluster Reports","text":"<p>The report will visualize the clustering results, which is shown below. </p> <p>Another important visualization is plotting QC metrics of cells against the clustering results. This is to ensure that the clustering results are not significantly biased by the different sequencing depth and coverage of cells in the dataset. </p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#marker-reports","title":"Marker Reports","text":"<p>In this section the up-regulated genes in each cell population defined by clustering algorithm are visualized using heatmap. The figure below is an example of such a heatmap. </p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#markers","title":"Markers","text":"<p>In this section the full list of gene markers of each cell population is ready to be viewed and downloaded. </p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#singler-rmd-report","title":"Singler Rmd Report","text":"<p>Singler Rmd Report and Marker Reports is a html file generated by Rmarkdown.  SingleR is an algorithm that annotates single cell RNA-Seq dataset based on gene markers of each cell population from clustering and a corresponding reference map. In this section the results of this SingleR annotation are shown. </p>"},{"location":"ViaFoundry/demo_single_cell_rna-seq_tutorial/#annot-out-seurat-h5ad-and-shinyapp","title":"Annot Out, Seurat H5ad and Shinyapp","text":"<p>In these sections the goal is to visualize end results of single cell RNA-Seq dataset using custom shiny Apps and cellxgene.</p>"},{"location":"ViaFoundry/faq/","title":"Frequently Asked Questions","text":""},{"location":"ViaFoundry/faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"ViaFoundry/faq/#installation-guide","title":"Installation Guide","text":""},{"location":"ViaFoundry/faq/#how-can-i-install-singularity","title":"How can I install Singularity?","text":"<p>Follow this link to install Singularity (Version 3) for your pipelines. For your convenience, attached below are the commands needed to download the newest version in a Linux environment:</p> <pre><code>## Remove old version of Singularity\n# sudo rm -rf /usr/local/libexec/singularity /usr/local/var/singularity /usr/local/etc/singularity /usr/local/bin/singularity /usr/local/bin/run-singularity /usr/local/etc/bash_completion.d/singularity\n\n## Install Singularity Version 3\napt-get install -y build-essential libssl-dev uuid-dev libgpgme11-dev libseccomp-dev pkg-config squashfs-tools\nwget https://dl.google.com/go/go1.12.7.linux-amd64.tar.gz\ntar -C /usr/local -xzf go1.12.7.linux-amd64.tar.gz\nexport PATH=$PATH:/usr/local/go/bin\nexport VERSION=3.2.1 \nwget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz\ntar -xzf singularity-${VERSION}.tar.gz\ncd singularity\n./mconfig &amp;&amp; make -C ./builddir \nsudo make -C ./builddir install\n</code></pre>"},{"location":"ViaFoundry/faq/#how-can-i-install-docker","title":"How can I install Docker?","text":"<p>Follow this link to install Docker for pipelines, or follow the commands below:</p> <pre><code>## Uninstall Old Versions of Docker\n# sudo apt-get remove docker docker-engine docker.io\n\n## Install Docker\nsudo apt install docker.io\n</code></pre>"},{"location":"ViaFoundry/faq/#how-can-i-install-java","title":"How can I install JAVA?","text":"<p>Installing Java v8+ for Nextflow:</p> <pre><code>apt-get install -y openjdk-8-jdk &amp;&amp; \napt-get install -y ant &amp;&amp; \napt-get clean;\n\n# Fix certificate issues\napt-get update &amp;&amp; \napt-get install ca-certificates-java &amp;&amp; \napt-get clean &amp;&amp; \nupdate-ca-certificates -f;\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/\n</code></pre>"},{"location":"ViaFoundry/faq/#how-can-i-install-nextflow","title":"How can I install Nextflow?","text":"<p>JAVA (v8+) should be installed before installing Nextflow. Once Java has been successfully installed, click this link to install Nextflow or use the commands below:</p> <pre><code>## To install to your ~/bin directory:\nmkdir ~/bin\ncd ~/bin\ncurl -fsSL get.nextflow.io | bash\n\n# Add Nextflow binary to your bin PATH or any accessible path in your environment:\nchmod 755 nextflow\nmv nextflow ~/bin/\n# OR system-wide installation:\n# sudo mv nextflow /usr/local/bin\n</code></pre>"},{"location":"ViaFoundry/faq/#connection-issues","title":"Connection Issues","text":""},{"location":"ViaFoundry/faq/#why-cant-i-validate-my-ssh-keys","title":"Why can't I validate my SSH Keys?","text":"<p>If you're having trouble validating your SSH keys, here are a few things to check:</p> <ol> <li> <p>Make sure you copy the entire key, including the initial part       (e.g. \"ssh-rsa\"). The key should span the entire file, like in       the following example:</p> <pre><code>  ssh-rsa\n  AA1AB3N4nX3a....................\n  ................................\n  ................................\n  ...............b9Rj @viafoundry\n</code></pre> </li> <li> <p>The SSH protocol requires specific permissions for files and       directories to establish secure connections. Please execute the       following commands to make sure your SSH-related files are       properly secured:</p> <pre><code>  chmod 700 ~/.ssh\n  chmod 600 ~/.ssh/authorized_keys\n</code></pre> </li> <li> <p>Ensure that your home directory is not writable by other       users. Setting the permissions of your home directory to 777 can       create security issues and block SSH connections. Instead, set the       permissions to more secure options such as 750, 755 or 754.</p> </li> </ol>"},{"location":"ViaFoundry/faq/#how-can-i-create-ssh-keys-in-my-computer","title":"How can I create SSH keys in my computer?","text":"<p>You can find your SSH key pairs on your local machine at their default location: <code>~/.ssh/id_rsa</code> for private and <code>~/.ssh/id_rsa.pub</code> for public key. If no keys exist there or you want to create new ones, then on the command line, enter:</p> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>You will be prompted to supply a filename and a password. If you want to accept the default filename (and location) for your key pair, just press Enter without entering a filename. Your SSH keys will be generated using the default filename (<code>id_rsa</code> and <code>id_rsa.pub</code>), and they will be saved in the \"~/.ssh/\" directory in your machine.</p>"},{"location":"ViaFoundry/faq/#run-questions","title":"Run Questions","text":""},{"location":"ViaFoundry/faq/#i-cant-reach-my-files-in-the-file-window","title":"I can't reach my files in the file window","text":"<p>There might be a connection issue. Please check to make sure you've followed these steps:</p> <ol> <li> <p>The SSH protocol requires specific permissions for files and       directories to establish secure connections. Please execute the       following commands to make sure your SSH-related files are       properly secured:</p> <pre><code>  chmod 700 ~/.ssh\n  chmod 600 ~/.ssh/authorized_keys\n</code></pre> </li> <li> <p>Ensure that your home directory is not writable by other       users. Setting the permissions of your home directory to 777 can       create security issues and block SSH connections. Instead, set the       permissions to more secure options such as 750, 755 or 754.</p> </li> </ol>"},{"location":"ViaFoundry/faq/#error-run-directory-cannot-be-created","title":"Error: Run directory cannot be created","text":"<p>It's possible that there's an issue with your connection. Please check the Why can't I validate my SSH Keys section to ensure you've followed all the necessary steps.</p>"},{"location":"ViaFoundry/faq/#profile-questions","title":"Profile Questions","text":""},{"location":"ViaFoundry/faq/#how-should-i-configure-my-executor-settings","title":"How should I configure my executor settings?","text":"<p>In Via Foundry, there are four different sections to control executor settings: the first two are defined in Profile -  Run Environment, and the remaining two are adjusted in the Advanced tab of the run page. If you select an executor other than \"Local\" or \"Ignite\", Via Foundry prompts you to enter additional settings, such as the queue/partition, memory, CPU, and time.</p> <p>1. Executor of Nextflow (navigate to Profile -  Run   Environments):</p> <p>This setting controls how Via Foundry initiates Nextflow. Currently,     Via Foundry supports the Local, SGE, SLURM, and LSF executors to     initiate Nextflow. For the SGE, SLURM, and LSF executors, Via     Foundry only uses them to run Nextflow itself, so the time limit     should be long enough to execute all processes in the pipeline. For     local execution, Via Foundry limits the total amount of memory and     CPU that can be used, so these values should be close to the maximum     capacity of your computer.</p> <ul> <li>Suggested parameters for SGE/SLURM/LSF: long (queue) 8 (GB         Memory) 1 (CPU) 5000-8000 (min, Time)</li> <li>Suggested parameters for Local: 100 (GB Memory) 8 (CPU)</li> </ul> <p>2. Executor of Nextflow Jobs (navigate to Profile -  Run   Environments):</p> <p>This setting will be used as the default setting for submitted jobs     by Nextflow if you don't set any parameters in the Advanced     section of your run page.</p> <ul> <li>Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB         Memory) 1 (CPU) 240 (min, Time)</li> <li>Suggested parameters for Local: 20 (GB Memory) 1 (CPU)</li> </ul> <p>3. Executor Settings for All Processes (in the advanced tab of run   page):</p> <p>These settings will overwrite those in Executor of Nextflow Jobs     and set default parameters for all Nextflow Jobs.</p> <ul> <li>Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB         Memory) 1 (CPU) 240 (min, Time)</li> <li>Suggested parameters for Local: 20 (GB Memory) 1 (CPU)</li> </ul> <p>4. Executor Settings for Each Process (in the advanced tab of run   page):</p> <p>If a particular process requires different parameters than the     defaults (which are defined in the Executor Settings for All     Processes or Executor of Nextflow Jobs sections), you can     overwrite the general settings by clicking the checkbox of the     process that you want to change. This will only affect the settings     of the selected process and keep the original settings for the rest     of the processes.</p> <ul> <li>Suggested parameters for SGE/SLURM/LSF: long (queue) 20 (GB         Memory) 4 (CPU) 1000-5000 (min, Time)</li> <li>Suggested parameters for Local: 20 (GB Memory) 4 (CPU)</li> </ul> <p>Note:    If non-standard resources or settings are required for the executor,   then you can specify these parameters by using Other Options box.   For instance, to submit an SGE job with 3 CPU using parallel   environments, you can enter <code>-pe orte 3</code> (to use MPI for   distributed-memory machines) or <code>-pe smp 3</code> (to use OpenMP for   shared-memory machines) in the Other Options box, leaving the CPU   box empty.</p>"},{"location":"ViaFoundry/faq_developer/","title":"Frequently Asked Questions","text":""},{"location":"ViaFoundry/faq_developer/#frequently-asked-questions-for-developers","title":"Frequently Asked Questions for Developers","text":""},{"location":"ViaFoundry/faq_developer/#process-guide","title":"Process Guide","text":"<ol> <li> <p>Inside a Docker container, avoid running additional Docker commands within the script block, as the current process is already running within Docker. Instead, modify the Docker configuration for the desired process.</p> </li> <li> <p>When writing a bash script in the script section, refrain from using <code>#!/bin/bash</code> to prevent interference with nextflow's error detection mechanism.</p> </li> </ol>"},{"location":"ViaFoundry/faq_developer/#syntax-errors","title":"Syntax Errors","text":"<ol> <li> <p>Dollar characters <code>$</code> should be escaped with a backslash <code>\\</code> in script blocks.</p> <pre><code>#Change:\nhelper($library, $star_index)\n#To:\nhelper(\\$library, \\$star_index)\n</code></pre> </li> <li> <p>Backslash characters <code>\\</code> should be escaped with another backslash <code>\\</code>.</p> </li> </ol>"},{"location":"ViaFoundry/faq_developer/#process-exits-with-code-1-without-output","title":"Process Exits with Code 1 without Output","text":"<pre><code>Caused by:\n  Essential container in task exited\n\nCommand executed:\n\n  my_command 2&gt;/dev/null\n\nCommand exit status:\n  1\n\nCommand output:\n  (empty)\n</code></pre> <p>Using <code>command 2&gt;/dev/null</code> will cause the process to exit without providing the reason for the error. Instead use the following pattern: <code>my_command 2&gt;/dev/null || true</code></p>"},{"location":"ViaFoundry/get_faq/","title":"Frequently Asked Questions","text":""},{"location":"ViaFoundry/get_faq/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"ViaFoundry/get_faq/#via-scientific-capabilities","title":"Via Scientific Capabilities","text":""},{"location":"ViaFoundry/get_faq/#what-is-the-primary-focus-of-via-scientific-and-foundry-platform","title":"What is the primary focus of Via Scientific and Foundry platform?","text":"<p>Via Scientific is primarily a data processing and analytics company that specializes in creating advanced bioinformatics pipelines with seamless metadata integration to feed ML/AI algorithms and other analytical processes in an automatized fashion. The Foundry platform facilitates seamless integration with 3rd party applications enabling automatic data aggregation from various 3rd party sources. By automating the data collection and processing needs, and by further automating the interrogation and exploration of the resultant data through an expanding library of statistical and model / AI driven tool sets we empower scientists and researchers to focus on their core scientific analysis tasks, eliminating tedious manual data handling, dev ops tasks and modeling tasks accelerating their science and while simultaneously eliminating costs and and creating  overall efficiency.</p> <p>While we are an analytics company, and not  a \u201cdata broker\u201d , it is important to note that data is an integral part of our operations. As our Via Foundry platform gathers data from 3rd party platforms for various experiments, we inevitably handle and manage substantial amounts of data. Furthermore we not only process and manage that data, the analytical processes inherent in the various omics workflows will generally expand the data and create additional, richer data prior to it moving into the analytical and reporting steps, Its also important to note that as part of our platform we do manage, and allow our customers to manage various reference data sets (e.g. genome builds etc.)  This data is crucial for driving the analysis processes and generating valuable insights for our customers.  Lastly, customers accumulate data within Foundry  for additional comparative analysis (or they might have us feed a data lake  moving data between Foundry and the data lake environment)</p>"},{"location":"ViaFoundry/get_faq/#via-foundry-ai-in-omics-data-analysis-genomics-research-benefits","title":"Via Foundry: AI in omics data analysis, genomics research benefits?","text":"<p>Via foundry platform specializes in the aggregation, analysis, and sharing of vast amounts of genomic and clinical data. Here's a general idea of the kinds of data we aggregate:</p> <p>Data Sets:</p> <p>1. Omics Data: This could include whole genome sequences, exome sequences, genotyping data, transcriptomic data, metagenomic data, epigenomic data, and more. This data is usually obtained from a variety of sources, including individual research studies, larger initiatives like the 1000 Genomes Project, and direct-to-consumer genetic testing companies.</p> <p>2. Phenotypic and Clinical Data: This includes medical histories, lab results, diagnostic images, and other clinical data that can be linked to genomic data to find correlations between genetic variations and health outcomes. This kind of data often comes from electronic health records, clinical trials, and longitudinal cohort studies.</p> <p>3. Environmental Data: Increasingly, genomic studies are integrating data about individuals' environments and lifestyles to better understand how these factors interact with genetic factors to influence health.</p> <p>4. Other Metadata: This is data about the other datasets \u2014 where it came from, when it was collected, how it was processed, what type of perturbations done etc. This kind of data is crucial for managing, integrating, and interpreting the other types of data.</p> <p>The value of aggregating these data sets in Via Platform in omics is multi-fold:</p> <p>1. Interdisciplinary Research: The aggregation of diverse datasets facilitates cross-disciplinary research and accelerates translational development. For instance, researchers can investigate how genomic data interacts with environmental factors to influence health outcomes.  </p> <p>2. Scale and Statistical Power: Aggregated datasets are larger, which provides more statistical power and reliability to detect smaller effects or rare occurrences. </p> <p>3. Data Reuse: Once data is collected and made available in an aggregated format, other researchers can use it to conduct their own studies without having to collect new data or with having the ability to add new data they can also contextualize the models according to their needs, especially using transfer learning.</p> <p>4. Collaboration: Via Foundry facilitates collaboration and data sharing between researchers and institutions across geographical and organizational boundaries, promoting scientific progress.</p> <p>5. Discovery of Novel Insights: By analyzing different datasets together, researchers can potentially uncover novel insights that would have been impossible to detect in isolated datasets.</p> <p>6. Training: based upon the analytics that customers are doing, the data they feed into Foundry can also become training data (and also operational input data) to customize certain models made available to our customers  inside of Foundry ( this training loop is a new capability being deployed here in 2H-2023.</p> <p>The usage of such data requires stringent data governance policies and processes to protect privacy and ensure ethical use, especially in the case of genomic and clinical data.</p> <p>Via platform not only serves as repositories for data but also provides powerful computational resources and tools for analyzing these data, including machine learning (ML) and artificial intelligence (AI) algorithms. </p> <p>Running ML and AI on the platform allow researchers to identify patterns, make predictions, and even generate hypotheses for further investigation. Here are some examples of how these technologies are applied in this field:</p> <p>1. Genomic Variant Prediction: ML algorithms are trained on large datasets to predict the potential impact of genomic variants. For example, an algorithm might be trained on known pathogenic (disease-causing) and benign variants to classify novel variants as either benign or likely pathogenic. This can aid in diagnosing genetic disorders and understanding the function of different genes and variants.</p> <p>2. Drug Discovery and Personalized Medicine: AI/ML are also being used to identify potential new drugs or to predict how individual patients will respond to existing drugs based on their genetic and clinical data. For instance, machine learning models identify genomic markers that predict a patient's response to a particular chemotherapy agent, helping to customize treatment plans for each individual patient.</p> <p>3. Disease Risk Prediction: AI/ML models are developed to predict an individual's risk of developing certain diseases based on their genetic and clinical data. Successful outcome of this could allow for early intervention or preventive measures in high-risk individuals.</p> <p>4. Phenotype Prediction: AI is used to predict physical traits or disease phenotypes based on genotypic/transcriptomic data. This can help in identifying individuals who may be at risk for certain diseases even before symptoms appear.</p> <p>5. Data Integration and Visualization: AI can also be useful in integrating different types of data (genomic, clinical, environmental, etc.) and visualizing complex relationships between them. This can provide novel insights and generate new hypotheses for research.</p>"},{"location":"ViaFoundry/get_faq/#how-does-via-foundry-handle-noisy-genomics-data","title":"How does Via Foundry handle noisy genomics data?","text":"<p>In areas like genomics and healthcare, noise can indeed be quite high, meaning they may contain errors, inconsistencies, or irrelevant information. Noise can arise from a variety of sources, including measurement errors, inconsistent data collection methods, missing data, and more. Here are some common strategies Foundry uses to deal with noisy data.  These strategies and capabilities are part of Foundry\u2019s core value as Foundry enables the below methods for our customers based on the pipelines they select or create within Foundry and based on their own sensitivities.</p> <p>1. Data Cleaning: This is the process of identifying and correcting errors in the data. It might involve correcting obvious mistakes, dealing with missing or inconsistent data, and standardizing the data format across the dataset. For example, the same patient in vitiligo project entered the system two times because of using an extra name and two unique IDs were given by human error. Once data processing is done, the system detects that this is the same individual from its genetic information. </p> <p>2. Data Normalization and batch effect correction: This involves adjusting the values measured on different scales to a common scale. It reduces the distortion due to different scales and helps to manage the noise. Especially different batches coming from different labs usually have batch effect issues. Our systems have built-in modules to deal with this issue using different methods (i.e Combat, Harman). </p> <p>3. Feature Selection: This process involves identifying the most relevant variables for a particular analysis and discarding the rest. Feature selection can help to reduce noise by eliminating irrelevant or redundant variables using methods like PCA and others.</p> <p>4. Outlier Detection: Outliers are data points that are significantly different from others. They might be due to noise or error, or they might represent genuinely unusual cases. Outlier detection methods can be used to identify these points. Our QC modules have different plots and statistical methods to detect and identify outliers for our customers  by profiling them using existing data. </p> <p>5. Robust Statistical Methods: These are techniques that are less sensitive to outliers and noise than traditional methods. For instance, median is a more robust measure of central tendency than mean because it's less affected by outliers. Since omics data mostly comes from count based experiments (genomic read counts), the data distribution is based on negative binomial distribution that helps us to remove 0 counts and apply robust differential analysis like DESeq2 or Limma to find differences between cell states. </p> <p>6. Machine Learning Techniques: Some machine learning techniques are particularly well-suited to dealing with noisy data. For instance, ensemble methods like random forests or boosting can improve prediction accuracy and robustness in the presence of noise. Within Foundry  these methods are often used  for tissue and sub cell type annotation use cases.</p> <p>7. Imputation Methods: When dealing with missing data, imputation methods can be used to fill in the gaps. These methods estimate the missing values based on the information in the rest of the data. </p> <p>8. Validation and Cross-Validation: These techniques are used to assess the performance of a model on an independent dataset and can help to prevent overfitting to the noise in the training data.</p> <p>9. Regularization: This is a technique used in model fitting that adds a penalty term to the loss function (the function being minimized) that discourages overfitting to the noise in the data.</p> <p>Data preprocessing is a crucial step in data analysis and needs to be handled carefully to avoid introducing biases. However, if done correctly, it can significantly improve the quality and reliability of the results. </p> <p>Via Foundry provides a robust and scalable environment to run complex machine learning and AI algorithms on large genomic and clinical datasets to reduce the noise and complexity of data. However, it's crucial to note that any such predictive models need to be built and evaluated carefully by the users using appropriate metrics and confidence values to ensure their reliability and validity, given the high stakes of medical and genetic data. Foundry is actively simplifying model creations and calculating appropriate metrics to evaluate the models and sharing and recording the underlying code and parameters with the users. The evaluation of the results are made by the users.</p>"},{"location":"ViaFoundry/get_faq/#how-does-foundry-handle-versioning-for-structured-and-unstructured-data","title":"How does Foundry handle versioning for structured and unstructured data?","text":"<p>Our structured and unstructured data supports document differences by automatic versioning.  Foundry follows these general steps:</p> <p>1. Data Schema Design: Platform ensures that the data schema includes fields to store metadata about each document's version, such as a timestamp or and a unique hash for that entry. Metadata of metadata is also kept in the collections in MongoDB as well rather than as static json objects required to be loaded while starting an app.</p> <p>2. Store/Backup Complete Versions: Instead of directly updating documents, Foundry  saves complete copies (snapshots) of the documents. Additionally Foundry does not delete any fields if they are removed, those fields are deprecated to allow supporting continuity and backward compatibility.</p> <p>3. Audit Trail: Foundry creates an \"audit trail\" in separate log files to store the history of changes made to the documents. The changes are tracked with timestamps, user identifiers, and any other relevant metadata.</p> <p>4. Querying the History: When there is a need to retrieve historical data or show the diff indications, Foundry enables the user to query the audit trail logs to fetch the relevant versions of the document and compare them to identify the differences.</p> <p>5. Displaying Diffs: Currently the Foundry UI does not support  displaying differences, however, in our roadmap, the algorithms such as the \"Longest Common Subsequence\" (LCS) are going to be used to efficiently compute the differences between two versions of a document. These differences can then be presented as \"diff indications\" to the users, showing what has been added, deleted, or modified.</p> <p>6. Managing Storage: Foundry manages storage, periodic archival and / or cleanup of older versions. Foundry also has manual two step delete and prune options if the user chooses to remove data.</p>"},{"location":"ViaFoundry/get_faq/#how-does-foundry-enable-customizable-data-ingestion-for-genomics-research-needs","title":"How does Foundry enable customizable data ingestion for genomics research needs?","text":"<p>Data from third-party sources is often software-based and is usually ingested in various formats like excel sheets, images, and PDF files, particularly when it originates from hardware with web API support. Foundry, designed for broad data processing and analytics, can also handle standardized industry formats in specific sectors like genomics. Metadata attached to this data is typically sourced from Electronic Lab Notebook software or Research Electronic Data Software using secure, token-based communication. Foundry features a user-friendly interface, facilitating necessary collections and matching fields from APIs to the fields in the collection.</p> <p>We offer standard normalized metadata tables and ETL pipelines, commonly used by many of our customers. However, we recognize that the unique needs and specific use cases of our customers necessitate customization and configuration. This understanding is where Foundry excels - it's designed with the flexibility and adaptability to meet diverse needs.</p> <p>Foundry allows users to tailor the metadata and pipelines to align with their individual research goals. It empowers customers to modify metadata, adjust input and output parameters, integrate proprietary algorithms, and adapt workflows to suit specific datasets, thereby truly making the system their own.</p> <p>To ensure a smooth and efficient customization process, our team of experts is always available to offer guidance and assistance. Moreover, we believe that customer feedback is invaluable, and we use it to continually enhance Foundry's capabilities. As a result, Foundry is constantly evolving and improving based on the changing needs and methods of the user community. This fosters a collaborative and symbiotic relationship between Foundry and its users, promoting shared growth and progress.</p>"},{"location":"ViaFoundry/get_faq/#how-does-via-foundry-handle-metadata-organization-and-data-normalization","title":"How does Via Foundry handle metadata organization and data normalization?","text":"<p>Foundry\u2019s metadata system ensures compliance with NCBI's GEO submission requirements, which are based on standards developed by the wider genomics research community. These standards enable data sharing and interoperability, and also enhance the reproducibility of scientific research.</p> <p>1. The challenge of metadata organization lies in the complexity and variability of the data. For genomics and other biological research data, there is a need to capture a wide range of metadata including not just the raw data, but also factors such as experimental conditions, processing methods, and other relevant details. This creates a complex matrix of data to manage and organize.</p> <p>The metadata is not only derived from the user entries. While the majority of metadata comes from the sequencing machinery and associated software, other crucial metadata sources include the biological source material, reagents, lab conditions, and even the researcher's notes and observations. In addition, post-processing of data can also generate its own metadata. Therefore, the metadata can come from a variety of sources, both human and machine-generated.</p> <p>The propagation of new metadata across all samples is a dynamic process. Our system updates the metadata repository in real time as new data comes in. This helps ensure that the metadata for all samples remains up-to-date and consistent, even as new data is added.</p> <p>2. Regarding normalization, indeed, there are challenges presented by disparate data sources, especially in terms of variability in format, structure, and semantics. Our system uses a combination of automated processes and manual curation to normalize the data across different sources.</p> <p>Our normalization process typically involves the following steps: </p> <ul> <li>First, the data is mapped to a common data model which provides a standard structure for the data. </li> <li>Then, various the data transformation techniques are used to make the data more consistent, such as converting measurements to a standard unit or mapping variable names to a standard nomenclature.</li> <li>Lastly, quality checks are pefromed to ensure the consistency and reliability of the data.</li> </ul> <p>To further enhance our data integration capabilities, wcommon normalized template metadata collections are used to serve as a starting point for every project. These templates contain the minimum information required to integrate different omic fields, providing a foundation for future expansion if needed. </p> <p>Moreover, to ensure the metadata is both interoperable and accurately interpreted across different systems, ontology support is incorprated into our fields. This means that the terms used in the metadata fields are linked to controlled vocabularies and standards used in the broader scientific community. This also allows users to add additional, specialized metadata fields to the system to meet their specific needs, while ensuring that this new data remains compliant with the established standards.</p>"},{"location":"ViaFoundry/get_faq/#how-does-foundry-enhance-pipeline-development-and-support-interoperability-in-bioinformatics","title":"How does Foundry enhance pipeline development and support interoperability in bioinformatics?","text":"<p>We have implemented specialized training sessions for developers, enabling scientists and bioinformaticians to seamlessly learn the art of pipeline development. By showcasing best-case scenarios and fostering the creation of reusable processes, we have significantly increased the efficiency of pipeline development. This optimization not only saves valuable time but also translates into tangible cost reductions for our clients.</p> <p>A game-changer in our approach has been the removal of dependencies on pipeline managers. By standardizing the process development with clear input, output, and script sections, our platform empowers users to concentrate solely on the scientific analysis required, bypassing the typical challenges associated with pipeline management. As a result, our customers experience a dramatic boost in productivity and can focus on what truly matters \u2013 advancing groundbreaking research and discoveries.</p> <p>Furthermore, Foundry is committed to interoperability. Foundry supports third-party pipelines such as nf-core and other nextflow pipelines.  Accordingly, our customers can also readily incorporate into Foundry historical pipelines they have created, or that they have  harnessed from the open source scientific community while simultaneously tapping into Foundry\u2019s larger capabilities and powers. Lastly, other protocols used in the bioinformatic and computational biology communities, such as snakemake and WDL, are actively being incorporated into Foundry making Foundry a  /the  universal bioinformatic abstraction layer - eliminating pipeline and metadata complexity, accelerating data, analytical and multi-omics fusion and opening up deeper analytical possibilities that accelerates insights and their mission to advance therapeutic development.</p>"},{"location":"ViaFoundry/gsea_explorer/","title":"GSEA Explorer","text":""},{"location":"ViaFoundry/gsea_explorer/#gsea-explorer-tutorial","title":"GSEA Explorer Tutorial","text":"<p>The goal of Gene Set Enrichment Analysis (GSEA) is to determine whether a particular gene set or pathway is enriched in the gene expression data, indicating its potential biological relevance to the studied condition. It operates on the principle that biologically relevant gene sets or pathways are likely to exhibit coordinated changes in expression, rather than individual genes.</p>"},{"location":"ViaFoundry/gsea_explorer/#launching-gsea-explorer","title":"Launching GSEA Explorer","text":"<p>The GSEA Explorer application can be launched either in stand-alone mode or after running Foundry's full RNA-sequencing pipeline or the Differential Expression module:</p>"},{"location":"ViaFoundry/gsea_explorer/#launching-stand-alone-mode","title":"Launching Stand-alone Mode","text":"<ol> <li> <p>In the top banner click <code>Apps</code> and then the <code>Shared Apps</code> tab</p> </li> <li> <p>Search for <code>GSEA Explorer</code> or find the app in the table</p> </li> <li> <p>Click the <code>Launch</code> Button</p> </li> </ol> <p></p>"},{"location":"ViaFoundry/gsea_explorer/#launching-from-report-tab","title":"Launching from Report Tab","text":"<ol> <li> <p>Navigate to the <code>Report</code> tab</p> </li> <li> <p>Select any .Rmd file (It doesn't matter which one)</p> </li> <li> <p>Click the <code>Launch</code> button in the GSEA Explorer card</p> </li> </ol> <p></p>"},{"location":"ViaFoundry/gsea_explorer/#inputs","title":"Inputs","text":"<p>On the right side in the inputs section, select the corresponding species. The DESeq results from the run and the GMT file loaded at runtime will be available in their respective dropdowns. If additional inputs are needed, they can be uploaded to either the Inputs or GMT directory in the GSEA explorer section of the Reports tab by clicking the + sign above the launch script.</p>"},{"location":"ViaFoundry/gsea_explorer/#advanced-options","title":"Advanced Options","text":""},{"location":"ViaFoundry/gsea_explorer/#gene-set-size","title":"Gene Set Size","text":"<p>The minimal and maximal number of genes allowed in a gene set can be set. Gene sets that are not within these bounds will not be run.</p>"},{"location":"ViaFoundry/gsea_explorer/#fold-change-shrinkage","title":"Fold Change Shrinkage","text":"<p>If fold change shrinkage was applied after DEseq was run, the use of the log2FoldChange and shrinked log2FoldChange can be toggled on/off. It is generally recommended to use shrunken log2FoldChanges in GSEA.</p>"},{"location":"ViaFoundry/gsea_explorer/#outputs","title":"Outputs","text":"<p>In the main panel, the results of GSEA will include for each gene set:</p> <ol> <li> <p>Name of gene set</p> </li> <li> <p>Number of genes in the gene set</p> </li> <li> <p>Normalized Enrichment Score</p> </li> <li> <p>Adjusted p-value</p> </li> </ol> <p>Selecting a row will display the corresponding Enrichment plot on the right and leading edge genes on the bottom</p>"},{"location":"ViaFoundry/metadata_guide/","title":"Metadata Guide","text":""},{"location":"ViaFoundry/metadata_guide/#metadata-tracker-guide","title":"Metadata Tracker Guide","text":"<p>Welcome to metadata tracking system designed to support a wide range of use cases, from basic to complex, for multi-organization and lab-based projects. Metadata Tracker seamlessly integrates automated data processing pipelines, data analysis, and visualization modules, making it a powerful tool for managing and organizing your project's metadata. This guide will take you through all you need to know about Metadata Tracker and its integration with Via Foundry, including a comprehensive overview of Metadata Tracker projects, collections, fields, and events.</p> <p>(For information about Metadata Tracker's API, we invite you to reference this document).</p>"},{"location":"ViaFoundry/metadata_guide/#brief-overview","title":"Brief Overview","text":""},{"location":"ViaFoundry/metadata_guide/#benefits-of-metadata-trackers-design","title":"Benefits of Metadata Tracker's Design","text":"<ul> <li>Flexible Design: Define project collection and fields flexibly   with a software architecture developed by the MongoDB data management system. MongoDB supports a flexible data field schema where users can efficiently update the data fields of any entry or the information of the associated metadata.</li> <li>Web APIs: Metadata Tracker also incorporates extensive and secure   token-based Web APIs to make the metadata and processed data   available to its users in accordance with FAIR standards.</li> <li>Event-based Management: Events are specific types of insertion,   deletion and edit operations defined by the project   administrators to allow internal users to manage multiple specifically permitted fields and collections in an organized manner.</li> <li>Validation with Ontology Servers: Data fields are linked to ontology servers or user admin-specified dictionaries, which allows the standardized collection of information from each user.</li> <li>Share: Each project, collection, field, and document in Metadata Tracker   has dynamic permission controls, enabling project admins to update or submit operations to, and limit the access of specified groups or users.</li> </ul>"},{"location":"ViaFoundry/metadata_guide/#who-is-metadata-tracker-for","title":"Who is Metadata Tracker for?","text":"<p>Metadata Tracker is designed for a wide variety of users, from bench biologists to expert bioinformaticians.</p> <ul> <li>Data submission requires no programming knowledge whatsoever. We've created an intuitive event-based process to simplify and streamline this process.</li> <li>Metadata setup only requires basic database knowledge and   familiarity with MongoDB to effectively use its operators. You   don't need to learn all of MongoDB's syntax; instead, you can   easily focus on the field settings. The rest (e.g. creating parent-child relationships with collections,   delivering data from ontology servers, etc.) is handled by Metadata Tracker.</li> </ul>"},{"location":"ViaFoundry/metadata_guide/#data-guide","title":"Data Guide","text":"<p>In this section, we will investigate Metadata Tracker's data tables and how data can be inserted to, updated in, and deleted from collections.</p>"},{"location":"ViaFoundry/metadata_guide/#basics","title":"Basics","text":"<p>At the top of Metadata Tracker's dashboard, you will notice the Collections and Events tabs. You can insert/update/delete data from collections using the All Collections tab, whereas the All Events tab is used for inserting new or updating existing data in a simplified and structured way.</p>"},{"location":"ViaFoundry/metadata_guide/#all-events","title":"All Events","text":"<p>In the <code>Events</code> tab of a given project, all pre-made events are listed in the initial dropdown menu. Please see the example below, which depicts a project containing clinical data on patients and patient visits:</p> <p></p> <p>From this dropdown, you can simply select the action you want to perform. As an example, Update Patient Visit is selected, which loads the pre-defined form fields as below:</p> <p></p> <p>You can either scroll through the dropdown menu to find the data you'd like, or type the name of the data point and select it that way. In this example, you can either scroll to and manually choose the patient in question or type their name before selecting them.</p> <p></p> <p>If you have fields dependent on/subordinate to others, as Patient Visit is to Patient in this example, once the superordinate field has been defined, only the data with the given value for the superordinate field appear in the dropdown menu for the subordinate field. In plainer English, since Patient CL067 is selected here, only Patient CL067's Patient Visits are available in the Patient Visit dropdown.</p> <p></p> <p>As soon as a patient visit is selected, form fields are loaded with selected visit data. After making changes, you can click the save button to submit changes.</p> <p>Once you've made a selection, the blank form fields will be automatically filled with the values associated with that selection. You can make whatever edits you wish, then click <code>Save</code> to store your changes.</p> <p></p> <p>Here, a Patient Visit event has been updated. You'll find that much more is possible with Metadata Tracker events, so for more information on how to create and customize events, check out this section.</p>"},{"location":"ViaFoundry/metadata_guide/#all-collections","title":"All Collections","text":"<p>All collections are listed in the initial dropdown menu of the Metadata Tracker <code>Collections</code> tab. From this dropdown menu, you can simply select the collection you want to view. For example, the Biosamples collection for some arbitrary project is selected:</p> <p></p> <p>At the top of this table, you'll find six action buttons: <code>Insert</code>, <code>Edit</code>, <code>Delete</code>, <code>Edit in Spreadsheet</code>, <code>Download as Excel File</code>, and <code>Import Excel File</code>. For a visual aid, here are all the icons with their corresponding names.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#insert","title":"Insert","text":"<p>You can insert data into a collection by using the <code>Insert</code> button, which will open an <code>Insert &lt;Collection Name&gt;</code> window, in which you can instantiate the data fields associated with your collection. Please check the example below, in which a Biosample is being inserted, with associated values for such dimensions as Clinical Phenotype, Skin, Biosample Type, and Patient Visit ID.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#edit","title":"Edit","text":"<p>After clicking one of the checkboxes, you can start editing your collection member by clicking the <code>Edit</code> button. If you check off multiple items at the same time, Metadata Tracker lets you update only certain fields of the selected data, leaving other fields unique and untouched for all the checked-off items.</p>"},{"location":"ViaFoundry/metadata_guide/#single-data-edit","title":"Single Data Edit","text":"<p>In this image, only one biosample has been selected, so its data can be edited at will.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#multiple-data-edit","title":"Multiple Data Edit","text":"<p>Take a look at this image, in which multiple biosamples have been selected. The Clinical Phenotype has been set to \"Lupus Erythermatosus\" for both of the samples, but all of the other fields are untouched; they will keep their original values.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#delete","title":"Delete","text":"<p>In order to remove data from a collection, you can check off one or multiple items and click the <code>Delete</code> button.</p>"},{"location":"ViaFoundry/metadata_guide/#edit-in-spreadsheet","title":"Edit in Spreadsheet","text":"<p>Another way to edit multiple items is by using spreadsheet view. After clicking the <code>Edit in Spreadsheet</code> button, you'll see your collection data in a spreadsheet format, as opposed to the default table format.</p> <p>Table View:</p> <p></p> <p>Spreadsheet View:</p> <p></p> <p>Now, you can copy and paste into multiple fields as in the Excel file. As an example, the 1st visit value is filled into four cells by copy and paste below. Updated cells are highlighted.</p> <p></p> <p>After clicking the Save Changes button at the top, the highlighted cells are synchronized with Metadata Tracker's database, and the update's status is tracked:</p> <p></p> <p>Table view:</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#download-as-excel-file","title":"Download as Excel File","text":"<p>You can download all collection data by clicking the Download as Excel File button.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#import-excel-file","title":"Import Excel File","text":"<p>After you edit your data in Excel, you can import your changes by clicking the Import Excel File button.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#collection-guide","title":"Collection Guide","text":"<p>This guide explains how to create, edit, and delete collections in Metadata Tracker.</p> <p>Collections are tables used to store and organize data. Metadata Tracker enables you to establish parent-child relationships between collections through referencing techniques. Let's explore the basics of managing collections.</p> <p>First, navigate to a project in Via Foundry. At the top of the <code>Metadata</code> tab interface, click <code>Configure Metadata</code>. You'll find yourself on the <code>All Collections</code> page, where you will find three action buttons to handle collections: <code>Insert</code>, <code>Edit</code>, and <code>Delete</code>.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#inserting-collections","title":"Inserting Collections","text":"<p>In order to insert a new collection, you need to click on the <code>Insert</code> button, which will open the <code>Insert Collection</code> window, as shown below. Here, you can fill in the following fields:</p> <ul> <li>Name (required): Name of your collection. This will not appear in Metadata Tracker's dropdown menus, but is simply for back-end storage purposes.</li> <li>Label (required): Long version of your collection name. This will be displayed as your collection's \"name\" in Metadata Tracker.</li> <li>Project (required): Choose the project you'd like the collection to go into.</li> <li>Version (required): Version of the collection.</li> <li>RestrictTo: Select users or groups who are allowed to   Insert new data into collection. You can grant this permission to new users/groups   by clicking the <code>Insert</code> button. If you later want to remove that   permission, just click options -&gt; delete.</li> <li>Permissions: User or groups are selected who are allowed to   view or edit the collection. You may add new user/groups by   clicking <code>Share</code> button. Afterwards, please choose user/group and   read/write permission and click <code>Save</code>. If you want to   edit/remove the permission, please click options button.</li> <li>Other fields: If you've configured your collection to have additional fields, you'll be able to fill them in in this window as well.</li> </ul> <p></p>"},{"location":"ViaFoundry/metadata_guide/#editing-collections","title":"Editing Collections","text":"<p>If you'd like to edit a collection, simply click the checkbox next to its name, then click the <code>Edit</code> button at the top of the window.</p> <p></p> <p>You'll see an <code>Edit Collection</code> window, wherein you can manipulate any of the previously set fields.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#deleting-collections","title":"Deleting Collections","text":"<p>If you'd like to remove a collection, you can click the checkbox next to its name, then the <code>Delete</code> button.</p> <p></p> <p>You'll be prompted to confirm your intents, at which point you can click <code>Remove</code>. NOTE: Collection deletion is irrevocable, so make sure you absolutely want to delete a collection before doing so.</p>"},{"location":"ViaFoundry/metadata_guide/#fields-guide","title":"Fields Guide","text":"<p>This guide will walk you through the details of Metadata Tracker's fields, which are a fundamental aspect of the Via Foundry platform that allow you to define how different types of data are represented behind the scenes. In other words, they enable you to specify the structure and characteristics of various data classes, making it easier to organize and manage your data effectively.</p> <p>When you work with Via Foundry, you'll often encounter various types of data, such as runs, biosamples, experiments, and more. Each of these data types has specific attributes and properties that need to be captured and stored. This is where Metadata Tracker fields come into play.</p> <p>Let's say you have a run, which represents the execution of a bioinformatics pipeline. For such runs, you can create fields to capture important information, such as the Run Environment, Work Directory, Status, and Run ID. These fields will allow you to keep track of where the run is executed, its status (e.g., running, completed), and unique identifiers to differentiate between different runs.</p> <p>Similarly, if you're dealing with biosamples, you can define fields like Organism, Aliquot, and Clinical Phenotype. These fields will help you record relevant details about each biosample, such as the organism it belongs to, its aliquot information, and any relevant clinical phenotypes associated with it.</p> <p>Basically, Metadata Tracker fields are the building blocks that enable you to customize and organize your data in a structured manner. By defining these fields for different data classes, you can ensure that your data is well-organized and easy to access.</p> <p>In Metadata Tracker, all metadata is stored as key-value pairs in the MongoDB database. When creating a collection, it's essential to specify its fields, which determine the type of data to be stored behind the scenes. Fields also facilitate the establishment of parent-child relationships between collections. Since fields are integral to the creation of events, developing an understanding of fields will be incredibly useful moving forward in your Metadata Tracker and overall Via Foundry experience.</p> <p>Let's explore the basics of working with fields in Metadata Tracker. You'll want to navigate to the <code>Configure Metadata</code> view of the <code>Metadata</code> tab in Via Foundry, then to the tab of the collection whose fields you'd like to work with. At the top of the interface, you will find three action buttons to manage fields: <code>Insert</code>, <code>Edit</code>, and <code>Delete</code>.</p>"},{"location":"ViaFoundry/metadata_guide/#inserting-fields","title":"Inserting Fields","text":"<p>To insert a new field in Metadata Tracker, follow these steps:</p> <ol> <li>Click on the Insert button, which will open a new form to add a field.</li> <li> <p>Enter the following details for the field:</p> </li> <li> <p>Name (required): Name of the field.</p> </li> <li>Label (required): Long version of the field name.</li> <li>Type (required): The main configuration parameter for MongoDB fields, with the following available options:<ul> <li>String: To declare a field as a string.</li> <li>Number: To declare a field as a number.</li> <li>Boolean: To declare a field as a boolean.</li> <li>Array: To declare a field as an array. They implicitly have a default value of an empty array (<code>[]</code>).</li> <li>Date: To declare a field as a date.</li> <li>Mixed: Anything goes into this field.</li> <li>ObjectId: An ObjectId is a special type typically used when child reference is defined.</li> </ul> </li> <li>Collection: Choose the target collection in which to insert the field.</li> <li>Description: Description of the field, shown in the form when inserting new data.</li> <li>Unique: A boolean value to define a unique index on this field.</li> <li>Hidden: A boolean value to determine whether to show this field to users.</li> <li>Required: A boolean (e.g., <code>true</code>) or an array (e.g., <code>[true, \"Patient must have a name.\"]</code>) to specify if the field is mandatory. When an array is used, a warning message is defined at the second index of the array to show a message when the field is not entered.</li> <li>Ontology: This option shows dropdown options from an ontology API or entered Keyword Array. It is configured with the following parameters:<ul> <li>url: URL of the ontology API used to query available options.</li> <li>authorization: Optional authorization header of the API request. Typically, a token could be entered for authorization.</li> <li>filter: Additional query parameters to filter out retrieved data. For example, to only retrieve specific (EFO) ontology data, you can use \"&amp;ontologies=EFO\".</li> <li>field: Describes the location of the keyword in the result of the API. Commonly, APIs return an array of objects. By using dot notation, the location of the keyword (e.g., <code>collection.prefLabel</code>) can be described for the following example:</li> </ul> </li> </ol> <pre><code>JSON\n{\n    \"status\": \"success\",\n    \"data\": {\n        \"data\": [\n            {\n                \"_id\": \"5fdb8820d6330eb80d503a31\",\n                \"collection\": {\n                                \"prefLabel\": \"Lupus\",\n                                \"obsolete\": false\n                               }\n            },\n            {\n                \"_id\": \"5fdb8820d6330eb80d503a31\",\n                \"collection\": {\n                                \"prefLabel\": \"Vitiligo\",\n                                \"obsolete\": false\n                               }\n            }\n        ]\n    }\n}\n</code></pre> <ul> <li>include: Declare possible options for the dropdown.</li> <li>exclude: Exclude certain options from the dropdown.</li> <li>create: Declare if users are allowed to insert new options into this field (other than specified include keywords or API results). If this option is set to true, new options will be saved into the include array.</li> </ul> <p>API example: Experiment Type options can be delivered   from the ontology server (NCBO -- Experimental Factor Ontology) by   using the following configuration:</p> <pre><code>JSON\n{\n    \"url\":\"http://data.bioontology.org/search/?q=\",\n    \"authorization\":\"apikey token=39a74770-b709-4c1c-a69d-45d8e117e87a\",\n    \"include\":[\"ChIP-seq\"],\n    \"field\":\"collection.prefLabel\",\n    \"create\":true,\n    \"filter\":\"&amp;ontologies=EFO&amp;suggest=true\"\n}\n</code></pre> <p>Keyword Array example: Clinical Phenotype options can also   be delivered from the include array:</p> <pre><code>JSON\n{\n    \"create\":true,\n    \"include\":[\"GVHD\",\"Healthy Control\",\"Lichen Planus\",\"Lupus\",\"Vitiligo\"],\n    \"exclude\":[\"Control\"]\n\n}\n</code></pre> <ul> <li> <p>CheckValid: A function or an array that adds a validator function for this property. When an array is used, a warning message is defined at the second index of the array to show a message when the entered value is not valid. You can use validators from third-party libraries (see   examples here) inside these   functions. Some examples of CheckValid include:</p> </li> <li> <p><code>[\"(function(v){ return validator.isEmail(v) })\", \"It is not a valid E-mail\"]</code></p> </li> <li><code>[\"(function(v){ return validator.isAlphanumeric(v) })\", \"Only letters and numbers.\"]</code></li> <li><code>[\"(function(v){ return validator.isURL(v) })\", \"It is not a valid URL\"]</code></li> <li><code>[\"(function(v){ return v.length  2 })\", \"Field must be longer than 2 characters\"]</code></li> <li> <p><code>[\"(function(v){ return !v.search(/^[a-zA-Z0-9-_/]+$/)})\",\"The field must not contain any whitespace or special characters\"]</code></p> </li> <li> <p>Default: Sets a default value for the field. You can specify a default value, which will be used when a new record is created and no value is provided for this field.</p> </li> <li> <p>Ref: <code>${ProjectName}_${CollectionName}</code>. This field is used for child referencing.</p> </li> </ul> <p>Consider the following hierarchy of collections:</p> <p></p> <p>In this example, Patient Visit is the child collection of the Patient collection. Therefore, each Patient Visit datum will have a reference to the superordinate Patient collection. In order to achive this hierarchy, the following <code>patient_id</code> field could be added in the Patient Visit collection:</p> <p></p> <ul> <li> <p>Enum: An array that creates a validator to check if the value is in the given array.</p> </li> <li> <p>Min: A number that creates a validator to check if the value is greater than or equal to the given minimum.</p> </li> <li> <p>Max: A number that creates a validator to check if the value is less than or equal to the given maximum.</p> </li> <li> <p>Lowercase: A boolean to determine whether to save the value with lowercase characters.</p> </li> <li> <p>Uppercase: A boolean to determine whether to save the value with uppercase characters.</p> </li> <li> <p>Trim: A boolean to determine whether to save the value after removing white spaces.</p> </li> <li> <p>Header: A boolean to show this field at the top of the dropdown options. This feature is used when this collection is referenced within other collections.</p> </li> <li> <p>Minlength: A number that creates a validator to check if the value length is not less than the given number.</p> </li> <li> <p>Maxlength: A number that creates a validator to check if the value length is not greater than the given number.</p> </li> <li> <p>Permissions: Specifies the users or groups allowed to view or edit the field. You can add new users/groups by clicking the Share button, choose the user/group, set read/write permissions, and then click Save. To edit/remove permissions, click the Options button.</p> </li> </ul> <p></p>"},{"location":"ViaFoundry/metadata_guide/#editing-fields","title":"Editing Fields","text":"<p>As with many of the other Metadata Tracker features, after clicking a field's checkbox, you can start editing it by clicking the <code>Edit</code> button, and a window similar to this one will open, enabling you to edit whatever fields you wish.</p> <p></p>"},{"location":"ViaFoundry/metadata_guide/#deleting-fields","title":"Deleting Fields","text":"<p>In order to remove a field, you can check off one or multiple items and click the <code>Delete</code> button.</p>"},{"location":"ViaFoundry/metadata_guide/#events-guide","title":"Events Guide","text":"<p>Metadata Tracker Events in Via Foundry provide a powerful framework for performing various actions, such as uploading data or initiating runs, within your projects. Events are like neatly packaged scaffoldings that allow you to interact with your data and collections seamlessly. This guide will walk you through the details of creating events and how to utilize them effectively.</p>"},{"location":"ViaFoundry/metadata_guide/#creating-events","title":"Creating Events","text":"<p>After defining Metadata Tracker fields to specify the structure of your data, you can proceed to create event forms that enable you to insert or update fields across multiple collections. Let's explore the basic steps to create events:</p> <ol> <li>Using the Projects dropdown menu at the top of Foundry, click on the project in which you'd like to create an event.</li> <li>Navigate to the <code>Metadata -&gt; Configure Metadata -&gt; All Events</code> tab specific to your project.</li> <li>Click the <code>Insert</code> button to start creating a new event, and provide a meaningful <code>Event Name</code>.</li> <li>Choose the target collection from the dropdown list and define the behavior of the form. You can choose between <code>Insert</code>, <code>Update</code>, or <code>Multiple</code> (for Multiple Insert) options.</li> <li>Select the specific fields that should be associated with the chosen collection.</li> <li>To add additional fields to the event form, click the \"Plus\" button and choose from the available fields.</li> <li>If you want to include data from another collection, you can use the <code>Insert Group</code> button to add more collections.</li> <li>The permissions for the form can be adjusted using the <code>Permissions</code> section at the bottom.</li> </ol> <p></p>"},{"location":"ViaFoundry/metadata_guide/#creating-new-run-event","title":"Creating \"New Run\" Event","text":"<p>One particularly useful event is the \"New Run\" event, which simplifies the process of submitting runs within Foundry. Here are the key details for creating this event:</p> <ul> <li>Event Name: New Run</li> <li>Collection: Runs</li> </ul> <ul> <li> <p>Fields</p> <ul> <li> <p>Name (required)</p> </li> <li> <p>Server ID (requiredref.)</p> </li> <li> <p>Run Environment</p> </li> <li> <p>Template Run ID</p> </li> <li> <p>Inputs</p> </li> <li> <p>Outputs</p> </li> <li> <p>Work Directory</p> </li> <li> <p>Run URL</p> </li> </ul> </li> </ul> <p>For reference, here's what a newly created <code>New Run</code> event would look like in the Metadata Tracker user interface:</p> <p></p> <p>With Metadata Tracker Events, you can streamline data uploads, perform complex actions, and initiate runs with ease. These events offer a structured and efficient way to interact with your data, empowering you to make the most of Via Foundry. Whether you are uploading new data or orchestrating complex workflows, Metadata Tracker Events provide the flexibility and organization you need to handle your projects effectively.</p>"},{"location":"ViaFoundry/network_explorer/","title":"Network Explorer","text":""},{"location":"ViaFoundry/network_explorer/#network-explorer-tutorial","title":"Network Explorer Tutorial","text":""},{"location":"ViaFoundry/network_explorer/#launching-network-explorer","title":"Launching Network Explorer","text":""},{"location":"ViaFoundry/network_explorer/#launching-in-stand-alone-mode","title":"Launching in Stand-alone Mode","text":"<p>To launch the Network Explorer app in stand-alone mode:</p> <ol> <li>In Foundry's top banner, click <code>Apps</code></li> <li>Open the <code>Shared Apps</code> tab</li> <li>Search for <code>Network Explorer</code> or find the app in the table</li> <li>Click <code>Launch</code></li> </ol> <p></p>"},{"location":"ViaFoundry/network_explorer/#launching-from-report-tab","title":"Launching from Report Tab","text":"<p>The Network Explorer app can also be launched with pre-loaded data following a completed Foundry run. To do so:</p> <ol> <li>After opening the completed run, select the <code>Report</code> tab</li> <li>Select any file with an .Rmd extension (it doesn't matter which one)</li> <li>Click <code>Launch</code> in the Network Explorer box</li> </ol> <p></p>"},{"location":"ViaFoundry/network_explorer/#network-input","title":"Network Input","text":"<p>Network information can be taken from multiple locations:</p>"},{"location":"ViaFoundry/network_explorer/#string","title":"STRING","text":"<p>All annotated protein-protein interactions in the STRING database for a specified protein can be extracted by selecting stringDB as the input source and providing the desired gene or protein of interest.</p> <p>The \"Include Secondary Neighbors\" toggle will add all annotated interactions that are two degrees away from the specified protein.</p> <p></p>"},{"location":"ViaFoundry/network_explorer/#user-defined","title":"User Defined","text":"<p>Users can upload a network of interactions directly with a tab separated input file of edges. The minimal file consists of at least two columns:</p> <ol> <li>source</li> <li>target</li> </ol> <p>where an interaction exists from the source to the target. Addition columns may be present.</p> <p>Note: The uploaded file must contain columns that are exactly \"source\" and \"target\"</p> <p></p>"},{"location":"ViaFoundry/network_explorer/#node-options","title":"Node Options","text":"<p>Additional metadata for nodes can be uploaded in the metadata section which can be used to customize node appearance. If launched from the report tab, the corresponding metadata will be pre-loaded if possible. One can still upload additional metadata by setting the <code>Metadata Source</code> to 'Manual'</p> <p>The minimal metadata file consists of a column called exactly \"gene\". The values in this column correspond to the id of the nodes.</p> <p>The node color and size can be controlled by metadata information and calculated values (node centrality) by selecting a column. Only columns with underlying numeric data will be available.</p> <p></p>"},{"location":"ViaFoundry/network_explorer/#layout","title":"Layout","text":"<p>The network layout can be changed by selecting the desired layout from the drop down in the layout panel.</p> <p></p>"},{"location":"ViaFoundry/network_explorer/#centrality","title":"Centrality","text":"<p>Centrality will be calculated for every node depending. The centrality algorithm can be selected in the drop-down in the centrality panel. </p> <p></p> <p>Available algorithms are:</p> <ul> <li>Betweenness</li> <li>Closeness</li> <li>Degree</li> </ul> <p>Depending on the algorithm, other parameters may be necessary.</p> <p></p>"},{"location":"ViaFoundry/network_explorer/#zoom","title":"Zoom","text":"<p>The network can automatically be zoomed to fit all or the currently selected nodes in the zoom panel.</p> <p></p>"},{"location":"ViaFoundry/network_explorer/#node-selection","title":"Node Selection","text":"<p>Nodes can be selected by clicking on their corresponding circle on the graph. Holding shift or control will allow for the selection of multiple nodes. Alternatively, specific nodes can be selected in the box, and then selected with the \"select\" button.</p> <p>Nodes can be unselected by clicking on a selected node or all nodes can be unselected at once with the 'unselect all button'.</p> <p>The \"get neighboring nodes\" button will select all nodes that neighbor the currently selected node(s)</p> <p></p>"},{"location":"ViaFoundry/network_explorer/#node-metadata","title":"Node Metadata","text":"<p>The node data can be found by selecting the \"Node Properties\" tab above the graph. This will contain internally calculated values as well as any uploaded metadata.</p> <p></p>"},{"location":"ViaFoundry/news/","title":"News","text":""},{"location":"ViaFoundry/news/#foundry-news","title":"Foundry News","text":""},{"location":"ViaFoundry/news/#14102023","title":"14.10.2023","text":""},{"location":"ViaFoundry/news/#foundry-version-164-released","title":"Foundry Version 1.6.4 Released","text":""},{"location":"ViaFoundry/news/#features","title":"Features:","text":"<ul> <li>Google Cloud Batch support has been implemented.</li> <li>An import pipeline feature is now accessible to all users.</li> <li>Foundry pipelines now generate Nextflow DSL2 workflows.</li> <li>Metadata Tracker feature added to projects.</li> <li>The Metadata tab on the run page now allows bulk file insertion and submitting runs with selected files.</li> <li>Introduced a retry feature for SSH queries when submitting a job.</li> </ul>"},{"location":"ViaFoundry/news/#bug-fixes","title":"Bug Fixes:","text":"<ul> <li>Fixed the Download Pipeline button for nf-core/nextflow pipelines.</li> <li>Emails updating on run status now include a link to the Foundry platform.</li> <li>Addressed a bug affecting the initial runs for triple and quadruple samples.</li> <li>Resolved issues with downloading the same report files multiple times in apps.</li> </ul>"},{"location":"ViaFoundry/news/#pipeline-releases","title":"Pipeline Releases:","text":"<ul> <li>TE Transcripts Pipeline is now available.</li> </ul>"},{"location":"ViaFoundry/news/#app-releases","title":"App Releases:","text":"<ul> <li>IGV Browser is now released.</li> <li>TCGA App is now released.</li> </ul>"},{"location":"ViaFoundry/news/#14072023","title":"14.07.2023","text":""},{"location":"ViaFoundry/news/#foundry-version-163-released","title":"Foundry Version 1.6.3 Released","text":""},{"location":"ViaFoundry/news/#features_1","title":"Features:","text":"<ul> <li>Updated the Pipeline files section to improve the overall user experience.</li> <li>Introduced the pipeline revision feature, allowing users to track and manage revisions effectively.</li> </ul>"},{"location":"ViaFoundry/news/#bug-fixes_1","title":"Bug Fixes:","text":"<ul> <li>Fixed a bug related to module name changes on module revision update.</li> <li>Resolved the issue with the run duration column for initial runs, displaying accurate information.</li> <li>Fixed the broken symlinks error during NF-core/Nextflow run creation.</li> </ul>"},{"location":"ViaFoundry/news/#pipeline-releases_1","title":"Pipeline Releases:","text":"<ul> <li>DE Analysis Module is now available.</li> <li>LimmaVoom module is now available.</li> <li>Fastqc module is now available.</li> </ul>"},{"location":"ViaFoundry/news/#app-releases_1","title":"App Releases:","text":"<ul> <li>Network Explorer is now released.</li> <li>PCA Explorer is now released.</li> </ul>"},{"location":"ViaFoundry/news/#18062023","title":"18.06.2023","text":""},{"location":"ViaFoundry/news/#foundry-version-162-released","title":"Foundry Version 1.6.2 Released","text":""},{"location":"ViaFoundry/news/#features_2","title":"Features:","text":"<ul> <li>Implemented security updates for file accesses to enhance system protection.</li> <li>Admin users can now view all groups within the system.</li> <li>Added unique group name validation to prevent duplicate group names.</li> <li>Introduced the process revision feature, allowing users to track and manage revisions effectively.</li> <li>When importing pipelines, users now have the option to either overwrite existing pipelines or create a new revision.</li> <li>Pipeline files can now be viewed in a tree structure for improved organization and navigation.</li> <li>A new Run Report tab has been added to the pipeline page, providing users example reports related to the pipeline.</li> <li>The paramsFile feature has been upgraded, enhancing its functionality and usability.</li> <li>Added support for downloading single files from Amazon S3 storage.</li> <li>Multiple app selection support added. Run page app section upgraded.</li> </ul>"},{"location":"ViaFoundry/news/#bug-fixes_2","title":"Bug Fixes:","text":"<ul> <li>Disabled the spreadsheet mode for S3 files, ensuring compatibility and consistent file handling.</li> <li>Fixed CSV parse error in the spreadsheet viewer for seamless data handling.</li> <li>Users in the same host environment can now perform single file downloads.</li> </ul>"},{"location":"ViaFoundry/news/#pipeline-releases_2","title":"Pipeline Releases:","text":"<ul> <li>Cell Ranger Multi Pipeline is now available.</li> </ul>"},{"location":"ViaFoundry/news/#app-releases_2","title":"App Releases:","text":"<ul> <li>GSEA Explorer is now released.</li> </ul>"},{"location":"ViaFoundry/news/#21052023","title":"21.05.2023","text":""},{"location":"ViaFoundry/news/#foundry-version-161-released","title":"Foundry Version 1.6.1 Released","text":""},{"location":"ViaFoundry/news/#features_3","title":"Features:","text":"<ul> <li>NF-core custom param file upload feature added.</li> <li>New editor added for run, report, and pipeline pages with image support.</li> <li>Import from Bitbucket support added.</li> <li>Custom Nextflow pipeline support initiated.</li> <li>Core pipeline and use paramsfiles features added.</li> </ul>"},{"location":"ViaFoundry/news/#bug-fixes_3","title":"Bug Fixes:","text":"<ul> <li>The upload tab is hidden for AWS batch environments.</li> <li>Loading spinner added for the reports tab.</li> <li>Reports page loading upgraded.</li> <li>Optional channel support added.</li> </ul>"},{"location":"ViaFoundry/news/#pipeline-releases_3","title":"Pipeline Releases:","text":"<ul> <li>scRNA Analysis Module is now available.</li> </ul>"},{"location":"ViaFoundry/news/#app-releases_3","title":"App Releases:","text":"<ul> <li>ISEE App is now released.</li> <li>scViewer is now released.</li> </ul>"},{"location":"ViaFoundry/news/#05052023","title":"05.05.2023","text":""},{"location":"ViaFoundry/news/#foundry-version-160-released","title":"Foundry Version 1.6.0 Released","text":""},{"location":"ViaFoundry/news/#features_4","title":"Features:","text":"<ul> <li>NF-Core pipeline support added.</li> <li>Regex support upgraded for S3 file searches.</li> <li>Directory tree support was added to the Report page.</li> </ul>"},{"location":"ViaFoundry/news/#bug-fixes_4","title":"Bug Fixes:","text":"<ul> <li>Parentheses bug fixed for single file imports.</li> </ul>"},{"location":"ViaFoundry/news/#pipeline-releases_4","title":"Pipeline Releases:","text":"<ul> <li>ATAC-Seq Pipeline is now available.</li> <li>CHIP-Seq Pipeline is now available.</li> </ul>"},{"location":"ViaFoundry/news/#app-releases_4","title":"App Releases:","text":"<ul> <li>Jupyter Notebook Lab is now released.</li> <li>Cellxgene App is now released.</li> <li>DEBrowser App is now released.</li> <li>Rstudio App is now released.</li> </ul>"},{"location":"ViaFoundry/overview/","title":"Platform Overview","text":""},{"location":"ViaFoundry/overview/#short-overview","title":"Short Overview","text":""},{"location":"ViaFoundry/overview/#what-is-via-foundry","title":"What is Via Foundry?","text":"<p>Via Foundry is the web interface of an intuitive and powerful bioinformatics platform designed to simplify pipeline design, development and maintenance, enabling analysis and management of mass quantities of samples on High Performance Computing (HPC) environments, cloud services (AWS, Google Cloud etc.), or personal workstations. It automatically builds Nextflow pipelines by assembling components such as processes and modules, enabling seamless implementation of complex bioinformatics workflows. Moreover, we offer assistance for NF-core or customized Nextflow pipelines, allowing easy integration of these pipelines directly from Github or Bitbucket repositories.</p>"},{"location":"ViaFoundry/overview/#benefits-of-via-foundrys-design","title":"Benefits of Via Foundry's Design","text":"<ul> <li>Build: Via Foundry's drag-and-drop interface allows for     effortless creation of new pipelines, without the need to write     commands from scratch. Existing processes/modules can be reused to     create new pipelines, which can then be exported as Nextflow files     or readily run within Via Foundry.</li> <li>Run: Pipelines can be executed in any host environment with     different executors, such as SGE, LSF, SLURM, Ignite, and more. Via     Foundry also seamlessly integrates with Amazon/Google Cloud,     allowing for easy creation of a cluster in the cloud, execution of     the pipeline, and transfer of results to a cloud storage service     (Amazon Storage, S3 or Google Storage, GS).</li> <li>Resume: Via Foundry's continuous checkpoint mechanism keeps     track of each step of the running pipeline, enabling partially     completed pipelines to be resumed at any stage, even after parameter     changes.</li> <li> <p>Analyze: Via Foundry's report section provides a concise summary of  each executed step, facilitating efficient data analysis. It also allows  for seamless exploration of reported files through interactive applications  like Shiny App, R-Studio (R-Markdown), Jupyter Notebook, or any other containerized app.</p> </li> <li> <p>Improve: Via Foundry's revisioning system keeps track of     pipeline and process versions, as well as their parameters. This     allows for easy editing, improvement, and customization of shared     pipelines according to your needs.</p> </li> <li>Share: Via Foundry allows for easy sharing of pipelines across     different platforms, with the ability to isolate pipeline-specific     dependencies in a container and easily replicate methods in other     environments (clusters, clouds etc.).</li> </ul>"},{"location":"ViaFoundry/overview/#what-is-nextflow","title":"What is Nextflow?","text":"<p>Nextflow is an incredibly potent and versatile framework. Based on the dataflow programming model, it is used for building parallelized, scalable and reproducible workflows using software containers, which provides an abstraction layer between the execution and the logic of the pipeline, which means that the same pipeline code can be executed on multiple platforms.</p>"},{"location":"ViaFoundry/overview/#who-is-via-foundry-for","title":"Who is Via Foundry for?","text":"<p>Via Foundry is designed for a wide variety of users, from bench biologists to expert bioinformaticians.</p> <p>Executing pipelines in Via Foundry requires no programming knowledge. We aim to provide comprehensive explanations to guide users before they execute their pipelines. After a run completes, we provide an overall execution summary of each step, gathering all of the samples in simple tables or interactive apps/plots for ultimate comparison.</p> <p>Building pipelines in Via Foundry requires basic scripting knowledge and familiarization with Nextflow to effectively use its operators. You don't need to learn all of the Nextflow syntax; instead, you can easily focus on the processes where basic input and output nodes are defined. The rest, such as creating and linking the channels, is handled by Via Foundry.</p>"},{"location":"ViaFoundry/overview/#why-not-just-write-a-nextflow-pipeline","title":"Why not just write a Nextflow pipeline?","text":"<p>In many cases, building a static Nextflow pipeline is sufficient for achieving our goals. However, it can be difficult to maintain process and module revisions using simple text editors. With the help of Via Foundry's user interface, you can easily track the evolution of each process and module by accessing their previous revisions. When upgrading existing pipelines, it's much easier to update each process because all of the process-related scripts are isolated in a process circle, and you don't need to deal with other parameters or channel operations that are defined in other parts of the Nextflow script. This modular structure gives us more flexibility and dynamism to create very complex pipelines with little effort.</p> <p>Furthermore, Via Foundry has a built-in form creator that dynamically updates the run page according to the parameters defined in the process. This tool becomes especially powerful when creating complex pipelines with hundreds of optional parameters. As seen in the example below, you can easily isolate process-related parameters in their specific windows.</p> <p></p> <p>Please feel free to reference this image, which outlines the features of Via Foundry and Nextflow, to better understand the ethos of Via Foundry.</p> <p></p>"},{"location":"ViaFoundry/overview/#public-pipelines","title":"Public Pipelines","text":"<p>Attached is a brief list of Via Foundry's current public pipelines, along with some important sub-modules, all of which are ready to execute in your environment.</p> <ul> <li> <p>RNA-Seq Pipelines (RSEM, HISAT, STAR, Salmon, Kallisto, FeatureCounts)</p> </li> <li> <p>ATAC-Seq Pipeline (MACS2)</p> </li> <li> <p>ChIP Seq Pipeline (MACS2)</p> </li> <li> <p>Single Cell Pipelines (10X Genomics, Indrop)</p> </li> <li> <p>piRNA Pipelines (piPipes ChIP-Seq, Degradome/RAGE/CAGE, smallRNA)</p> </li> <li> <p>Sub-Modules:</p> <ul> <li>Trimmer</li> <li>Adapter Removal</li> <li>Quality Filtering</li> <li>Common RNA Filtering</li> <li>ESAT</li> <li>FastQC,</li> <li>MultiQC</li> <li>RSeQC</li> <li>Picard</li> <li>IGV and UCSC genome browser file conversion</li> </ul> </li> </ul>"},{"location":"ViaFoundry/pipeline_example/","title":"Pipeline Examples","text":""},{"location":"ViaFoundry/pipeline_example/#pipeline-examples","title":"Pipeline Examples","text":"<p>There are numerous publicly available pipelines and processes available on the Via Foundry website. Please note that this document only contains a small sampling of the pipelines Via Foundry offers. For the full list of pipelines and to explore each pipeline in detail, please visit the Via Foundry Pipeline page.</p> <p>Once you navigate to the Pipelines tab, simply click on the respective pipeline's box to access a comprehensive overview. This summary provides detailed information regarding the pipeline, such as its usage instructions and example output/report sections.</p> <p></p>"},{"location":"ViaFoundry/pipeline_example/#rna-seq-pipeline","title":"RNA-Seq Pipeline","text":"<p>The RNA-seq pipeline publicly available in Via Foundry includes several key steps for processing RNA-seq data:</p> <ol> <li>Quality Control: FastQC is used to generate quality control (QC)     outputs. Optional processes such as read quality filtering     (trimmomatic), read quality trimming (trimmomatic), and adapter     removal (cutadapt) are available.</li> <li>rRNA Filtering and Genome Alignment: Bowtie2, Bowtie, and STAR are     utilized for counting or filtering out and estimating the abundance     of both standard and predefined sets of genomic loci, such as rRNAs,     miRNAs, tRNAs, piRNAs, snoRNAs, and ERCC.</li> <li>Gene and Isoform Expression Estimation: RSEM is employed to align     RNA-seq reads to reference transcripts and estimate gene and isoform     expression levels.</li> <li>Genome Alignment: HISAT2, STAR, Kallisto or Salmon are employed to align     RNA-seq reads to the genome. Optional estimation of gene and isoform     expression levels can be performed using featureCounts and Salmon.</li> <li>Quality Metrics and Reports: If the user opts to perform genomic     alignments, the pipeline generates overall quality metrics,     including coverage and the number of mapped reads to different     genomic and transcriptomic regions. These reports rely on Picard's     CollectRNASeqMetrics program (Broad Institute, n.d.) and the RSeQC     program (Wang, Wang, and Li 2012).</li> <li>Visualization: Optional generation of Integrative Genomics Viewer     (IGV) and Genome Browser Files (TDF and Bigwig) is available.</li> <li>Quantification Matrix and Analysis: The RNA-seq pipeline provides a     quantification matrix that includes estimated counts and transcript     per million (TPM) values for each gene and annotated isoform. These     matrices serve as input for differential gene expression analysis     and can be directly uploaded to an embedded instance of DEBrowser     software for interactive exploration of the resulting data     (Kucukural et al. 2019).</li> </ol> <p>Presented here is the example report tab for the RNA-Seq Run. Each section within the report consists of its own set of files, allowing you to thoroughly investigate and visualize the data within each respective section.</p> <p></p> <p>Below is a screenshot showcasing the interactive analysis of differential expression analysis using the Shiny app called DEbrowser.</p> <p></p>"},{"location":"ViaFoundry/pipeline_example/#atac-seq-and-chip-seq-pipelines","title":"ATAC-Seq and ChIP-Seq pipelines","text":"<p>Via Foundry offers comprehensive pipelines for the processing of ChIP-Seq and ATAC-Seq data, which are widely used in genomic research. Although these pipelines share many common processes, they exhibit specific differences at certain stages. Moreover, they rely on identical data preparation steps employed in the RNA-Seq pipeline, including read filtering, read quality reporting, and alignment to desired genomic locations.</p> <p>The key steps involved in the ChIP-Seq and ATAC-Seq pipelines are as follows:</p> <ol> <li>Quality Control: The pipelines utilize FastQC to assess the quality     of the sequencing reads and generate quality control outputs.     Additionally, optional processes such as read quality filtering     (trimmomatic), read quality trimming (trimmomatic), and adapter     removal (cutadapt) can be employed to further refine the data.</li> <li>Counting and Filtering: To estimate the abundance of both standard     and predefined sets of genomic loci (e.g., rRNAs, miRNAs, tRNAs,     piRNAs, snoRNAs, ERCC), the pipelines employ tools like     Bowtie2/Bowtie/STAR. These tools facilitate read counting or     filtering to obtain valuable insights into the genomic regions of     interest.</li> <li>Read Alignment: The short-read aligner Bowtie2 is employed to align     the sequencing reads to a reference genome (Langmead and Salzberg     2012). In cases where the input files are large, such as those     obtained from ATAC-Seq experiments, the pipeline optimizes alignment     speed by splitting the files into smaller chunks and performing     parallel alignments.</li> <li>PCR Duplicate Removal: The pipelines incorporate the Picard mark     duplicates function (Broad Institute, n.d.) and Samtools (H. Li et     al. 2009) to estimate and remove PCR duplicates. By employing merged     alignments, the duplicate reads can be efficiently identified and     eliminated, ensuring accurate downstream analysis.</li> <li>ATAC-Seq-specific Analysis: In the case of ATAC-Seq data, the     pipeline performs additional steps. It identifies accessible     chromatin regions by estimating the Tn5 transposase cut sites. This     estimation involves positioning on the 9th base upstream of the 5'     read end and extending by 29 bases downstream. This extension     process is based on studies (Donnard et al. 2018; Buenrostro et     al. 2013) that have shown it to more accurately reflect the exact     positions accessible to the transposase. Subsequently, peaks are     called using MACS2 (Zhang et al. 2008) in both the ChIP-Seq and     ATAC-Seq pipelines.</li> <li>Consensus Peak Calling and Quantification: When processing multiple     samples together, the ATAC-Seq and ChIP-Seq pipelines offer the     option of generating consensus peak calls. This is achieved by     merging all peaks individually called in each sample using Bedtools     (Quinlan and Hall 2010). Furthermore, the pipelines quantify the     number of reads in each peak location using Bedtools' coverage     function, facilitating comprehensive analysis of the data.</li> <li>Data Analysis: As a result, both the ATAC-Seq and ChIP-Seq pipelines     generate a matrix containing count values for each peak region and     sample. This matrix can be directly uploaded to the embedded version     of DEBrowser (Kucukural et al. 2019) for performing differential     analysis. Alternatively, the matrix can be downloaded for further     analysis using other tools or methods.</li> </ol>"},{"location":"ViaFoundry/pipeline_example/#how-to-cite-us","title":"How To Cite Us","text":"<p>If you use Via Foundry (formerly DolphinNext) in your research, please cite:</p> <p>Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x</p>"},{"location":"ViaFoundry/pipeline_example/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"},{"location":"ViaFoundry/pipeline_guide/","title":"Pipeline Guide","text":""},{"location":"ViaFoundry/pipeline_guide/#pipeline-guide","title":"Pipeline Guide","text":"<p>This guide will walk you through how to create, edit, delete and share pipelines on the Via Foundry platform.</p>"},{"location":"ViaFoundry/pipeline_guide/#basics","title":"Basics","text":"<p>After logging into your Via Foundry account, click on the \"Pipeline\" button located at the top left of the screen. This will take you to the pipeline section of Via Foundry, where you'll be able to see all the currently available public pipelines, in addition to those you've created.</p> <p>There are currently two different ways to create an editable pipeline. First, you can duplicate an existing pipeline by navigating to its page and clicking the <code>Duplicate Pipeline</code> button, represented by an icon of two folded pieces of paper, to the right of the pipeline's name.</p> <p></p> <p>This will create a copy of the pipeline, which you can then manipulate as your own.</p> <p>Alternatively, you can create a new pipeline by clicking the <code>New pipeline</code> button, shown as a circle of circles, located in the left sidebar menu of any pipeline (you'll need to navigate to any pipeline's page first to see this menu).</p> <p></p> <p>Regardless of which method you choose to create your pipeline, upon creation, you will see an \"Enter Pipeline Name\" box at the top of the new pipeline page. Enter a name for your pipeline in this box.</p> <p></p> <p>As you type the pipeline name and craft its workflow, you will notice <code>Saving...</code> and <code>All changes saved</code> notifications appear to the right of the pipeline name box and buttons. Via Foundry has an autosaving feature enabled, which automatically saves your pipeline as you work on it. Additionally, you can use the <code>Save Pipeline</code> button at any time to manually save your pipeline.</p> <p>Note: Via Foundry provides additional options for managing your pipelines. If you wish to delete a pipeline, you can do so by clicking the <code>Delete Pipeline</code> button. Furthermore, while creating your pipeline, you can always check the generated Nextflow script by clicking the <code>Download Pipeline</code> button.</p>"},{"location":"ViaFoundry/pipeline_guide/#processes","title":"Processes","text":"<p>You can access Via Foundry's suite of available processes from the \"Processes\" section of the left sidebar menu in any pipeline's page. Processes are grouped according to the categories they fall within; for instance, within the \"Summary\" process menu you'll find all processes underlying how various data are packaged and summarized in a given run. To better visualize, and be able to edit, a process, simply click on the <code>Workflow</code> tab of the pipeline page and drag and drop the desired process into the workspace area. Once you've done this, you'll see the process circle, a figure shown here:</p> <p></p> <p>When working with a process in Via Foundry, you will come across several icons on the process circle. Here's what each icon represents:</p> <ul> <li>Settings Icon: This icon is located at the top of the process       circle. When clicked, it opens the \"Select Process Revision\"       window. In this window, you can examine the current state of the       process, view its details, and navigate between different       revisions of the process. You can also replace the selected       revisions on the workplace with the chosen revision from this       window.</li> <li>Pencil Icon: The pencil icon is located at the center of the       process circle. Clicking on this icon allows you to edit the       process name. The process name is the name that will be used in       the generated Nextflow file as the process identifier. It is       important to ensure that process names are unique within the       overall pipeline. If there are duplicate process names, a warning will be displayed at the bottom of the workflow page to alert you about this issue.</li> <li>Bin Icon: The bin icon is located at the bottom of the process       circle. Clicking on this icon will delete the current process from       the pipeline, so exercise caution when deleting a process.</li> </ul>"},{"location":"ViaFoundry/pipeline_guide/#input-parameters","title":"Input Parameters","text":"<p>To add input files or values that will be used in the process, you can use the \"Input parameters\" circle located above the Pipelines and Processes sections in a pipeline's left sidebar menu. Just like adding processes, you can use the drag and drop feature to add input parameters to your pipeline. Once you drag an input parameter, you will see a smaller orange circle representing the input parameter in the workplace, as shown in the figure below:</p> <p></p> <p>By using the Pencil icon located at the left side of the circle, you can edit the name of the input parameter and define additional settings for it. Here are the options you can configure:</p> <ul> <li>Name: This represents the name of the input parameter, which       will be used to reference the process in the Nextflow file. It's       important to choose unique names for each input parameter within       your pipeline to avoid redundancies, which can yield unwanted       errors.</li> <li>Default value: You can specify a default value for the input       parameter, which will be autofilled in the run page. This is       useful when you have a commonly used value that can serve as a       default for most runs.</li> <li>Dropdown options: If you want the input parameter to be       presented as a dropdown menu on the run page, you can define the       dropdown options. Simply enter the options in a comma-separated       format. For example, if you have a yes/no parameter, you can enter       \"yes, no\" as the dropdown options.</li> <li>Show Settings: By clicking the checkbox for this option, you       can enable the display of settings for the connected process as a       wrench button on the run page. This allows users to access and       modify specific settings for that process. If you want to specify       alternative processes for the wrench button, you can enter their       process names in this field. For example, you can enter       \"map_STAR, map_RSEM\" to show the settings for either the STAR or       RSEM mapping process.</li> </ul> <p></p> <p>Here's an example of what an input row will look like on the run page:</p> <p></p>"},{"location":"ViaFoundry/pipeline_guide/#output-parameters","title":"Output Parameters","text":"<p>Newly created files that are defined in the Outputs section are represented by the Output Parameter circle. You can find the Output Parameters section located just below the Input Parameters section in the left sidebar menu. To add an output parameter to your workspace, simply drag and drop it from the Output Parameters section.</p> <p>The name of the Output Parameters circle corresponds to the name of the published directory for the associated files. You can edit this name by clicking on the pencil icon next to the circle. This allows you to customize the name of the directory where the output files will be transferred.</p> <p>In the example provided, there are two separate green circles representing the aligned and unaligned reads. Each circle corresponds to a specific output parameter, and the associated files will be transferred to separate directories named \"aligned\" and \"unaligned\" respectively.</p> <p>By defining and configuring output parameters, you can manage the generated files and their respective directories within your pipeline.</p> <p></p> <p>Tip: If you don't want to keep certain output files, you can simply leave the output nodes unconnected in your pipeline. By doing so, only the files that are selected as output parameters will be transferred to the published directory. This allows you to control which files are included as outputs. Additionally, If you want to transfer multiple outputs into the same directory, you can use the same name for the output parameters associated with those files. When the pipeline runs, the files from different output nodes with the same output parameter name will be transferred to the same directory.</p>"},{"location":"ViaFoundry/pipeline_guide/#edges","title":"Edges","text":"<p>There are three main reasons for creating edges:</p> <ol> <li>Establishing connections: Edges are used to connect the inputs and       outputs of multiple processes. This allows the data to flow from       one process to another, enabling the sequential execution of the       pipeline.</li> <li>Defining input files or values: By creating edges, you can specify       the input files or values that will be used as inputs for the       processes. This ensures that the required data is provided to each       process for its execution.</li> <li>Selecting published outputs: Edges are also used to select the       outputs that will be published in the output directory. By       connecting specific output nodes to the edge, you indicate that       those files should be included in the final output of the       pipeline.</li> </ol> <p>When creating edges, the pipeline creation tool, such as Via Foundry, typically shows the available nodes that can be connected. This makes it easier to establish the desired connections between processes. The figure below shows an example of this feature in action: since the mouse is hovering over the output node, <code>genome_index</code> of the Build_index process, Via Foundry shows the available node on the Map_tophat2 process by highlighting it.</p> <p>Important: The filtering of available nodes for connection is often based on factors such as file types for set and file parameters, and the name for val parameters. This ensures that the connections are made between compatible input and output nodes.</p> <p>This feature of showing available nodes for connection is also applicable when connecting input parameters to multiple processes, providing further flexibility in the pipeline creation process.</p> <p></p>"},{"location":"ViaFoundry/pipeline_guide/#etl-pipelines-and-setting-pipeline-defaults","title":"ETL pipelines and Setting Pipeline Defaults","text":"<p>An ETL (Extract, Transform, Load) pipeline refers to a systematic process of extracting data from various sources, transforming it into a suitable format, and loading it into a target database for further analysis and interpretation. To simplify this process, Via Foundry intruduces using <code>paramsFiles</code> feature.</p> <p>In order to restrict user modifications, you can utilize <code>paramsFiles</code> to define specific defaults for your pipeline. These paramsFiles are JSON files that outline the default values of the pipeline. By default, these values are disabled for users to modify. This approach provides you with a clear understanding of the default values used in the pipeline when specified parameters are employed.</p> <p>To enable this feature, First, click on the \"Advanced\" tab and enable the \"Use paramsFiles\" option. The next step is to create two folders in the pipeline repository: first, create a folder named \"via,\" and then within the \"via\" folder, create another folder named \"paramsFiles.\"  You can use <code>advanced tab</code> -&gt; <code>pipeline files</code> section to create these folders. Each file added to  <code>via/paramsFiles</code> folder will be an option to user to select. Lets check example below:</p> <ul> <li> <p>MolecularBiology option (Simple Settings): </p> </li> <li> <p>Microbiology option (Advanced Settings): </p> <pre><code>{\n    \"run_RSEM\" : \"yes\",\n    \"run_Adapter_Removal\":  \"yes\",\n    \"Adapter_Trimmer_Quality_Module_Adapter_Removal\": {\"min_length\" : 11, \"Adapter_Sequence\":\"AGATCGGAAGAGC\"},\n    \"via_groups\" : \"microbiology\",\n    \"via_params_ask_only\" : \"reads, mate, genome_build, MultiQC.multiqc_parameters\"\n    \"via_params_show\" : \"run_Adapter_Removal\"\n}\n</code></pre> </li> </ul> <p>In the <code>via/paramsFiles</code> folder, two JSON files named <code>MolecularBiology.json</code> and <code>Microbiology.json</code> have been added. Let's focus on the <code>MolecularBiology.json</code> file. In this file, specific values have been set for <code>run_RSEM</code> and <code>run_Adapter_Removal</code>. Instead of including all the parameters in this JSON file, you can use the <code>via_params_ask_only</code> keyword to disable all other parameters. In this example, the only parameters that can be edited are <code>reads</code>, <code>mate</code>, and <code>genome_build</code>. This allows users to focus on modifying these particular parameters while leaving the rest unchanged.</p> <p>In the <code>Microbiology.json</code> file, the process variables <code>min_length</code> and <code>Adapter_Sequence</code> for the <code>Adapter_Trimmer_Quality_Module_Adapter_Removal</code> step have been set. These variables define specific values for the minimum length and adapter sequence used in the adapter trimming process. In addition, the <code>via_groups</code> parameter is used to restrict the visibility of this parameter set on the user's run page. This means that only certain users or user groups will be able to view and modify these parameters during the pipeline execution, while others will not have access to them.</p> <ul> <li> <p>paramsFiles Configuration:</p> <ul> <li> <p>via_groups: This parameter is used to restrict the visibility of a parameter set on the user's run page. It allows you to specify certain user groups who will be able to see that particular parameter set, while others will not have access to it.</p> </li> <li> <p>via_params_ask_only: When using this parameter, you can specify a comma-separated list of values. Only the parameters included in this list will be editable by the user on the run page. The rest of the parameters will be disabled and hidden (under <code>system inputs</code> section) from the user, preventing any modifications.</p> </li> <li> <p>via_params_show: By default, only the parameters defined in via_params_ask_only will be displayed to the user on the run page. All other parameters will be hidden under the \"system inputs\" section. However, if you want to change this behavior, you can provide a comma-separated string with via_params_show. This will move the specified parameters from the \"system inputs\" section to the user interface, allowing them to be visible by the user.</p> </li> </ul> </li> </ul> <p>Here are the example run pages of the RNA-Seq pipeline:</p> <ul> <li>MolecularBiology option is selected:</li> </ul> <p></p> <ul> <li>Microbiology option is selected:</li> </ul> <p></p>"},{"location":"ViaFoundry/pipeline_guide/#exporting-pipeline-parameters","title":"Exporting Pipeline Parameters","text":"<p>To facilitate the exporting of used parameters in a pipeline, we have implemented a process called <code>publish_params_as_JSON</code>. This process can be found by using the search bar on the pipeline page. To enable this capability in a specific pipeline, you can simply drag and drop the <code>publish_params_as_JSON</code> process into the desired location.</p> <p>Once you have added the process to the pipeline, please drag and drop the <code>output parameter</code> to the workflow and rename it as <code>logs</code>. The following example workflow demonstrates this process:</p> <p></p> <p>After the run completes, a file named <code>run_params.json</code> will be generated under the \"logs\" folder, containing the exported parameters.</p> <pre><code>[\n    {\n        \"Adapter_Removal\": {\n            \"min_length\" : 11, \n            \"Adapter_Sequence\":\"AGATCGGAAGAGC\"\n        },\n        \"bed\": \"s3://viafoundry/genome_data/mouse/mm10/refseq/genes/genes.bed\", \n        \"hisat2Index\": \"s3://viafoundry/genome_data/mouse/mm10/refseq/Hisat2Index\", \n        \"outdir\": \"s3://viafoundry/runs/report159\"\n    }\n]\n</code></pre>"},{"location":"ViaFoundry/pipeline_guide/#pipeline-header-script","title":"Pipeline Header Script","text":"<p>This section, located in the <code>Advanced</code> tab of a pipeline's page, provides the capability to add additional inputs, scripts, or comments before the pipeline starts. This section is useful for defining common functions or variables that need to be used in multiple processes within the pipeline.</p> <p>By adding inputs, scripts, or comments in this section, you can ensure that they are available and accessible throughout the pipeline's execution. This allows you to define reusable code or parameters that can be referenced multiple times in different processes</p> <p></p>"},{"location":"ViaFoundry/pipeline_guide/#adding-pipeline-inputs-by-pipeline-header-script","title":"Adding Pipeline Inputs by Pipeline Header Script","text":"<p>You can use the pipeline header script field in the <code>Advanced</code> tab of a pipeline's page to add pipeline inputs, similar to adding dropdown options for process parameters. To create inputs in the pipeline header script, you need to use the following syntax:</p> <pre><code>params.variableName = defaultValue //* @dropdown @options:\"...\"\n</code></pre> <p>It's important to note that you need to use <code>params.</code> at the beginning of <code>variableName</code> to define the input parameter. The <code>defaultValue</code> can be specified with single or double quotes (for strings) or without any quotes (for numbers).</p> <p>On the run page, these input parameters will be displayed separately in the Inputs section. Users will be prompted to provide values for these parameters based on the defined options. Here's an example:</p> <pre><code>params.genome_build = \"\" //* @dropdown @options:\"human_hg19, mouse_mm10, custom\"\nparams.run_Tophat = \"no\" //* @dropdown @options:\"yes\",\"no\"\n</code></pre> <p></p>"},{"location":"ViaFoundry/pipeline_guide/#autofill-feature-for-pipeline-inputs","title":"Autofill Feature for Pipeline Inputs","text":"<p>A. Based on Hostname:</p> <p>In Via Foundry, you can use the autofill feature to specify input parameters based on the hostname of the execution environment. The syntax for autofilling input parameters based on hostname is as follows:</p> <pre><code>//* autofill\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n&lt;input parameters&gt;\n}\n//* autofill\n</code></pre> <p>In this example, <code>$HOSTNAME</code> is a Via Foundry-specific variable that represents the hostname of the execution environment. You can replace \"hostname\" with the actual hostname on which you want the autofilled inputs to apply. Within the if statement, you can define the <code>&lt;input parameters&gt;</code> that should be automatically filled when the pipeline is executed on the specified hostname. In this example, <code>&lt;input parameters&gt;</code> will be filled if the pipeline runs on ghpcc06.umassrc.org.</p> <p>Here's an example that demonstrates how to autofill the TrimPath parameter for a specific hostname:</p> <pre><code>//* autofill\nif ($HOSTNAME == \"garberwiki.umassmed.edu\"){\n    params.TrimPath =\"/project/Trimmomatic/trimmomatic-0.32.jar\"\n    } \n//* autofill\n</code></pre> <p></p> <p>B. Based on Selected Input:</p> <p>In Via Foundry, you can also use the autofill feature based on the selected parameters on the run page. This allows you to dynamically change the value of an input parameter based on the selection made by the user.</p> <p>Here's an example that demonstrates how to autofill the path of a program based on the selected pipeline parameter <code>method</code>:</p> <pre><code>params.method = \"\" //* @dropdown @options:\"trimmomatic, fastqx\"\n//* autofill\nif (params.method == \"trimmomatic\"){\n    params.TrimPath =\"/project/Trimmomatic/trimmomatic-0.32.jar\"\n}\nelse if (params.method == \"fastqx\"){\n    params.TrimPath =\"/project/fastqx/fastqx\"\n}\n//* autofill\n</code></pre> <p></p> <p>C. Dynamic Autofill:</p> <p>To autofill parameters that follow a specific pattern based on conditions, you can use the dynamic autofill feature in Via Foundry. This allows you to define variable parts of the pattern - using underscores; for example, <code>_species</code> or <code>_build</code> - and assign values to them based on the fulfillment of if conditions. You can then activate the autofill feature by checking the existence of your host. Here's an example of how to use dynamic autofill with proper syntax:</p> <pre><code>if (params.variableName &amp;&amp; $HOSTNAME){\n    &lt;input parameters&gt;\n}\n\nor\n\nif ($HOSTNAME){\n    &lt;input parameters&gt;\n}\n</code></pre> <p>For further clarification, reference the example below where the <code>params.genome</code> and <code>params.genomeIndexPath</code> parameters are filled according to selected parameters of <code>params.genome_build</code> and <code>$HOSTNAME</code>:</p> <pre><code>params.genome_build = \"\" //* @dropdown @options:\"human_hg19, mouse_mm10\"\ndef _species;\ndef _build;\ndef _share;\n//* autofill\nif (params.genome_build == \"human_hg19\"){\n    _species = \"human\"\n    _build = \"hg19\"\n} else if (params.genome_build == \"mouse_mm10\"){\n    _species = \"mouse\"\n    _build = \"mm10\"\n}\n\nif ($HOSTNAME == \"garberwiki.umassmed.edu\"){\n    _share = \"/share/dolphin_data/genome_data\"\n} else if ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n    _share = \"/project/data/genome_data\"\n}\nif (params.genome_build &amp;&amp; $HOSTNAME){\n    params.genome =\"${_share}/${_species}/${_build}/${_build}.fa\"\n    params.genomeIndexPath =\"${_share}/${_species}/${_build}/${_build}\"\n}\nif ($HOSTNAME){\n    params.TrimPath =\"${_share}/Trimmomatic/trimmomatic-0.32.jar\"\n}\n//* autofill\n</code></pre> <p></p>"},{"location":"ViaFoundry/pipeline_guide/#autofill-feature-for-pipeline-properties","title":"Autofill Feature for Pipeline Properties","text":"<p>To automate the filling of executor properties in Via Foundry, you can utilize the autofill feature. There are two types of autofill options available: hostname-independent autofill and hostname-dependent autofill.</p> <p>Hostname-Independent Autofill:</p> <p>To define executor properties that will be automatically filled regardless of the hostname, you can use the following syntax:</p> <pre><code>//* autofill\n&lt;executor properties&gt;\n//* autofill\n</code></pre> <p>Hostname-Dependent Autofill:</p> <p>If you need to overwrite the default executor properties based on specific hostnames, you can use hostname-dependent executor properties. Here's the syntax:</p> <pre><code>//* autofill\n&lt;executor properties&gt;\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n&lt;hostname dependent executor properties&gt;\n}\n//* autofill\n</code></pre> <p>In this example, the <code>&lt;executor properties&gt;</code> section will be filled for all hostnames. However, if the hostname is \"ghpcc06.umassrc.org\", the <code>&lt;hostname dependent executor properties&gt;</code> section will be additionally filled.</p> <p>The <code>$HOSTNAME</code> variable in Via Foundry represents the selected hostname in the run environment. By using this variable, you can apply specific executor properties based on the hostname.</p> <p>Executor Properties:</p> <p>There are five types of executor properties available to autofill Executor Settings for All Processes: <code>$TIME</code>, <code>$CPU</code>, <code>$MEMORY</code>, <code>$QUEUE</code>, <code>$EXEC_OPTIONS</code> which respectively define the Time, CPU, Memory, Queue/Partition and Other Options fields in the executor settings. You can set pipeline defaults by using the <code>$HOSTNAME == \"default\"</code> condition. Reference the example below:</p> <pre><code>//* autofill\nif ($HOSTNAME == \"default\"){\n    $MEMORY = 32\n    $CPU  = 1\n}\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n    $TIME = 3000\n    $CPU  = 4\n    $MEMORY = 100\n    $QUEUE = \"long\"\n    $EXEC_OPTIONS = '-E \"file /home/garberlab\"'\n}\n//* autofill\n</code></pre> <p></p> <p>Singularity/Docker Images: There are four types of image properties able to be autofilled: <code>$DOCKER_IMAGE</code>, <code>$DOCKER_OPTIONS</code>, <code>$SINGULARITY_IMAGE</code>, <code>$SINGULARITY_OPTIONS</code>, which automatically fill the <code>image path</code> and <code>RunOptions</code> fields of Docker and Singularity. See the example below for the appropriate syntax of autofilling Docker image properties:</p> <pre><code>//* autofill\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n    $DOCKER_IMAGE = \"docker://UMMS-Biocore/docker\"\n    $DOCKER_OPTIONS = \"-v /export:/export\"\n}\n//* autofill\n</code></pre> <p></p> <p>Likewise, here's the syntax needed when autofilling Singularity image properties:</p> <pre><code>//* autofill\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n    $SINGULARITY_IMAGE = \"shub://UMMS-Biocore/singularity\"\n    $SINGULARITY_OPTIONS = \"--bind /project\"\n}\n//* autofill\n</code></pre> <p></p> <p>When you want to define both Singularity and Docker images, you can set the <code>$DEFAULT_IMAGE</code> tag to either <code>\"docker\"</code> or <code>\"singularity\"</code>, depending on your desired use. In this example, a Docker image is set by default, but if the user's hostname is \"ghpcc06@umassrc.org,\" a Singularity image will be set:</p> <pre><code>//* autofill\nif ($HOSTNAME == \"default\"){\n    $DEFAULT_IMAGE = \"docker\"\n    $DOCKER_IMAGE = \"dolphinnext/rnaseq:1.0\"\n    $SINGULARITY_IMAGE = \"https://galaxyweb.umassmed.edu/pub/dnext_data/singularity/UMMS-Biocore-rna-seq-1.0.img\"\n}\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n    $DEFAULT_IMAGE = \"singularity\"\n    $DOCKER_IMAGE = \"dolphinnext/rnaseq:1.0\"\n    $SINGULARITY_IMAGE = \"https://galaxyweb.umassmed.edu/pub/dnext_data/singularity/UMMS-Biocore-rna-seq-1.0.img\"\n}\n//* autofill\n</code></pre> <p>Please note that Google Cloud accounts will overwrite the <code>$DEFAULT_IMAGE</code> tag, since Google Cloud accounts only support the use of Docker images.</p> <p>Platform Tag:</p> <p>To isolate platform-dependent parameters in Via Foundry, you can use the platform tag. This allows you to exclude platform-specific parameters from the exported pipeline, and when the pipeline is imported, existing platform-dependent parameters will not be overwritten. Here's an example of how to use the platform tag:</p> <pre><code>//* autofill\nif ($HOSTNAME == \"default\"){\n    $MEMORY = 32\n    $CPU  = 1\n}\n//* platform\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n    $TIME = 3000\n    $CPU  = 4\n    $MEMORY = 100\n    $QUEUE = \"long\"\n    $EXEC_OPTIONS = '-E \"file /home/garberlab\"'\n}\n//* platform\n//* autofill\n</code></pre> <p>In this example, since the run environment is set as \"ghpcc06.umassrc.org\", the autofill feature overwrote the default <code>$TIME</code> value (1000) with a value of 3000.</p>"},{"location":"ViaFoundry/pipeline_guide/#pipeline-details","title":"Pipeline Details","text":"<p>This section summarizes all processes used by the pipeline to give you an overarching perspective on the pipeline's architecture.</p> <p></p>"},{"location":"ViaFoundry/pipeline_guide/#permissions-and-groups","title":"Permissions and Groups","text":"<p>By default, all new pipelines are only seen by their owner. You have the option to share your pipeline with a specific group that you have created in the profile's \"Groups\" tab. To do this, choose \"Only my group\" and select the name of the desired group. Members of that group will then be able to view the run on their run status page.</p> <p>Alternatively, you can set the permissions to \"Everyone\". With this setting, the pipeline will only be visible to users who know the specific run link. The selected pipeline will not appear on their pipelines page, but they will be able to access it if they have the link.</p> <p>When collaborating with multiple individuals on a shared pipeline, you have the option to grant write permission to a group by utilizing the \"Group Permission to Write\" dropdown. This allows you to specify a group and authorize them to make changes and modifications to the pipeline.</p> <p></p>"},{"location":"ViaFoundry/pipeline_guide/#copying-and-revisions","title":"Copying and Revisions","text":"<p>To make changes, new trials, improvements on your or other people's pipelines, you can create a duplicate of a pipeline in Via Foundry by clicking the <code>Duplicate Pipeline</code> button within a pipeline's page. When your pipeline has been set to public or used/edited by other group members, you're not allowed to make changes that alter that same revision of the pipeline. As such, duplicating a pipeline allows you to make changes and improvements without affecting the original pipeline or other users' work. It's a convenient way to experiment and iterate on pipelines while keeping the original version intact.</p>"},{"location":"ViaFoundry/pipeline_guide/#how-to-import-pipelines","title":"How to Import Pipelines","text":"<p>To import nf-core pipelines or similar standard Nextflow pipelines, follow these steps:</p> <ol> <li> <p>Visit the \"Pipelines\" tab and select any pipeline. Click the \"Import Pipeline\" button.</p> <p></p> </li> <li> <p>Click the \"Remote Repository\" button and choose the source of your repository: Repository Type (Public or Private).</p> <p></p> <ol> <li> <p>Public Repository:</p> <ol> <li>If your pipeline is publicly accessible, choose \"Public.\"</li> <li>Enter the Repository URL and provide the tag/branch if available.</li> <li>Click the \"Pull Pipeline\" button.</li> </ol> </li> <li> <p>Private Repository:</p> <ol> <li>If your pipeline is private, choose \"Private.\"</li> <li>Choose the repository credentials that you have set up in the Profile -&gt; Repositories section. For more information, refer to the repository connections page.</li> <li>Enter the Repo URL and provide the tag/branch if available.</li> <li>Click the \"Pull Pipeline\" button.</li> </ol> </li> </ol> </li> <li> <p>Importable Pipeline Standards:</p> <ol> <li> <p>Pipeline Inputs Schema (nextflow_schema.json):</p> <p>To define the inputs for the pipeline, refer to the example inputs provided in the following link: Example Inputs.</p> <p>If you do not have the <code>nextflow_schema.json</code> file, you can create one using the Pipeline Schema Builder Tool. Ensure that all inputs are organized within a group section. To create a group, click the \"Add Group\" button.</p> <pre><code>{\n    \"$schema\": \"http://json-schema.org/draft-07/schema\",\n    \"$id\": \"https://raw.githubusercontent.com/YOUR_PIPELINE/master/nextflow_schema.json\",\n    \"title\": \"Nextflow pipeline parameters\",\n    \"description\": \"This pipeline uses Nextflow and processes some kind of data. The JSON Schema was built using the nf-core pipeline schema builder.\",\n    \"type\": \"object\",\n    \"definitions\": {\n        \"new_group_1\": {\n            \"title\": \"New Group 1\",\n            \"type\": \"object\",\n            \"description\": \"\",\n            \"default\": \"\",\n            \"properties\": {\n                \"new_param_1\": {\n                    \"type\": \"string\"\n                },\n                \"new_param_2\": {\n                    \"type\": \"string\"\n                }\n            }\n        }\n    },\n    \"allOf\": [\n        {\n            \"$ref\": \"#/definitions/new_group_1\"\n        }\n    ]\n} \n</code></pre> </li> <li> <p>Pipeline publish direcory should be set as <code>params.outdir</code>.</p> </li> </ol> </li> <li> <p>To import Via Pipelines, you need admin permission. Contact support@viascientific.com for more details.</p> </li> </ol>"},{"location":"ViaFoundry/pipeline_guide/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"},{"location":"ViaFoundry/process/","title":"Process Guide","text":""},{"location":"ViaFoundry/process/#process-guide","title":"Process Guide","text":"<p>This guide will walk you through the creation of Via Foundry processes.</p>"},{"location":"ViaFoundry/process/#process-window","title":"Process Window","text":"<p>Once you've logged into your Via Foundry account, enter the page of a pipeline you'd like to add a new process to by clicking on the <code>Pipeline</code> tab in the top left of the screen, then on the name of the desired pipeline. You'll notice several buttons on the left sidebar's menu. For our purposes now, the most important one is the button containing the image of an unbroken circle, second from the left. This is the <code>New Process</code> button, which enables you to create new processes.</p> <p></p>"},{"location":"ViaFoundry/process/#basics","title":"Basics","text":"<p>Once you click the <code>New Process</code> button, a new window will appear for you to define the components of your new process.</p> <ul> <li>Name: Enter the name of your process in this field. After     creating the process, its name will appear in the pipeline's left     sidebar menu under the selected menu group.</li> <li>Description: Provide a brief explanation of how the process     works in this field.</li> <li>Menu group: Select a menu group so that the process can be     properly categorized within the left sidebar menu. If necessary, you     can add new menu groups by clicking the <code>Add Menu Group</code> button. You     can also edit or delete these new groups by using the     <code>Edit Menu Group</code> or <code>Delete Menu Group</code> buttons.</li> </ul>"},{"location":"ViaFoundry/process/#parameters","title":"Parameters","text":"<p>This section is for you to create parameters for your process, which will be used while defining inputs and outputs. You can create new parameters via the <code>Add Parameter</code> button.</p> <p></p> <ul> <li> <p>Identifier: The identifier is simply the parameter name, and     once created, you'll be allowed to call the same parameters in     other processes.</p> </li> <li> <p>Qualifier: Five main types of qualifiers (<code>file</code>, <code>set/tuple</code>, <code>each</code>, <code>env</code> and <code>val</code>) exist in Via Foundry:</p> <p>File: This qualifier is used when the following     qualifier represents the name of a file. Example usage in a     Nextflow file:</p> <pre><code>    file genome\n</code></pre> <p>Set/Tuple: This qualifier enables you to handle a group of     input values with other qualifiers. Example usage:</p> <pre><code>    set val(name), file(genome)\n</code></pre> <p>Val: With this qualifier, you can access the received     input value by its name in the process script. Example     usage:</p> <pre><code>    val script_path\n</code></pre> <p>Each: This qualifier enables the execution of the     process for each entry in the input collection. Example     usage:</p> <pre><code>    each bed_list\n</code></pre> </li> <li> <p>File type: If the qualifier is set to <code>file</code> or <code>set</code>, a \"File     Type\" option will appear. This option is used to filter available     nodes when generating pipelines.</p> </li> </ul> <p>Tip: For example, you may create <code>genome</code> parameter by entering the identifier as \"genome\", qualifier as \"file,\" and file type as \"fasta\". Similarly, to create the \"script_path\" parameter you can define the identifier as \"script_path\" and the qualifier as \"val\".</p> <p>Note: When the qualifier is set to <code>val</code>, the identifier is used to filter available nodes when connecting each node. If <code>file</code> or <code>set</code> is selected as the qualifier, the file type is used to filter available nodes. When the qualifier is set to <code>each</code>, you can enter both the file type (if connecting with file nodes) and the identifier (if connecting to val nodes).</p>"},{"location":"ViaFoundry/process/#inputs","title":"Inputs","text":"<p>This section is where you enter all your process's input nodes. Clicking the <code>Add input...</code> dropdown will show a list of all existing inputs, from which you can select those you wish to use. After adding a selected parameter as a input node, you'll see an <code>Input Name</code> box, in addition to <code>Add/Remove Operator</code> (wrench icon) and <code>Optional</code> buttons.</p> <p></p> <p>The <code>Input Name</code> box is used to define Nextflow variables that will be used in the Scripts section. For example, if you enter the input name as \"genome,\" you can call this variable as <code>${genome}</code> in the Script field. Here are some other examples:</p> Qualifier Input name How to Reference in the Script val script_path ${script_path} each bed_list ${bed_list} file genome ${genome} set val(name), file(genome) ${genome} set val(name), file(genome) ${name} <p>Additionally, if you need to transform values emitted by a channel, you can click the <code>Add/Remove Operator</code> button, then select operators from the <code>Operators</code> dropdown menu that appears. For more detailed information, refer to the Operators section.</p> <ul> <li>Optional Inputs:</li> </ul> <p>If you want to use an optional input parameter, you can check the <code>Optional</code> checkbox. This feature provides flexibility to the user while defining their process, as the process will still be executed even if the input parameter is not provided. Here's an example use case:</p> <p></p> <p>Here, the <code>filter</code> parameter will be defined with the <code>NO_FILE</code> prefix in case the input parameter <code>genome</code> is empty. When the <code>genome</code> parameter is found, though, the <code>filter</code> parameter will be defined as <code>---filter ${genome}\"</code>.</p> <pre><code>filter = genome.name.startsWith('NO_FILE') ? \"\" : \"--filter ${genome}\"\n</code></pre>"},{"location":"ViaFoundry/process/#outputs","title":"Outputs","text":"<p>All the output nodes for a new process are defined in this section. In a process similar to that of adding inputs, you can click the <code>Add output...</code> dropdown menu to select from a list of output parameters. Once an output has been chosen, you'll see an <code>Output Name</code> box, as well as an <code>Add/Remove Operator</code> button (wrench icon) and an <code>Operators</code> button.</p> <p></p> <p>You can select output files created by the process by using the <code>Output Name</code> box. Entering a pattern of output files will highlight and grab all files starting with that pattern; for instance, entering \"genome.index*\" in this field will grab all the files whose names begin with \"genome.index.\" You can also use Nextflow variables, which are defined in the Inputs and Scripts sections. Here are some examples of input and output qualifiers and names:</p> Input Qualifier Input name Output Qualifier Output name file genome file \"${genome}_out.txt\" set val(name), file(reads) set val(name), file(\"${name}.unmap*.fq\") set val(name), file(reads) file \"${name}.bam\" set val(name), file(genome) file \"genome.index*\" <p>Additionally, if you need to transform values emitted by a channel, you can click the <code>Add/Remove Operator</code> button, then select operators from the <code>Operators</code> dropdown menu that appears. For more detailed information, refer to the Operators section.</p> <ul> <li>A Note on Optional Outputs:</li> </ul> <p>If your process doesn't generate some outputs in some circumstances, you can check the <code>Optional</code> checkbox, which tells Nextflow not to fail and exit the process if the given output is not created.</p>"},{"location":"ViaFoundry/process/#scripts","title":"Scripts","text":"<p>All scripts for the process are defined in this region. Nextflow supports three different modes: Script, Shell, and Exec. These modes define how the command gets executed by the process.</p> <p>A. Script:</p> <p>For simplicity, Via Foundry uses the script format by default. This means that defined commands will be executed as a BASH script in the host machine, which is the same as using three double quotes <code>\"\"\"</code> at the start and the end of the command block. If you use three double quotes on separated lines, Via Foundry will interpret the area between those lines as a command block. As such, both of the following two blocks will be executed the same way:</p> <pre><code>script:\n\"\"\" \ntophat2 -o . ${indexPath} ${reads} \n\"\"\"\n\nOR\n\ntophat2 -o . ${indexPath} ${reads}\n</code></pre> <p>The script block can contain any command or script that is typically used in a terminal shell or BASH script.</p> <p></p> <p>Let's analyze a more complicated version of the script block. Here, the Nextflow variable <code>name</code> is defined in between the <code>script:</code> keyword and three opening double quotes <code>\"\"\"</code>:</p> <pre><code>script:\nname =  reads.toString() - '.fastq'  // local scope for nextflow variables\n\n\"\"\" \nnewPath=\"/mypath\"   ## inside of \"\"\" block is used to define bash variables in local scope\nSTAR -o . \\${newPath} ${name}  \n\"\"\"\n</code></pre> <p>Note:</p> <ul> <li>The <code>newPath</code> variable is defined in the bash script and used in         the STAR command as <code>\\${newPath}</code>. (Note that bash variables         need to be escaped by backslashes in script block)</li> <li> <p>The <code>name</code> variable is defined in Groovy's scope as a Nextflow         variable, and used in the STAR command as <code>${name}</code>.</p> </li> <li> <p>Conditional Scripts:</p> </li> </ul> <p>Conditional scripts can be used with control statements such as \"if\" and \"switch.\" To use this feature, you need to start the code block with the keyword \"script:\". This tells the interpreter to evaluate all the subsequent statements as one code block and identifies the script string to be executed. Here's an example to illustrate this:</p> <pre><code>script:\nname =  reads.toString() - '.fastq'\n\n// This block runs if the mate variable is initialized with the value \"pair\"\nif (mate == \"pair\") {\n    \"\"\"\n    bowtie2 -x genome.index -1 ${reads.join(' -2 ')} -S ${name}_alignment.sam --un-conc ${name}_unalignedreads\n    \"\"\"\n}\n\n// This block runs if the mate variable is initialized with value \"single\"\nelse if (mate == \"single\") {\n    \"\"\"\n    bowtie2 -x genome.index -U $reads -S ${name}_alignment.sam --un ${name}_unalignedreads\n    \"\"\"\n}\n</code></pre> <p>Tip: As shown in the example above, you can run Bowtie based on the mate status of the reads. In order to enable this capability, you must add the <code>mate</code> parameter as an input. When you are running the pipeline, you can choose <code>single</code> or <code>pair</code> based on your needs.</p> <p>B. Shell:</p> <p>Alternatively, you can use the <code>shell</code> block, wherein Nextflow variables are declared with an exclamation mark <code>!</code>. This feature allows you to use both Nextflow and BASH variables in the same code without using an escape character. In order to use the shell mode, add <code>shell:</code> at the beginning of your code, and three single quotes (<code>'''</code>) at the start and the end of the command block, like so:</p> <pre><code>shell:\n'''\necho $PATH and !{new_path}\n'''\n</code></pre> <p>Here is an example use case of a Perl script block with correct syntax:</p> <pre><code>shell:\nname =  reads.toString() - '.fastq'  //local scope for nextflow variables\n\n''' \n#!/usr/bin/env perl // inside of ''' block you can define perl (or other language) variables in local scope\n$newPath=\"/mypath\";\nsystem(\"STAR -o . ${newPath} !{name}\");\n'''\n</code></pre> <p>Note:</p> <ul> <li>The <code>$newPath</code> variable is defined in the Perl script, and used         in the Tophat command as <code>${newPath}</code>. (Note that variables         don't need to be escaped by backslash if the <code>shell:</code> keyword         is used.)</li> <li>The <code>name</code> variable is defined in Groovy's scope as a Nextflow         variable, and used in the Tophat command as <code>!{name}</code>.</li> </ul> <p>C. Exec:</p> <p>Nextflow processes allows you to execute native code other than just system commands. This lets you easily follow local Nextflow variables. To initialize exec mode, you can add an <code>exec:</code> block at the beginning of the script, like this:</p> <pre><code>exec:\nprintln \"${genome}\"\n</code></pre>"},{"location":"ViaFoundry/process/#operators","title":"Operators","text":"<p>If you need to transform values emitted by a channel, you can click the <code>Operators</code> button and select operators from the dropdown menu. This allows you to apply various operations or modifications to the emitted values. Optionally, you can specify the operator content to customize its behavior according to your requirements. Additionally, you can add multiple operators by enclosing them within parentheses <code>( )</code>.</p> <p>Here are some examples of operators and their usage:</p> Operator Operator Content Usage flatMap ().buffer(size:3) Groups 3 emitted items into one new channel. mode flatten To emit each output file as a sole item. groupTuple Collects tuples (or lists) of values emitted and groups them based on their key value. <p>Tip: To get more information about operators, check out this link to Nextflow's documentation.</p>"},{"location":"ViaFoundry/process/#foundry-variables","title":"Foundry Variables","text":"<p>Foundry Variables are provided by the Foundry platform and are intended to enhance the flexibility and adaptability of your workflows. These variables (e.g. <code>FOUNDRY_RUN_ID</code>, <code>FOUNDRY_PIPELINE_VERSION</code>, etc.) can be used in your process scripts or configuration files.</p> Variable Description {{FOUNDRY_PIPELINE_ID}} The identifier assigned to each pipeline. {{FOUNDRY_PIPELINE_URL}} The URL assigned to each pipeline. {{FOUNDRY_PIPELINE_VERSION}} The version assigned to each pipeline. {{FOUNDRY_RUN_ID}} The unique identifier assigned to each run. {{FOUNDRY_RUN_URL}} The URL that links to the detailed information and logs for the current run. {{FOUNDRY_WEB_REPORT_DIR}} The URL where run reports are located. {{FOUNDRY_WEB_RUN_DIR}} The URL where run logs are located. {{FOUNDRY_PUBLISH_DIR}} The directory path where all reported files are located. {{FOUNDRY_LAB}} The lab associated with the Foundry account under which the current run is running. {{FOUNDRY_USERNAME}} The username associated with the Foundry account under which the current run is running. {{FOUNDRY_EMAIL}} The email address associated with the Foundry account under which the run is running. <p>Usage:  When scripting within the Foundry platform, you can directly reference these variables by enclosing them in double curly braces. For example:</p> <pre><code>echo \"Current Run ID: {{FOUNDRY_RUN_ID}}\"\n</code></pre>"},{"location":"ViaFoundry/process/#header-script","title":"Header Script","text":"<p>This section, found within the <code>Advanced Options</code> menu, allows you to add additional scripts or comments before the beginning of the process block. This allows you to re-call the same function several times in the Script section.</p> <p></p>"},{"location":"ViaFoundry/process/#process-options","title":"Process Options","text":"<p>Via Foundry contains functionality supporting seamless separation of main process inputs and optional user-manipulable parameters, the latter being controllable with the Process Options feature. This section will go over how to configure process options, but before that, here is how the results of the commands entered in the Process Options are displayed; note that all process options are located below the Inputs section of the run page, and can be edited by the user depending on their needs.</p> <p></p> <p>In order to create these forms, you need to use the following syntax in the Script or Process Header section of the <code>Add New Process</code> or <code>Edit/Delete Process</code> page for your process (shown below the syntax):</p> <pre><code>variableName = defaultValue //* @formType @description:\"...\" @tooltip:\"...\" @options:\"...\"\n</code></pre> <p></p> <p>Note that when defining a defaultValue for a variable in Via Foundry, you can use single or double quotes for strings, or no quotes for numbers. If you want to define an array of default values for a variable using the @style tag, you can use the following format: <code>variableName = [\"defaultValue1\", \"defaultValue2\"]</code>. This allows you to set multiple default values for the variable.</p>"},{"location":"ViaFoundry/process/#formtype","title":"@formtype","text":"<p>In Via Foundry's Process Options section, you can use four different commands to specify the type of form that will be displayed for a given parameter: <code>@input</code>, <code>@textbox</code>, <code>@checkbox</code>, and <code>@dropdown</code>. Here are brief descriptions of each, accompanied by images highlighting what they look like on the run page:</p> <p>@input: Creates a single-line text field. Here is an example usage, along with the form field these commands create on the run page:</p> <pre><code>readsPerFile = 5000000 //* @input @description:\"The number of reads per file\"\n    params_tophat = \"-N 4\" //* @input @description:\"Tophat parameters\" @tooltip:\"parameters for Tophat2 version 2.6\"\n</code></pre> <p></p> <p>@textbox: Creates a multi-line text field. Example usage:</p> <pre><code>Adapter_Sequence = \"\" //* @textbox @description:\"You can enter a single sequence or multiple sequences in different lines.\"\n</code></pre> <p></p> <p>@checkbox: Creates a checkbox field with options <code>true</code> or <code>false</code> by default. Example usage:</p> <pre><code>run_rRNA_Mapping = \"false\" //* @checkbox @description:\"Check the box to activate rRNA mapping.\"\nrRNA_filtering = \"true\" //* @checkbox @description:\"Check the box to filter rRNA reads.\"\n</code></pre> <p></p> <p>@dropdown: Creates a dropdown menu, whose options can be specified with the <code>@options</code> feature. Example usage:</p> <pre><code>genomeType = \"\" //* @dropdown @description:\"Genome type for pipeline\" @options:\"hg19\",\"mm10\", \"custom\"\n</code></pre> <p></p>"},{"location":"ViaFoundry/process/#non-form-type-specifiers","title":"Non-Form Type Specifiers","text":"<p>@description: Provide brief, informative descriptions of inputs with the <code>@description</code> tag. For example, in the @dropdown example above, \"Genome type for pipeline\" is provided as the value of <code>@description</code>, so in the run page, the blurb beneath the dropdown menu says \"Genome type for pipeline\".</p> <p>@tooltip: Provide more detailed descriptions of inputs and/or supplement your <code>@description</code> with the <code>@tooltip</code> tag. See the example below for possible usage:</p> <pre><code>params_tophat = \"-N 4\" //* @input @tooltip:\"parameters for Tophat2 version 2.6\" @description:\"Tophat parameters\"\n</code></pre> <p>@title: You can create a header on top of a variable by using the <code>@title</code> tag. This allows you to organize complex form structures more easily. Here's an example:</p> <pre><code>params_tophat = \"-N 4\" //* @input @title:\"Alignment Section\" @description:\"Tophat parameters\"\n</code></pre> <p>In this example, the <code>params_tophat</code> variable is grouped under the \"Alignment Section\" header in the form, making it easier to distinguish and organize related variables.</p> <p>@optional: When defining an input, you can designate it as optional by using the <code>@optional</code> tag. This gives users the freedom to leave certain inputs empty if they so desire. See the example below:</p> <pre><code>params.tsv_input = \"\" //* @input @optional @description:\"TSV input for process\"\nparams.csv_input = \"\" //* @input @optional @description:\"CSV input for process\"\n</code></pre> <p>@file: You can specify the type of window that will appear for a selected input. By default, a <code>value modal</code> is used. However, if you add the @file tag, the <code>file modal</code> will be displayed. This allows users to upload files and use them as a parameter. Here's an example:</p> <pre><code>params.tsv_input = \"\" //* @input @file @description:\"TSV file path for process\"\n</code></pre> <p>Explanation: In this example, the <code>params.tsv_input</code> parameter is defined as a file input. When the user selects this input in the run page, a file modal will be displayed, allowing them to upload a TSV file to be used as the parameter value.</p> <p>@options: When you define a dropdown form field using the <code>@dropdown</code> formType, you should also specify the available options using the <code>@options</code> tag. Here's a simple example:</p> <pre><code>genomeType = \"\" //* @dropdown @options:\"hg19\",\"mm10\",\"custom\"\n</code></pre> <p>Here, the <code>genomeType</code> parameter is defined as a dropdown field. The available options for selection are \"hg19,\" \"mm10,\" and \"custom.\" When the user selects this input in the run page, a dropdown menu will be displayed with these options to choose from. The selected option will be used as the parameter value.</p>"},{"location":"ViaFoundry/process/#conditional-options-version-1-advanced-usage","title":"Conditional Options - Version 1 (Advanced Usage):","text":"<p>Since the same process can be viewed differently based on the pipeline being used, in order to control the visibility of dropdown options in different pipelines, you can define variables in the pipeline header starting with an underscore. For example:</p> <pre><code>_nucleicAcidType = \"rna\" //In RNA-seq pipeline header\n_nucleicAcidType = \"dna\" //In ChIP-seq pipeline header\n</code></pre> <p>You can then use these variables to control which options will be visible in the dropdown fields of different pipelines using the following format:</p> <pre><code>param = \"\" //* @dropdown @options:{_nucleicAcidType=\"rna\",\"rRNA\",\"miRNA\",\"snRNA\"},{_nucleicAcidType=\"dna\", \"ercc\",\"rmsk\"}\n</code></pre> <p>Now, the <code>param</code> dropdown will have three options (\"rRNA\", \"miRNA\", \"snRNA\") in the RNA-seq pipeline and two options (\"ercc\", \"rmsk\") in the ChIP-seq pipeline. You can also define default options by not assigning any value, as shown in the example below:</p> <pre><code>param = \"\" //* @dropdown @options:{\"rRNA\",\"miRNA\",\"snRNA\"},{_nucleicAcidType=\"dna\",\"ercc\",\"rmsk\"}\n</code></pre> <p>In this case, by default, the three options (\"rRNA\", \"miRNA\", \"snRNA\") will be visible unless the pipeline header defines <code>_nucleicAcidType=\"dna\"</code>.</p>"},{"location":"ViaFoundry/process/#conditional-options-version-2-advanced-usage","title":"Conditional Options - Version 2 (Advanced Usage):","text":"<p>To control the visibility of dropdown options based on the selected parameter in another dropdown, you can use the following syntax. In this example, the dropdown called <code>sequence</code> controls the visible options of the <code>dropdown</code> aligner:</p> <pre><code>aligner = \"\" //* @dropdown @options:{sequence=(\"rRNA\",\"miRNA\",\"snRNA\"),\"bowtie\",\"bowtie2\"},{sequence=\"genome\", \"star\"}\n</code></pre> <p>When <code>sequence</code> is selected as one of the options \"rRNA\", \"miRNA\", or \"snRNA\", the <code>aligner</code> dropdown will display options \"bowtie\" and \"bowtie2\". Similarly, when <code>sequence</code> is selected as \"genome\", the <code>aligner</code> dropdown will display the \"star\" option.</p> <p>This allows you to dynamically control the available options in a dropdown based on the selected value of another dropdown parameter.</p>"},{"location":"ViaFoundry/process/#styles-for-process-options","title":"Styles for Process Options","text":"<p>Via Foundry supports the use of additional tags to shape the layouts of form fields: <code>@multicolumn</code>, <code>@array</code>, and <code>condition</code>.</p>"},{"location":"ViaFoundry/process/#multicolumn","title":"@multicolumn","text":"<p>The <code>@multicolumn</code> form type enables you to group variables on the same line as one another for more coherent visualizations.</p> <p>Example usage:</p> <pre><code>var1 = \"\" //* @input @description:\"description of var1\"\nvar2 = \"\" //* @input @description:\"description of var2\"\nvar3 = \"\" //* @input @description:\"description of var3\"\nvar4 = \"\" //* @input @description:\"description of var4\"\nvar5 = \"\" //* @input @description:\"description of var5\"\nvar6 = \"\" //* @input @description:\"description of var6\"\n//* @style @multicolumn:{var1, var2, var3}, {var5, var6}\n</code></pre> <p>Here, var1, var2, and var3 will be displayed on the same row, as they are grouped in the same list in the @multicolumn value. Similarly, var5 and var6 will share a row, and since var4 isn't listed in the @multicolumn argument, it will fill a single row by default. See the image below:</p> <p></p>"},{"location":"ViaFoundry/process/#array","title":"@array","text":"<p>The <code>@array</code> form type lets you group variables together and link them with add/remove buttons. Here's an example usage:</p> <pre><code>var1 = \"\" //* @input @description:\"description of var1\" @title:\"Step 1\"\nvar2 = \"\" //* @input @description:\"description of var2\"\nvar3 = \"\" //* @input @description:\"description of var3\"\nvar4 = \"\" //* @input @description:\"description of var4\" @title:\"Step 2\"\n//* @style @array:{var1, var2}, {var4}\n</code></pre> <p>In this example, var1 and var2 are grouped together and linked to add/remove buttons. Clicking the add button will create new var1 and var2 fields just below the existing ones. Similarly, the remove button will remove the generated copies of form fields. The same features apply to var4 as well. The below image demonstrates this nicely.</p> <p></p> <p>A helpful tip: You can combine multiple style options on the same variables. For example, <code>//* @style @array:{var1, var2}, {var4} @multicolumn:{var1, var2}</code> will combine both the multicolumn and array features for var1 and var2, as seen in this image.</p> <p></p> <p>You can also define multiple default values by using the following syntax:</p> <pre><code>var1 = [\"defVal1\", \"defVal2\"] //* @input @description:\"description of var1\"\n</code></pre> <p>With this line, on the run page, two rows will be displayed for var1, and their default values will be \"defVal1\" and \"defVal2\".</p>"},{"location":"ViaFoundry/process/#condition","title":"@condition","text":"<p>The <code>@condition</code> tag allows you to bind the value of one form field to the visibility of other form fields. Here's an example:</p> <pre><code>var1 = \"\" //* @dropdown @description:\"description of var1\" @options:\"yes\", \"no\" @title:\"Step 1\"\nvar2 = \"\" //* @input @description:\"description of var2\"\nvar3 = \"\" //* @input @description:\"description of var3\"\nvar4 = \"\" //* @input @description:\"description of var4\"\nvar5 = \"\" //* @input @description:\"description of var5\" @title:\"Step 2\"\n//* @style @condition:{var1=\"yes\", var2}, {var1=\"no\", var3, var4}\n</code></pre> <p>In this example, the value of var1 is linked to other form fields. When var1 is selected as \"yes\", the field for var2 will be shown. On the other hand, when var1 is changed to \"no\", the var2 field will disappear, and the fields for var3 and var4 will appear. Since var5 is not defined in the @condition tag, it will always be visible regardless of changes in other fields. Feel free to check this image for clarification:</p> <p></p> <p>You can combine multiple style options on the same variable, including @condition. For example, <code>//* @style @condition:{var1=\"yes\", var2}, {var1=\"no\", var3, var4} @array:{var1, var2, var3, var4} @multicolumn:{var1, var2, var3, var4}</code> will combine all the features just discussed.</p> <p></p>"},{"location":"ViaFoundry/process/#autofill-feature-for-process","title":"Autofill Feature for Process","text":"<p>To automate the filling of executor properties in Via Foundry, you can utilize the autofill feature. There are two types of autofill options available: hostname-independent autofill and hostname-dependent autofill.</p> <p>Hostname-Independent Autofill:</p> <p>To define executor properties that will be automatically filled regardless of the hostname, you can use the following syntax:</p> <pre><code>//* autofill\n&lt;executor properties&gt;\n//* autofill\n</code></pre> <p>Hostname-Dependent Autofill:</p> <p>If you need to overwrite the default executor properties based on specific hostnames, you can use hostname-dependent executor properties. Here's the syntax:</p> <pre><code>//* autofill\n&lt;executor properties&gt;\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n&lt;hostname dependent executor properties&gt;\n}\n//* autofill\n</code></pre> <p>In this example, the <code>&lt;executor properties&gt;</code> section will be filled for all hostnames. However, if the hostname is \"ghpcc06.umassrc.org\", the <code>&lt;hostname dependent executor properties&gt;</code> section will be additionally filled.</p> <p>The <code>$HOSTNAME</code> variable in Via Foundry represents the selected hostname in the run environment. By using this variable, you can apply specific executor properties based on the hostname.</p>"},{"location":"ViaFoundry/process/#executor-properties","title":"Executor Properties:","text":"<p>There are five types of executor properties available to autofill Executor Settings for All Processes: <code>$TIME</code>, <code>$CPU</code>, <code>$MEMORY</code>, <code>$QUEUE</code>, <code>$EXEC_OPTIONS</code> which respectively define the Time, CPU, Memory, Queue/Partition and Other Options fields in the executor settings. See the example below:</p> <pre><code>//* autofill\n$TIME = 1000\nif ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n    $TIME = 3000\n    $CPU  = 4\n    $MEMORY = 100\n    $QUEUE = \"long\"\n    $EXEC_OPTIONS = '-E \"file /home/garberlab\"'\n}\n//* autofill\n</code></pre> <p></p> <p>In this example, since the run environment is set as \"ghpcc06.umassrc.org\", the autofill feature overwrote the default <code>$TIME</code> value (1000) with a value of 3000.</p>"},{"location":"ViaFoundry/process/#platform-tag","title":"Platform Tag:","text":"<p>To isolate platform-dependent parameters in Via Foundry, you can use the platform tag. This allows you to exclude platform-specific parameters from the exported process, and when the process is imported, existing platform-dependent parameters will not be overwritten. Here's an example of how to use the platform tag:</p> <pre><code>//* autofill\n$MEMORY = 32\n$CPU  = 1\n    //* platform\n    if ($HOSTNAME == \"ghpcc06.umassrc.org\"){\n        $TIME = 3000\n        $CPU  = 4\n        $MEMORY = 100\n        $QUEUE = \"long\"\n        $EXEC_OPTIONS = '-E \"file /home/garberlab\"'\n    }\n    //* platform\n//* autofill\n</code></pre>"},{"location":"ViaFoundry/process/#permissions-groups-and-publish","title":"Permissions, Groups and Publish","text":"<p>By default, all new processes are only seen by their owner. You have the option to share your process with a specific group that you have created in the profile's \"Groups\" tab. To do this, choose \"Only my group\" and select the name of the desired group. Members of that group will then be able to view the process on their pipeline page.</p> <p>When collaborating with multiple individuals on a shared pipeline, you have the option to grant write permission to a group by utilizing the \"Group Permission to Write\" dropdown. This allows you to specify a group and authorize them to make changes and modifications to the process.</p> <p></p>"},{"location":"ViaFoundry/process/#copying-and-revisions","title":"Copying and Revisions","text":"<p>You can easily create a copy of your process by clicking the <code>Settings</code> button, identifiable by an image of three dots and located at the top right corner of the process window, and then selecting \"Copy Process\" from the dropdown menu. This allows you to create a duplicate of the process while retaining the original one.</p> <p>It's important to note that once your process becomes public or is being used by other group members, you are not allowed to make changes to the same revision directly. Instead, a new revision of the process will be created, and any modifications or updates can be made to this new revision. This ensures that the original process remains intact and unchanged, while allowing you to iterate and improve upon a separate copy.</p> <p></p>"},{"location":"ViaFoundry/process/#how-to-cite-us","title":"How To Cite Us","text":"<p>If you use Via Foundry (formerly DolphinNext) in your research, please cite:</p> <p>Yukselen, O., Turkyilmaz, O., Ozturk, A.R. et al. DolphinNext: a distributed data processing platform for high throughput genomics. BMC Genomics 21, 310 (2020). https://doi.org/10.1186/s12864-020-6714-x</p>"},{"location":"ViaFoundry/process/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"},{"location":"ViaFoundry/profile/","title":"Profile Guide","text":""},{"location":"ViaFoundry/profile/#profile-guide","title":"Profile Guide","text":"<p>In this guide, you'll discover all the available options you can explore on your Via Foundry Profile page.</p>"},{"location":"ViaFoundry/profile/#profile-page","title":"Profile Page","text":"<p>After logging in, simply click on the \"Profile\" tab located at the top-right corner of your screen. You'll see a number of different tabs on your profile page that you can click through and explore.</p> <p></p> <p>The platform offers several options for creating connection profiles and managing your credentials.</p> <ul> <li>First up is the Run Environments tab, which is your main hub for     creating connection profiles.</li> <li>Next, you can use the Groups tab to create a group and add     members to it, allowing you to share your runs or pipelines with     others.</li> <li>To manage your SSH keys, you'll need to head over to the SSH Keys tab. This section allows you to perform essential actions such as generating new keys or inputting pre-existing SSH key pairs within the Edit section. By utilizing this functionality, you can establish secure connections with various hosts. It's worth mentioning that for the majority of accounts, these keys are automatically generated,</li> <li>You can also add your Amazon Keys or Google Keys in order to     execute your runs in the cloud.</li> <li>Under the Repositories tab, you can     enter your security credentials to pull/push pipeline information to     your Github or Bitbuckets accounts.</li> <li>If you currently rely on a password for logging into your account, you can easily modify your password by accessing the dedicated Change Password section. </li> <li>Lastly, in the Notification tab, you can opt into receiving     emails about completed or failed runs.</li> </ul> <p>It's important to note that before creating a run environment, you'll need to create SSH keys in the Run Environments tab. And if you plan on executing runs in the cloud, you'll need to add your Amazon or Google keys as well.</p>"},{"location":"ViaFoundry/profile/#ssh-keys","title":"SSH Keys","text":"<p>To create or enter existing SSH key pairs, navigate to the SSH Keys tab and click on the <code>Add SSH Key</code> button in the top right corner.</p> <p>Next, you'll need to choose between two methods:</p> <ul> <li>A. Use your own keys: If you choose this option, you'll need to     provide your private and public key pairs. You can find these keys     on your computer at the default location: '~/.ssh/id_rsa 'for the     private key and '~/.ssh/id_rsa.pub' for the public key. Simply     copy and paste these keys into the appropriate fields in your     browser. If these files don't exist or you want to create new ones,     check out this      link     for guidance.</li> <li>B. Create new keys: To generate a new pair of SSH keys, simply     click the \"Generate Keys\" button.</li> </ul> <p>Once you have saved your key, rest assured that your information will be encrypted and securely stored. To establish a connection, you need to add your public key to the '~/.ssh/authorized_keys' file on the host machine. This step ensures that your public key is recognized and authorized for secure communication with the host. If you require assistance with this step or have any questions, please don't hesitate to contact us at support@viascientific.com.</p>"},{"location":"ViaFoundry/profile/#amazon-keys","title":"Amazon Keys","text":"<p>To enter your AWS security credentials (access key, secret key, and default region), head over to the Amazon Keys tab and click on the <code>Add Amazon Key</code> button. Rest assured that your information will be encrypted and kept secure, and only you will have full access to view and modify your key information.</p> <p>Note: Once you've saved your key, it won't be visible for security purposes. However, you can always overwrite it with a new key or delete it if needed.</p>"},{"location":"ViaFoundry/profile/#google-keys","title":"Google Keys","text":"<p>To enter your Project ID and Service Account Key in the Google keys tab, start by clicking the <code>Add Google Key</code> button.</p> <ul> <li>For your Project ID, head over to the Google Cloud Console and     navigate to the Dashboard section. From there, check the Project     info box to find your Project ID, which should look something like     \"viafoundry-193616\".</li> <li>To input your Service Account Key, also head over to the Google     Cloud Console and navigate to APIs &amp; Services \u2192 Credentials.     From there, click on the <code>Create Credentials</code> drop-down and select     <code>Service Account Key</code>. On the following page, choose an existing     service account or create a new one if needed, then select JSON as     the \"Key Type\". Finally, click the Create button and download the     JSON file with a name of your choice (e.g., creds.json).</li> </ul> <p>Remember that, after saving your key, you won't be able to view your Service Account Key for security reasons. However, you can always overwrite it with a new key or delete it if necessary.</p>"},{"location":"ViaFoundry/profile/#groups","title":"Groups","text":"<p>In the Groups tab, you can create groups by selecting the <code>Create a Group</code> button. Once you have created a group, you can add members by clicking the <code>Options &gt; Edit Group Members</code> button. This interactive platform allows you to share your process, pipeline, or projects with your group members. To view the current members of the group, select the <code>Options &gt; View Group Members</code> button. Additionally, you have the option to delete your group by selecting the <code>Options &gt; Delete Group</code> button, or to edit its name with <code>Options &gt; Edit Group Name</code>.</p>"},{"location":"ViaFoundry/profile/#run-environments","title":"Run Environments","text":"<p>This section is used for defining connection profiles by clicking on the <code>Add Environment</code> button. You can choose from three options: Host, Amazon or Google.</p> <ul> <li>Host: The \"Host\" option is designed for users who have access to High Performance Computing (HPC) environments or personal workstations. If you intend to submit jobs to AWS Batch, you can utilize the \"Host\" option as well. Whether you are working with HPC systems or leveraging the power of AWS Batch, the \"Host\" option provides a flexible and scalable solution for running your workflows.</li> <li>Amazon: This option is for users who have an Amazon Web Services     (AWS) account or plan to create an EC2 instance to run jobs in the     cloud.</li> <li>Google: This option is for users who want to use their Google     Cloud account to run jobs in the cloud.</li> </ul>"},{"location":"ViaFoundry/profile/#a-defining-host-profiles","title":"A. Defining Host Profiles:","text":"<ul> <li> <p>Username/Hostname: To connect to a remote host, you will need to provide your username and the hostname of the remote host separately. For example, if your run will be submitted to <code>us2r@ghpcc06.umassrc.org</code>, you should enter <code>us2r</code> as the username and <code>ghpcc06.umassrc.org</code> as the hostname. </p> </li> <li> <p>SSH Port (optional): By default, Via Foundry uses TCP port 22     for SSH connections. However, you can specify a different port     number if needed.</p> </li> <li> <p>SSH Keys: Via Foundry stores your SSH keys in the SSH keys tab     and uses them to authenticate your SSH connections.</p> </li> <li> <p>Nextflow Path (optional): If the Nextflow executable is not in     your $PATH, you can specify the path to the executable in this     block. For example:</p> <pre><code>/project/umw_biocore/bin\n</code></pre> </li> <li> <p>Singularity Cache Folder: Via Foundry uses a local directory to     store Singularity images downloaded from remote hosts. By default,     this directory is located in your home directory. </p> </li> <li> <p>Profile Variables: To facilitate the use of genome reference and     index files in your pipelines, you can specify a download directory     in which these files are stored. If multiple users are using Via     Foundry, it is recommended to use a shared path in your cluster. For     example:</p> <pre><code>params.DOWNDIR=\"/share/viafoundry\"\n</code></pre> </li> <li> <p>Environment Variables: You can set BASH environmental variables     here. Note: don't use spaces to separate multiple variables; use     newlines instead.</p> </li> <li> <p>Executor Settings: In Via Foundry, there are four different     sections to control executor settings: the first two are defined in     Profile -&gt; Run Environment, and the remaining two are     adjusted in the Advanced tab of the run page. If you select an     executor other than \"Local\" or \"Ignite\", Via Foundry prompts you     to enter additional settings, such as the queue/partition, memory,     CPU, and time.</p> <p>1. Executor of Nextflow (navigate to Profile -  Run Environments):</p> <p>This setting controls how Via Foundry initiates Nextflow. Currently, Via Foundry supports the Local, SGE, SLURM, and LSF executors to initiate Nextflow. For the SGE, SLURM, and LSF executors, Via Foundry only uses them to run Nextflow itself, so the time limit should be long enough to execute all processes in the pipeline. For local execution, Via Foundry limits the total amount of memory and CPU that can be used, so these values should be close to the maximum capacity of your computer.</p> <ul> <li>Suggested parameters for SGE/SLURM/LSF: long (queue) 8 (GB     Memory) 1 (CPU) 5000-8000 (min, Time)</li> <li>Suggested parameters for Local: 100 (GB Memory) 8 (CPU)</li> </ul> <p>2. Executor of Nextflow Jobs (navigate to Profile -  Run Environments):</p> <p>This setting will be used as the default setting for submitted jobs by Nextflow if you don't set any parameters in the Advanced section of your run page.</p> <ul> <li>Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB     Memory) 1 (CPU) 240 (min, Time)</li> <li>Suggested parameters for Local: 20 (GB Memory) 1 (CPU)</li> </ul> <p>3. Executor Settings for All Processes (in the advanced tab of run page):</p> <p>These settings will overwrite those in Executor of Nextflow Jobs  and set default parameters for all Nextflow Jobs.</p> <ul> <li>Suggested parameters for SGE/SLURM/LSF: short (queue) 20 (GB     Memory) 1 (CPU) 240 (min, Time)</li> <li>Suggested parameters for Local: 20 (GB Memory) 1 (CPU)</li> </ul> <p>4. Executor Settings for Each Process (in the advanced tab of run page):</p> <p>If a particular process requires different parameters than the defaults (which are defined in the Executor Settings for All Processes or Executor of Nextflow Jobs sections), you can overwrite the general settings by clicking the checkbox of the process that you want to change. This will only affect the settings of the selected process and keep the original settings for the rest of the processes.</p> <ul> <li>Suggested parameters for SGE/SLURM/LSF: long (queue) 20 (GB Memory) 4 (CPU) 1000-5000 (min, Time)</li> <li>Suggested parameters for Local: 20 (GB Memory) 4 (CPU)</li> </ul> <p>Note:  If non-standard resources or settings are required for the executor,   then you can specify these parameters by using Other Options box.   For instance, to submit an SGE job with 3 CPU using parallel   environments, you can enter <code>-pe orte 3</code> (to use MPI for   distributed-memory machines) or <code>-pe smp 3</code> (to use OpenMP for   shared-memory machines) in the Other Options box, leaving the CPU   box empty.</p> </li> </ul>"},{"location":"ViaFoundry/profile/#b-defining-aws-batch-profiles","title":"B. Defining AWS Batch Profiles:","text":"<p>Please choose type of the run environment as \"Host\" and enter following information.</p> <ul> <li> <p>Username/Hostname: To connect to a remote host, you will need to provide your username and the hostname of the remote host separately. For example, if your run will be submitted to <code>us2r@ghpcc06.umassrc.org</code>, you should enter <code>us2r</code> as the username and <code>ghpcc06.umassrc.org</code> as the hostname. </p> </li> <li> <p>SSH Port (optional): By default, Via Foundry uses TCP port 22     for SSH connections. However, you can specify a different port     number if needed.</p> </li> <li> <p>SSH Keys: Via Foundry stores your SSH keys in the SSH keys tab     and uses them to authenticate your SSH connections.</p> </li> <li> <p>Nextflow Path (optional): If the Nextflow executable is not in     your $PATH, you can specify the path to the executable in this     block. For example:</p> <pre><code>/project/umw_biocore/bin\n</code></pre> </li> <li> <p>Profile Variables: To facilitate the use of genome reference and     index files in your pipelines, you can specify a download directory     in which these files are stored. If multiple users are using Via     Foundry, it is recommended to use a shared path in your cluster. For     example:</p> <pre><code>params.DOWNDIR=\"/share/viafoundry\"\n</code></pre> </li> <li> <p>Environment Variables: You can set BASH environmental variables     here. Note: don't use spaces to separate multiple variables; use     newlines instead.</p> </li> <li> <p>Executor of Nextflow: Please select the <code>Local</code> Nextflow     executor.</p> </li> <li> <p>Executor Settings for Nextflow: Please enter 10GB in the     Memory field and 1 in the CPU field.</p> </li> <li> <p>Executor of Nextflow Jobs: Please select <code>AWS Batch</code> for     Nextflow jobs.</p> </li> <li> <p>Queue, Memory, CPU, and other options: Please enter the queue     name and set the default memory and CPU you'll allocate for each     job (e.g. 10GB memory and 1CPU). These settings can be adjusted in     the run page.</p> </li> <li> <p>Amazon Keys: AWS credentials that are saved in the Amazon     Keys tab will allow you to submit jobs to AWS Batch. However, if an IAM role is already defined for the host machine, you can skip this step. The IAM role associated with the host machine grants the necessary permissions for interacting with AWS services, including AWS Batch. By either providing the AWS credentials or utilizing the existing IAM role, you can seamlessly submit your jobs to AWS Batch and leverage the power of cloud computing for your workflows.</p> </li> <li> <p>Default Working Directory: Default directory in the host machine     where runs will be executed. (eg. <code>/data/viafoundry</code>)</p> </li> <li> <p>Default Bucket Location for Publishing: The default bucket location where run reports will be published. (e.g.     <code>s3://bucket/viafoundry</code>)</p> </li> </ul>"},{"location":"ViaFoundry/profile/#c-defining-non-batch-amazon-web-services-profiles","title":"C. Defining Non-Batch Amazon Web Services Profiles:","text":"<ul> <li>SSH Keys: These are saved in the SSH Keys tab and will be used when connecting to a host. SSH keys are a secure way to authenticate and encrypt connections between servers. It's recommended to generate a new key pair for each instance you plan to connect to. </li> <li>Amazon Keys: These AWS credentials are saved in the Amazon Keys tab and allow you to start/stop Amazon EC2 instances. These credentials consist of an access key and a secret access key. </li> <li>Instance Type: This refers to the type of Amazon EC2 instance that you will use. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. For example, \"m3.xlarge\" is an instance type that provides a balance of CPU and memory resources. </li> <li>Image ID: This is the virtual machine ID (VM ID) that you will use to launch the instance. The image ID is specific to the region and operating system that you are using. For example, \"ami-032a33ebe57465518\" is a sample image ID for an Amazon Machine Image (AMI) that is based on the Ubuntu operating system.</li> </ul> <p>If you want to create your own image, please install the following programs:</p> <ul> <li>Singularity</li> <li>Docker engine (version 1.11 or  higher)</li> <li>Apache Ignite with Cloud-init package</li> <li>Nextflow</li> <li>AWS  CLI</li> <li> <p>Subnet ID/Security Group/Shared Storage ID/Shared Storage Mount:</p> <p>The filesystem needs to be created at https://console.aws.amazon.com/efs/, and this information will be obtained upon the creation of a shared file system.</p> <ul> <li>Subnet ID: Identifier of the VPC subnet to be applied,          e.g., subnet-05222a43.</li> <li>Security Group: Identifier of the security group to be          applied, e.g., sg-df72b9ba, which is used by default.</li> <li>Shared Storage ID: Identifier of the shared file system          instance, e.g., fs-1803efd1.</li> <li>Shared Storage Mount: Mount path of the shared file          system, e.g., /mnt/efs.</li> </ul> <p>Please make sure the following criteria are satisfied:</p> <ol> <li>The image used must have the directory to mount this          storage.</li> <li>The output directory must be under this mount location.</li> <li>The storage system must be created in the selected region,          and necessary rights need to be given in the console.</li> <li>The EC2FullAccess and S3FullAccess permissions have been          added.  Warning: Both the EFS and images should be located in the same location.</li> </ol> </li> <li> <p>Default Working Directory: This is the default directory in the     host machine where runs will be executed. It is an optional     parameter for AWS. For example, you can set it to \"/data/viafoundry\".</p> </li> <li> <p>Default Bucket Location for Publishing: The default bucket location where run reports will be published. (e.g.     <code>s3://bucket/viafoundry</code>)</p> </li> <li> <p>Nextflow Path (optional): If the Nextflow path is not added to     the $PATH environment variable, you can define the path in this     block. For example, you could set it to <code>/project/umw_biocore/bin</code>.</p> </li> <li> <p>Singularity Cache Folder: This is the directory where remote     Singularity images are stored. By default, the home directory is     used. </p> </li> <li> <p>Profile Variables: You can set commonly used pipeline variables     here. For instance,<code>params.DOWNDIR</code> is used in most of our public     pipelines to save all genome related files (fasta, index etc.), so     you can set this variable like this:</p> <p><code>params.DOWNDIR = \"/share/dnext_data\"</code> You can also enter multiple variables by separating them with newlines.</p> </li> <li> <p>Executor of Nextflow/Executor of Nextflow Jobs: Amazon instances     are automatically configured to use the Ignite executors. As such,     when defining an Amazon profile, you should select <code>Local</code> for     Executor of Nextflow and <code>Ignite</code> for Executor of Nextflow     Jobs.</p> </li> </ul>"},{"location":"ViaFoundry/profile/#d-defining-google-profiles","title":"D. Defining Google Profiles:","text":"<ul> <li> <p>SSH Keys: are saved in SSH keys tab and will be used while     connecting to host.</p> </li> <li> <p>Google Keys: Google credentials that are saved in Google keys     tab and will allow to start/stop Google Cloud instances.</p> </li> <li> <p>Zone: The Google zone where the computation is executed.(eg.     us-east1-b)</p> </li> <li> <p>Instance Type: Google Cloud machine     types that     comprise varying combinations of CPU, memory, storage, and     networking capacity (eg. <code>n1-standard-4</code>).</p> </li> <li> <p>Image ID: Virtual machine ID (eg.     <code>dolphinnext-193616/global/images/dolphinnext-images-v1</code>).</p> <p>If you want to create your own image, please install following programs:</p> <ul> <li>Docker engine (version 1.11 or          higher)</li> <li>Apache Ignite with Cloud-init package</li> <li>Nextflow</li> <li>gcloud</li> </ul> </li> <li> <p>Default Working Directory: This is the default directory in the     host machine where runs will be executed. It is mandatory for Google     Cloud profiles. (e.g., /data/dnext)</p> </li> <li> <p>Default Bucket Location for Publishing: This is the default     bucket location where Via Foundry reports will be published. It is     mandatory for Google Cloud profiles, and you can always edit this     path in the run page. For example, you could set it to     <code>gs://bucket/dnext</code>.</p> </li> <li> <p>Nextflow Path (optional): If the Nextflow path is not added to     the $PATH environment variable, you can define the path in this     block. For example, you could set it to <code>/project/umw_biocore/bin</code>.</p> </li> <li> <p>Singularity Cache Folder: This is the directory where remote     Singularity images are stored. By default, the home directory is     used. Note that when using a computing cluster, it must be a shared     folder that is accessible from all computing nodes.</p> </li> <li> <p>Profile Variables: You can set commonly used pipeline variables     here. For instance,<code>params.DOWNDIR</code> is used in most of our public     pipelines to save all genome related files (fasta, index etc.), so     you can set this variable like this:     <code>params.DOWNDIR = \"/share/dnext_data\"</code> You can also enter multiple     variables by separating them with newlines.</p> </li> <li> <p>Executor of Nextflow/Executor of Nextflow Jobs: Google instances     are automatically configured to use the Ignite executors. As such,     when defining a Google profile, you can select <code>Local</code> for     Executor of Nextflow and <code>Ignite</code> for Executor of Nextflow     Jobs.</p> </li> </ul>"},{"location":"ViaFoundry/profile/#repository-connections","title":"Repository Connections","text":"<p>You can enter your GitHub or Bitbucket security credentials (Username, E-mail, Password) by clicking on the <code>Add Account</code> button in the Repositories tab. Your information will be encrypted and kept secure. By adding a Github or Bitbucket account, you'll be able to push your pipeline information into a public account, where you can then share it with others.</p> <ul> <li> <p>GitHub Credentials: GitHub access token will be used for creating and updating GitHub repositories. Please follow this guide in GitHub to create a token. While creating token, please enable <code>repo</code> and <code>write:packages</code> in the scope section.</p> </li> <li> <p>Bitbucket Credentials: To utilize SSH Key pairs for importing and updating Bitbucket repositories, you need to follow these steps:</p> <ul> <li>In ViaFoundry:<ol> <li>Log in to your ViaFoundry account and go to the \"Profile\" section.</li> <li>Select the \"SSH Keys\" tab and click on the \"Add SSH Key\" button.</li> <li>Enable the \"Create new keys\" option and click on the \"Generate Keys\" button.</li> <li>Copy the generated Public Key and click the \"Submit\" button.</li> <li>Navigate to the \"Repositories\" tab and select the SSH key you just added while creating an account.</li> </ol> </li> <li>In Bitbucket:<ol> <li>Open Bitbucket and select the desired repository. Click on the \"Repository Settings\" option. Look for the \"Access Keys\" option and click on it.</li> <li>If you prefer to give global access Open Bitbucket and access your Personal settings by clicking on your avatar. Within Bitbucket, select \"SSH keys\". </li> <li>Click on the \"Add key\" button.</li> <li>Copy the public key that was generated in ViaFoundry.</li> <li>Paste the copied public key into the \"SSH Key\" field in Bitbucket.</li> </ol> </li> </ul> </li> </ul> <p>By following these steps, you will have successfully created and linked the SSH key pair between ViaFoundry and Bitbucket. This will allow you to create and update repositories in Bitbucket using the SSH keys from ViaFoundry.</p>"},{"location":"ViaFoundry/profile/#change-password","title":"Change Password","text":"<p>If you currently rely on a password for logging into your account, you can easily modify your password by accessing the dedicated Change Password section. </p>"},{"location":"ViaFoundry/profile/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"},{"location":"ViaFoundry/project/","title":"Project Guide","text":""},{"location":"ViaFoundry/project/#project-guide","title":"Project Guide","text":"<p>This guide will walk you through creating a project using the Via Foundry platform, and how to insert pipelines and files into it.</p>"},{"location":"ViaFoundry/project/#creating-projects-and-adding-pipelines","title":"Creating Projects and Adding Pipelines","text":"<p>Projects are platforms to categorize all of your runs and files. To run a pipeline, you must add it into your project. There are two ways to create a new project and add pipelines to it:</p> <ul> <li>A.  First, click the \"Projects\" button in the top of the screen.           From the dropdown menu, you can create a new project by           clicking <code>Add a New Project</code>. After saving the name of the           project, it will be added to your Projects table, which is           accessible from the dropdown menu, as shown in the image           below:</li> </ul> <p></p> <p>Note: You can always edit or remove your projects by clicking <code>View All Projects</code> button or clicking on the project   name from the dropdown menu, navigating to the <code>Settings</code> tab, and   clicking the <code>Edit</code> or <code>Delete Project</code> icons.</p> <p>Now you are able to enter the project page by clicking the name of the project from the <code>Projects</code> dropdown menu. You will notice five tabs in the project page:</p> <ul> <li>Dashboard: Your project's homepage. The dashboard contains the <code>Description</code> box, where you can write down information about the project; and the <code>Analysis</code> section, which will contain reports on the project and its runs.</li> <li>Data Collections: The project files can be organized in this section. You can create collections on the run page and reorganize them here. To add data, simply click the \"Add Collection\" button.</li> <li>Pipelines: To make the desired pipeline(s) available for your project, click on the \"Add Pipeline\" button. From the provided list, select the pipelines you want to include. Once selected, click the run button to navigate to the run page. If you wish to create a custom pipeline, click on the \"Create Pipeline\" button.</li> <li>Runs: Table containing information about all the runs         conducted in your current project (see image below). You can         enter a specific run's page by clicking on the name of the          run.</li> <li> <p>Settings: Information about your project: name, creator,         and date/time created. From this tab, you can edit or delete a         given project, as mentioned above.     </p> </li> <li> <p>B.  You can also create a project by clicking the <code>Pipelines</code>           button on the top left of the page. Select the pipeline you           want to run by clicking on it. At the top right of the           pipeline page, you'll see a <code>Run</code> button; press this to           initiate a run.</p> </li> </ul> <p></p> <p>The <code>Run</code> button opens a new window where you can create a new project   by clicking <code>Create a Project</code>. After entering and saving the project   name, it will be added to your project list. From here, you can select   your project by clicking on it, as shown in the image below.</p> <p></p> <p>You can proceed by entering the run name, which will subsequently be   added to your project's run list. Clicking <code>Save Run</code> will redirect   you to the \"Run Page\", where you can initiate your run.</p>"},{"location":"ViaFoundry/project/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"},{"location":"ViaFoundry/quick/","title":"Quick Start Guide","text":""},{"location":"ViaFoundry/quick/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"ViaFoundry/quick/#signing-up","title":"Signing Up","text":"<p>This guide will walk you through how to start using Via Foundry (formerly DolphinNext) pipelines. First off, you need to navigate to the Via Foundry web page at https://www.viafoundry.com and click the <code>Log in</code> button. You will be asked to enter your institute's log-in information. If you don't have an account, please let us know about it  support@viascientific.com. We will set an account for you.</p> <p></p>"},{"location":"ViaFoundry/quick/#run-environments","title":"Run Environments","text":"<p>To access your run environment, simply click on the Profile icon located at the top right corner and navigate to the Run Environment tab. In most cases, we automatically set up the run environment for you. However, if you require additional information on setting up and customizing your profile, please refer to our comprehensive Profile Guide.</p>"},{"location":"ViaFoundry/quick/#running-pipelines","title":"Running Pipelines","text":"<ol> <li> <p>The easiest way to run a pipeline is from the main page, by clicking     the <code>Pipelines</code> button at the top left of the screen. From here, you     can investigate publicly available pipelines as shown below and     select the pipeline you want to run by clicking on it.</p> <p></p> </li> <li> <p>Once the pipeline is loaded, you will notice a <code>Run</code> button at the     right top of the page.</p> <p></p> </li> <li> <p>Pressing this button opens a new window, where you can create a new     project by clicking <code>Create a Project</code>. After you enter and save the     name of the project, it will be added to your project list. Now you     can select your project by clicking on it, as shown in the figure     below.</p> <p></p> </li> <li> <p>After clicking <code>Select Project</code>, you may proceed with entering your     desired run name, which will be added to your project's run list.     Clicking <code>Save Run</code> will redirect you to the \"Run Page\".</p> </li> <li> <p>At first, in the header of the run page, you will see an orange \"Waiting\" button. To start a run, you need to enter/select the following:</p> <p></p> <p>A.  Run Environment: The environment, discussed in the Profile page, within which you'd like to conduct your run.</p> <p></p> <p>B. Work Directory: The work directory refers to the complete path of the directory from which the execution of Nextflow runs will take place. The path may be automatically filled in if a default value is provided in your run environment.</p> <p></p> <p>C. Inputs: In this section, you are required to enter various values and files that define the data to be processed and the corresponding processing instructions. For additional information, please check the Adding Files section.</p> <p></p> </li> <li> <p>Once all requirements are satisfied, the <code>Waiting</code> button will turn     into a green <code>Run</code> button as shown below. You can initiate your run     by clicking the <code>Run</code> button. Please go through the Run     Guide for detailed explanation about each module is used.</p> <p></p> </li> </ol>"},{"location":"ViaFoundry/quick/#adding-files","title":"Adding Files","text":""},{"location":"ViaFoundry/quick/#remote-files","title":"Remote Files","text":"<p>You can reach your remote files by entering:</p> <ul> <li>Full path of a directory: e.g.       <code>/share/data/umw_biocore/genome_data/mousetest/mm10/gz</code></li> <li>Web link: e.g.       <code>https://web.dolphinnext.com/umw_biocore/dnext_data/tutorial/fastq_data/pair</code></li> <li>Amazon (S3) Bucket: e.g. <code>s3://viafoundry/fastq</code></li> <li>Google (GS) Bucket: e.g. <code>gs://viafoundry/fastq</code></li> </ul>"},{"location":"ViaFoundry/quick/#geo-files","title":"Geo Files","text":"<p>If you want to download and use NCBI (GEO data) in the pipeline, you can simply use the <code>GEO Files</code> tab. Here are the few examples for GEO ID: <code>GSM1331276</code>, <code>GSE55190</code>, <code>SRR10095965</code></p> <p></p>"},{"location":"ViaFoundry/quick/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"},{"location":"ViaFoundry/run/","title":"Run Guide","text":""},{"location":"ViaFoundry/run/#run-guide","title":"Run Guide","text":"<p>In the previous tutorial (Project Guide), we went through the process of creating a project within the Via Foundry (formerly DolphinNext) platform and adding pipelines to it. In this guide, we will look through all the relevant run settings needed to initiate a new run.</p>"},{"location":"ViaFoundry/run/#brief-refresher","title":"Brief Refresher","text":"<p>To access a run page, navigate to the project the run is housed within by clicking on the appropriate project name in the <code>Project</code> dropdown menu. On the <code>Dashboard</code> page, under the <code>Analysis</code> section, click the name of your run.</p>"},{"location":"ViaFoundry/run/#basics","title":"Basics","text":"<p>On the run page's header, you can see the names of the current project, pipeline being used, and run in progress. Ensure that you're in the right project and running the correct pipeline before proceeding.</p> <p></p> <p>As you can see, <code>Save Run</code>, <code>Download Pipeline</code>, and <code>Delete Run</code> icons are conveniently placed next to the information mentioned above in the run page's header. Additionally, you can find <code>Delete Run</code>, <code>Duplicate Run</code>, and <code>Move Run</code> options by clicking on the three dots next to the status indicator.</p>"},{"location":"ViaFoundry/run/#run-settings-and-status","title":"Run Settings and Status","text":"<p>The status of your current run is displayed at the far right of the run page's header. Initially, you'll see an orange <code>Waiting</code> button. In order to initiate a run, the following data need to be entered:</p> <ol> <li>Run Environment: The environment, discussed in the      profile page, within which you'd like to conduct      your run.</li> <li>Work Directory: Full path of the directory where Nextflow runs      will be executed. (e.g. <code>/home/newuser/workdir</code>)</li> <li>Inputs: Various values and files, specifying which data      will be processed and how (i.e. whether single-end vs. paired-end      data are being used), need to be entered in the Run Settings      page. For additional information, please check the Adding Files       section.</li> </ol> <p></p> <p>All possible status messages are listed here:</p> Status Meaning Waiting Waiting for inputs, output directory and selection of active run environment Ready Ready to initiate run Connecting Sending SSH queries to selected host system Initializing Job is submitted, waiting for run execution Running Nextflow has executed and is running the jobs. Completed The job is completed. Run Error Error occurred before submitting the jobs or while executing the jobs. Terminated User terminated the run by using the <code>Terminate Run</code> button."},{"location":"ViaFoundry/run/#advanced-options","title":"Advanced Options","text":"<ul> <li> <p>Run Sharing (Permissions to View): By default, all runs are only visible to their owners. However, you have the option to share your run with a specific group that you have created in the profile's \"Groups\" tab. To do this, choose \"Only my group\" and select the name of the desired group. Members of that group will then be able to view the run on their run status page.</p> <p>Alternatively, you can set the permissions to \"Everyone\". With this setting, the run will only be visible to users who know the specific run link. The selected run will not appear on their run status page, but they will be able to access it if they have the link.</p> </li> <li> <p>Publish Directory: The Work Directory also serves as the default     directory to which output files are sent for Via Foundry runs. If     you want to change the path to a different directory, just enter the     full path of your desired Publish Directory in this box. Local paths     (e.g. <code>/home/user/test</code>), Amazon S3 paths (e.g.     <code>s3://yourbucket/test</code>) or Google Storage paths (e.g.     <code>gs://yourbucket/test</code>) are all accepted.</p> </li> <li> <p>Run Container: During the pipeline creation process, we specified the containers to be used for each process. Consequently, when you select the run environment, the corresponding run container (either Docker or Singularity) checkbox will be automatically selected, and the defined container will be populated in the image field.</p> <ul> <li> <p>A. Use Docker Image: </p> <ol> <li> <p>Image: Docker image name. Example:</p> <p>viascientific/rnaseq:4.0</p> </li> <li> <p>RunOptions (optional): Foundry has the ability to autodetect all the paths used and automounts all the required files to the container before the run starts. Moreover, you have the flexibility to enter any command line arguments supported by the Docker run command. Please click this Docker link for details on how you can configure this section.</p> </li> </ol> </li> <li> <p>Use Singularity Image: Instead of Docker, you can activate a Singularity image if you wish by clicking the <code>Use Singularity Image</code> checkbox and entering the relevant information, expounded upon below. In order to use a Singularity image, you must first install Singularity.</p> <ol> <li> <p>Image: Path to your desired Singularity image. For example:</p> <p><code>docker://viascientific/rnaseq:4.0</code></p> <p><code>shub://UMMS-biocore/singularitysc</code></p> <p><code>/project/umw_biocore/singularity/UMMS-Biocore-singularity-master.simg</code></p> </li> <li> <p>RunOptions (optional): Foundry automatically detects and mounts all the necessary files to the container by detecting the used paths. When using Singularity, you have the option to enter command line options supported by the Singularity exec command. One example of such an option is --bind, which allows you to mount directories. For more information about the command line arguments supported by Singularity, please refer to this link.</p> <p><code>--bind /project:/project --bind /nl:/nl</code></p> </li> </ol> </li> </ul> </li> <li> <p>Executor Settings: A series of parameters governing the     execution of your run, including what packages to run and how much     processing power to allocate to each package.</p> <p>1. Executor Settings for Nextflow: (navigate to Profile \u2192 Run Environments \u2192 Edit Run Environment): Here, you can specify the system on which Nextflow is initiated. Via Foundry currently supports various executors for running Nextflow itself, including Local, SGE, SLURM, and LSF. These executors are exclusively used for running Nextflow.</p> <p>Suggested parameters for the executor settings are as follows: long, 8GB memory, 1 CPU, and a time range of 5000-8000 minutes.</p> <p>2. Executor of Nextflow Jobs: (navigate to Profile --&gt; Run   Environments --  Edit Run Environment) This setting will be   used if you don't manually set any parameters in the Advanced   section of your run page. If any option other than Local is   selected, you'll be prompted to input values for <code>Queue</code>,   <code>Memory(GB)</code>, <code>CPU</code> and <code>Time(min.)</code>. These parameters can be adjusted according to your needs. </p> <p>Suggested parameters for this configuration are as follows: short, 20GB memory, 1 CPU, and 240 minutes of execution time.</p> <p></p> <p>3. Executor Settings for All Processes (in <code>Advanced</code> tab   of run page): This setting will override the parameters specified in the <code>Executor of Nextflow Jobs</code> section. It allows you to define the executor settings for all processes in your run.</p> <p>Suggested parameters for this configuration are as follows: short, 20GB memory, 1 CPU, and 240 minutes of execution time.</p> <p>4. Executor Settings for Each Process (in <code>Advanced</code> tab   of run page): If a particular process needs special parameters   other than Executor settings for all processes, you can   override the default parameters by clicking on the checkbox   corresponding to the process that you want to change. This will   only affect the settings of the selected process while retaining   the original settings for all other processes. Suggested   parameters: long 20GB 4CPU 1000-5000min</p> </li> <li> <p>Delete intermediate files after run: By default, Via Foundry     deletes any intermediate files created during a run, only retaining     the necessary files in report folder. This     setting is aimed at minimizing the storage required for a project,     but you can uncheck the box to keep all intermediate files.</p> </li> </ul>"},{"location":"ViaFoundry/run/#workflow","title":"Workflow","text":"<p>To provide a visualization of the current run's architecture, the selected pipeline and its modules are showed on this page. To see more information about the pipeline's settings, click the Go to Pipeline link at the top of this page.</p>"},{"location":"ViaFoundry/run/#run-logs","title":"Run Logs","text":"<p>This section keeps track of each run. You can monitor each stage of the run both before and after Nextflow execution, as shown here:</p> <p></p> <p>You can view various log files, such as timeline.html, dag.html, trace.txt, .nextflow.log, nextflow.nf, nextflow.config, as shown here:</p> <ul> <li>timeline.html:</li> </ul> <p></p> <ul> <li>dag.html:</li> </ul> <p></p> <ul> <li>trace.txt:</li> </ul> <p></p> <ul> <li>.nextflow.log:</li> </ul> <p></p> <ul> <li>nextflow.nf:</li> </ul> <p></p> <ul> <li>nextflow.config:</li> </ul> <p></p> <p>If an error occurred at any point during the run, a detailed explanation about the error will be displayed here, and the status of the run will change to <code>Run Error</code>.</p> <p></p>"},{"location":"ViaFoundry/run/#report","title":"Report","text":"<p>This tab will appear in the run page upon run initialization. You can view the output files in various modules such as R-Markdown, Datatables, Highcharts, HTML or PDF Viewer. For reference, check the example Report section of an RNA-Seq pipeline at below.</p> <p></p> <p>Each report row corresponds to an output parameter in the pipeline's workflow, and you can easily see a row's content by clicking on it. All these sections have <code>Download</code>, <code>Full Screen</code>, and <code>Open in New Window</code> icons to help you best analyze each report.</p> <p>Note: If you want to integrate your own visualization tool into Via Foundry, please let us know about it at support@viascientific.com, and we'd be happy to add it for you.</p>"},{"location":"ViaFoundry/run/#-shiny-app-debrowser-","title":"-   Shiny App - DEBrowser -","text":"<p>DEBrowser is an R library which provides an easy way to perform and visualize DE (Differential Expression) analysis. This module takes count matrices as input and allows interactive exploration of the resulting data. You can find their documentation here.</p> <p></p>"},{"location":"ViaFoundry/run/#-r-studio-r-markdown-","title":"-   R-Studio - R-Markdown -","text":"<p>The R-Studio launcher facilitates interactive analysis of the data generated from a run. We have prepared a set of R-Markdown reports that provide access to your report in HTML or PDF format immediately after the run is completed.</p> <p>For instance, the code below performs differential expression analysis for each comparison listed in the compare file. It generates volcano and MA plots for differentially expressed genes in each comparison:</p> <p></p>"},{"location":"ViaFoundry/run/#-jupyter-notebook-","title":"-   Jupyter Notebook -","text":"<p>The Jupyter Notebook app, due to its interactive and flexible nature, it allows bioinformatics researchers to combine code, visualizations, and explanatory text in a single document. Bioinformaticians can write and execute code snippets in real-time, visualize data using various plotting libraries, and document their analyses step-by-step.</p> <p></p>"},{"location":"ViaFoundry/run/#-shiny-app-gsea-explorer-","title":"-   Shiny App - GSEA Explorer -","text":"<p>GSEA Explorer is an R library that offers a convenient method for conducting and visualizing Gene Set Enrichment Analysis (GSEA). GSEA aims to assess whether a specific gene set or pathway is enriched in gene expression data, indicating its potential biological significance in the studied condition. The GSEA Explorer application can be accessed after executing Foundry's complete RNA-sequencing pipeline or the standalone Differential Expression module. By leveraging GSEA Explorer, researchers can gain valuable insights into the functional implications of gene sets and pathways, aiding in the interpretation of RNA-seq results and facilitating a deeper understanding of biological mechanisms.</p> <p></p>"},{"location":"ViaFoundry/run/#-shiny-app-network-explorer-","title":"-   Shiny App - Network Explorer -","text":"<p>The Network Explorer allows bioinformaticians to explore and analyze these complex networks, helping them uncover hidden patterns, identify key players, and understand the underlying biological mechanisms. The Network Explorer application can be launched after running Foundry's full RNA-sequencing pipeline or the stand-alone Differential Expression module.</p> <p></p>"},{"location":"ViaFoundry/run/#-datatables-","title":"-   Datatables -","text":"<p>This module, powered by Datatables, allows you to view, sort, and search the table's content. The following two examples depict alignment and RSEM summaries within Datatables.</p> <ul> <li>Alignment Summary:</li> </ul> <p></p> <ul> <li>RSEM Summary:</li> </ul> <p></p> <p>You can fit the entire table in your screen by clicking the <code>Full screen</code> icon at the top of the module.</p>"},{"location":"ViaFoundry/run/#-htmlpdf-viewer-","title":"-   HTML/PDF Viewer -","text":"<p>You can easily embed HTML/PDF content in our Report section by using HTML/PDF Viewer. Reference this image, which shows MultiQC output, for an example:</p> <p></p>"},{"location":"ViaFoundry/run/#support","title":"Support","text":"<p>For any questions or help, please reach out to support@viascientific.com with your name and question.</p>"}]}